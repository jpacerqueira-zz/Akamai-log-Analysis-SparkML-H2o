#!/usr/bin/env bash

DATE_V1=$1
if [ -z "$DATE_V1" ]
then
 DATE_V1=20170101
fi

#  Spark Job operations
MAIN_CLASS1=ptv.content.akamailogs.staged.parquetindexer.SparkPublishLogsArchiveProd
APP_JAR=/home/oracle/projects/dazn_ott/logs-archive-production/data_etl/parquetindexer-1.0-TPOC-jar-with-dependencies.jar
EXTRA_JAR=/home/oracle/projects/dazn_ott/logs-archive-production/data_etl/spark-csv_2.11-1.5.0.jar

# Additional for Spark container 

MASTER_URL=yarn
DEPLOY_MODE=client

NUM_EXECUTORS=20
DRIVER_MEMORY=4g
EXECUTOR_MEMORY=2G
EXECUTOR_CORES=4

# Additional Logic workarround for new null token DI
#
#

echo "STAGED  d=${DATE_V1} FILES COUNT : "
hdfs dfs -ls -R /data/staged/ott_dazn/docomo_investigations/logs-archive-production/parquet/dt=${DATE_V1}/*.parquet | wc -l 

echo "Clean Hive Published to Avoid Duplicates of dt=20${DATE_V1}"
hdfs dfs -rm -r -f -skipTrash  /user/hive/warehouse/published_ott_dazn.db/massive_elb_logs/dt=${DATE_V1}/*.parquet 

# Extra Parameters

IN_PATH="/data/staged/ott_dazn/docomo_investigations/logs-archive-production/parquet/"
OUT_PATH="/user/hive/warehouse/published_ott_dazn.db/massive_elb_logs/"
HIVE_METASTORE="thrift://anltstg3.stage.ix.perform.local:9083"

spark2-submit  --class ${MAIN_CLASS1} --master ${MASTER_URL} --deploy-mode ${DEPLOY_MODE} --jars ${APP_JAR} ${EXTRA_JAR} --processdate ${DATE_V1} --inputpath ${IN_PATH} --outputpath ${OUT_PATH} --hivemetastore ${HIVE_METASTORE} >>  /home/oracle/projects/dazn_ott/logs-archive-production/data_etl/logs/SparkPublishLogsArchiveProd-1-${DATE_V1}.log

echo 1
