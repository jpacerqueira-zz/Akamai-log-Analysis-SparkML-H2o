{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- beat: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- logzio_id: string (nullable = true)\n",
      " |-- awsRegion: string (nullable = true)\n",
      " |-- beat: struct (nullable = true)\n",
      " |    |-- hostname: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- version: string (nullable = true)\n",
      " |-- correlationId: string (nullable = true)\n",
      " |-- function_version: string (nullable = true)\n",
      " |-- host: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_type: string (nullable = true)\n",
      " |-- invoked_function_arn: string (nullable = true)\n",
      " |-- it: string (nullable = true)\n",
      " |-- level: long (nullable = true)\n",
      " |-- logGroup: string (nullable = true)\n",
      " |-- logStream: string (nullable = true)\n",
      " |-- logzio_codec: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- messageType: string (nullable = true)\n",
      " |-- msg: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- owner: string (nullable = true)\n",
      " |-- sessionId: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- token: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- beat: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- logzio_id: string (nullable = true)\n",
      " |-- awsRegion: string (nullable = true)\n",
      " |-- beat: struct (nullable = true)\n",
      " |    |-- hostname: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- version: string (nullable = true)\n",
      " |-- correlationId: string (nullable = true)\n",
      " |-- function_version: string (nullable = true)\n",
      " |-- host: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_type: string (nullable = true)\n",
      " |-- invoked_function_arn: string (nullable = true)\n",
      " |-- it: string (nullable = true)\n",
      " |-- level: long (nullable = true)\n",
      " |-- logGroup: string (nullable = true)\n",
      " |-- logStream: string (nullable = true)\n",
      " |-- logzio_codec: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- messageType: string (nullable = true)\n",
      " |-- msg: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- owner: string (nullable = true)\n",
      " |-- sessionId: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- token: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- beat: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- logzio_id: string (nullable = true)\n",
      " |-- awsRegion: string (nullable = true)\n",
      " |-- beat: struct (nullable = true)\n",
      " |    |-- hostname: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- version: string (nullable = true)\n",
      " |-- correlationId: string (nullable = true)\n",
      " |-- function_version: string (nullable = true)\n",
      " |-- host: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- hostname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_type: string (nullable = true)\n",
      " |-- invoked_function_arn: string (nullable = true)\n",
      " |-- it: string (nullable = true)\n",
      " |-- level: long (nullable = true)\n",
      " |-- logGroup: string (nullable = true)\n",
      " |-- logStream: string (nullable = true)\n",
      " |-- logzio_codec: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- messageType: string (nullable = true)\n",
      " |-- msg: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- owner: string (nullable = true)\n",
      " |-- sessionId: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- token: string (nullable = true)\n",
      " |-- messagecut: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- beat: struct (nullable = true)\n",
      " |    |-- hostname: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- version: string (nullable = true)\n",
      " |-- host: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- it: string (nullable = true)\n",
      " |-- logzio_codec: string (nullable = true)\n",
      " |-- logzio_id: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- messagecut: string (nullable = true)\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- beat: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- message: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- @metadata: struct (nullable = true)\n",
      " |    |-- beat: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- version: string (nullable = true)\n",
      " |-- @timestamp: string (nullable = true)\n",
      " |-- __logz_account_id: string (nullable = true)\n",
      " |-- __logz_received_timestamp: string (nullable = true)\n",
      " |-- __logzio_X-B3-Sampled: string (nullable = true)\n",
      " |-- __logzio_X-B3-SpanId: string (nullable = true)\n",
      " |-- __logzio_X-B3-TraceId: string (nullable = true)\n",
      " |-- __logzio_id: string (nullable = true)\n",
      " |-- beat: struct (nullable = true)\n",
      " |    |-- hostname: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- version: string (nullable = true)\n",
      " |-- host: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- input_type: string (nullable = true)\n",
      " |-- it: string (nullable = true)\n",
      " |-- logzio_codec: string (nullable = true)\n",
      " |-- logzio_id: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- messagecut: string (nullable = true)\n",
      " |-- metadata: struct (nullable = true)\n",
      " |    |-- beat: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- message: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ngrams_7: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- ngramscounts_7: vector (nullable = true)\n",
      "\n",
      "root\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ngrams_7: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- ngramscounts_7: vector (nullable = true)\n",
      " |-- features_85: vector (nullable = true)\n",
      "\n",
      "Calculation of most frequent fraud_ngram notfraud_ngram - Start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features_85: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- fraud_label: long (nullable = true)\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- ngrams_7: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ngramscounts_7: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      "\n",
      "Value Print: standard_fraud_ngram=\n",
      "                                        hash_message\n",
      "0  d71ec5039b409ec52b112cdf682cf2f4554329bc882073...\n",
      "root\n",
      " |-- features_85: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- fraud_label: long (nullable = true)\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- ngrams_7: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ngramscounts_7: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      "\n",
      "Value Print: standard_notfraud_ngram=\n",
      "                                        hash_message\n",
      "0  f55d39ad779e11e71a4133be7be5f3f5b65364e972f8e5...\n",
      "Calculation of most frequent fraud_ngram notfraud_ngram - Done!\n",
      "Preparation of Data Done!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import findspark\n",
    "findspark.init()\n",
    "#\n",
    "import pyspark\n",
    "from pyspark.sql import functions as pfunc\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Window, types\n",
    "#\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from scipy.stats import kstest\n",
    "from scipy import stats\n",
    "#\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "#\n",
    "#import org.apache.spark.ml.feature.NGram\n",
    "from pyspark.ml.feature import NGram\n",
    "#\n",
    "from collections import Counter\n",
    "#\n",
    "from pyspark.ml.feature import NGram\n",
    "#\n",
    "from pyspark.ml.feature import NGram, CountVectorizer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "#\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector\n",
    "#\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "#\n",
    "#\n",
    "sc = pyspark.SparkContext(appName=\"FraudCanada-Model-NGrams-CountVectorizer-KL-KS-Entropy-Model-CleanData\")\n",
    "sqlContext = SQLContext(sc)\n",
    "#\n",
    "# Arguments\n",
    "#\n",
    "#import argparse\n",
    "## Parse date_of execution\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--datev1\", help=\"Execution Date\")\n",
    "#args = parser.parse_args()\n",
    "#if args.datev1:\n",
    "#    processdate = args.datev1\n",
    "# GENERAL PREPARATION SCRIPT\n",
    "#\n",
    "#  Date in format YYYYMMDD\n",
    "#process_date = processdate\n",
    "#if not process_date:\n",
    "#    process_date = \"20181231\"\n",
    "#\n",
    "#\n",
    "process_date=\"20190206\"\n",
    "#\n",
    "input_file1_playback_fraud=\"hdfs:///data/staged/ott_dazn/fraud-canada-tokenizedwords/dt=*/*.json\"\n",
    "output_file1=\"hdfs:///data/staged/ott_dazn/advanced-model-data/fraud-notfraud-canada-tokenizedwords-ngrams-7-features-85/dt=\"+process_date\n",
    "#\n",
    "input_file2_playback_not_fraud=\"hdfs:///data/staged/ott_dazn/logs-archive-production/parquet/dt=\"+process_date+\"/*.parquet\"\n",
    "output_file2=\"hdfs:///data/staged/ott_dazn/advanced-model-data/not-fraud-canada-tokenizedwords/dt=\"+process_date\n",
    "input_file3=output_file2\n",
    "#\n",
    "input_file4=\"hdfs:///data/staged/ott_dazn/advanced-model-data/fraud-notfraud-canada-tokenizedwords-ngrams-7-features-85/dt=\"+process_date+\"/*.*\"\n",
    "#\n",
    "output_most_frequent_fraud_df=\"hdfs:///data/staged/ott_dazn/advanced-model-data/the-most-frequent-fraud-hash_message/dt=\"+process_date\n",
    "#\n",
    "output_most_frequent_notfraud_df=\"hdfs:///data/staged/ott_dazn/advanced-model-data/the-most-frequent-notfraud-hash_message/dt=\"+process_date\n",
    "#\n",
    "#  FILTER Non-Fraud AND LABEL\n",
    "from pyspark.sql import functions as F\n",
    "#\n",
    "#\n",
    "df2= sqlContext.read.parquet(input_file2_playback_not_fraud)\n",
    "df2.printSchema()\n",
    "#\n",
    "df3 = df2.filter(\" (message LIKE '%\\\"Url\\\":\\\"https://isl-ca.dazn.com/misl/v2/Playback%') AND (message LIKE '%,\\\"Response\\\":{\\\"StatusCode\\\":200,\\\"ReasonPhrase\\\":\\\"OK\\\",%') AND ( ( (message LIKE '%&Format=MPEG-DASH&%' OR message LIKE '%&Format=M3U&%') ) OR (message NOT LIKE '%\\\"User-Agent\\\":\\\"Mozilla/5.0,(Macintosh; Intel Mac OS X 10_12_6),AppleWebKit/605.1.75,(KHTML, like Gecko),Version/11.1.2,Safari/605.1.75\\\"},%')   )  \")\n",
    "df3.printSchema()\n",
    "df4 = df3.withColumn(\"messagecut\", expr(\"substring(message, locate('|Livesport.WebApi.Controllers.Playback.PlaybackV2Controller|',message)+60 , length(message)-1)\"))\n",
    "#\n",
    "# val regexTokenizer = new RegexTokenizer().setInputCol(\"messagecut\").setOutputCol(\"words\").setPattern(\"\\\\w+|\").setGaps(false)\n",
    "#\n",
    "regexTokenizer = RegexTokenizer(minTokenLength=1, gaps=False, pattern='\\\\w+|', inputCol=\"messagecut\", outputCol=\"words\", toLowercase=True)\n",
    "#\n",
    "tokenized = regexTokenizer.transform(df4)\n",
    "tokenized.printSchema()\n",
    "tokenized.coalesce(1).write.json(output_file2)\n",
    "# Tokenize NON-Fraud-LABEL\n",
    "# hash the message de-duplicate those records\n",
    "notfraud_file=sqlContext.read.json(input_file3).repartition(500)\n",
    "notfraud_file.printSchema()\n",
    "#\n",
    "notfraud_df=notfraud_file\\\n",
    ".filter(\"message IS NOT NULL\").filter(\"words IS NOT NULL\")\\\n",
    ".withColumn('fraud_label',lit(0).cast('int'))\\\n",
    ".withColumn('hash_message',F.sha2(col('message'),512)).groupby(col('hash_message'))\\\n",
    ".agg(F.first(col('fraud_label')).alias('fraud_label'),F.first(col('words')).alias('words'),F.first(col('message')).alias('message'))\\\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "notfraud_df.printSchema()\n",
    "# Only the Not-Fraud are randomly sorted\n",
    "#\n",
    "from pyspark.sql.functions import rand\n",
    "#\n",
    "df_notfraud_words = notfraud_df.filter(\"message IS NOT NULL\").select(col('fraud_label'),col('hash_message'),col('words'))\\\n",
    ".orderBy(rand())\\\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "df_notfraud_words.printSchema()\n",
    "#\n",
    "# FILTER FRAUD AND LABEL \n",
    "# Join with Internal Curation Data in urltopredict staged folder\n",
    "# hash the message de-duplicate those records\n",
    "fraud_file=sqlContext.read.json(input_file1_playback_fraud).repartition(500)\n",
    "fraud_file.printSchema()\n",
    "#\n",
    "fraud_df=fraud_file\\\n",
    ".filter(\"message IS NOT NULL\").filter(\"words IS NOT NULL\")\\\n",
    ".withColumn('fraud_label',lit(1).cast('int'))\\\n",
    ".withColumn('hash_message',F.sha2(col('message'),512)).groupby(col('hash_message'))\\\n",
    ".agg(F.first(col('fraud_label')).alias('fraud_label'),F.first(col('words')).alias('words'),F.first(col('message')).alias('message'))\\\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "fraud_df.printSchema()\n",
    "#\n",
    "df_words = fraud_df.filter(\"message IS NOT NULL\").select(col('fraud_label'),col('hash_message'),col('words'))\\\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "df_words.printSchema()\n",
    "#\n",
    "# Limit to 250,000 Daily Not-Fraud Records input in the nGrams Graph analysis\n",
    "#\n",
    "result_fraud_nofraud_words = df_words.union(df_notfraud_words).limit(150000)\\\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "## Register Generic Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "# Build ngrams 75 90 n=6 \n",
    "# support : https://stackoverflow.com/questions/51473703/pyspark-ml-ngrams-countvectorizer-sorted-based-on-count-weights\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_ngrams_part(inputCol=\"words\", n=6):\n",
    "    ngrams = [ \n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"ngrams_{0}\".format(i)) \n",
    "        for i in range(7, n + 1) ]\n",
    "    vectorizers = [ \n",
    "        CountVectorizer(inputCol=\"ngrams_{0}\".format(i), outputCol=\"ngramscounts_{0}\".format(i)) \n",
    "        for i in range(7, n + 1) ]\n",
    "    return Pipeline(stages=ngrams + vectorizers)\n",
    "#    assembler = [VectorAssembler( inputCols=[\"ngramscounts_{0}\".format(i) for i in range(1, n + 1)], outputCol=\"features\" )]\n",
    "#    return Pipeline(stages=ngrams + DenseVector(SparseVector(vectorizers).toArray()))\n",
    "#\n",
    "# \n",
    "# -----------------------------------------------------------------------------\n",
    "#ngram = build_ngrams_part().fit(df_words)\n",
    "#ngramDataFrame = ngram.transform(df_words)\n",
    "#ngramDataFrame.coalesce(1).write.json(output_file1)\n",
    "#\n",
    "ngram = NGram(n=7, inputCol=\"words\", outputCol=\"ngrams_7\")\n",
    "countvector = CountVectorizer(inputCol=\"ngrams_7\", outputCol=\"ngramscounts_7\")\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "countvModel = CountVectorizer(inputCol=\"words\", outputCol=\"features_85\", vocabSize=85, minDF=2.0)\n",
    "# fit a PCA Dimensionality reduction into 7/3=2.x components from ngramscounts_7 ## Too Heavy 1st PCA\n",
    "pcaNgrams = PCA(k=3, inputCol=\"ngramscounts_7\", outputCol=\"pcaweightngrams\")\n",
    "# fit a PCA Dimensionality reduction into 85/17=5 components from words\n",
    "pcaWords = PCA(k=5, inputCol=\"features_85\", outputCol=\"pcaweightwords\")  ## Too Heavy 2nd PCA\n",
    "#\n",
    "ngram_fraud_DF = ngram.transform(result_fraud_nofraud_words)\n",
    "ngram_vc_fraud_DF = countvector.fit(ngram_fraud_DF).transform(ngram_fraud_DF)\\\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "ngram_vc_fraud_DF.printSchema()\n",
    "#\n",
    "# Trial of PCA now with Less Data\n",
    "#modelPCA_ngram_fraud_DF = pcaNgrams.fit(ngram_vc_fraud_DF).transform(ngram_vc_fraud_DF)\\\n",
    "#.persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "#modelPCA_ngram_fraud_DF.printSchema()\n",
    "##\n",
    "############# ISSUE - NGRAMS ARE TOO BIG ############\n",
    "#Traceback (most recent call last):\n",
    "#  File \"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/python/pyspark/sql/utils.py\", line 63, in deco\n",
    "#    return f(*a, **kw)\n",
    "#  File \"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\", line 320, in get_return_value\n",
    "#py4j.protocol.Py4JJavaError: An error occurred while calling o200.fit.\n",
    "#: java.lang.IllegalArgumentException: Argument with more than 65535 cols: 262144\n",
    "#\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.checkNumColumns(RowMatrix.scala:133)\n",
    "#\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:332)\n",
    "#\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:387)\n",
    "#\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:53)\n",
    "#\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:99)\n",
    "#\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
    "#\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
    "#\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
    "#\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
    "#\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
    "#\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
    "#\tat py4j.Gateway.invoke(Gateway.java:282)\n",
    "#\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
    "#\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
    "#\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
    "#\tat java.lang.Thread.run(Thread.java:748)\n",
    "#\n",
    "#\n",
    "#During handling of the above exception, another exception occurred:\n",
    "#\n",
    "#Traceback (most recent call last):\n",
    "#  File \"/home/siemanalyst/projects/logs-archive-production/fraud-canada-tokenizedwords/notebooks/x1-FraudCanada-Model-NGrams-CountVectorizer-KL-KS-Entropy-Model-CleanData.py\", line 173, in <module>\n",
    "#####################################################\n",
    "#\n",
    "#result_ngrams_words_fraud_DF = countvModel.fit(modelPCA_ngram_fraud_DF).transform(modelPCA_ngram_fraud_DF)\\\n",
    "result_ngrams_words_fraud_DF = countvModel.fit(ngram_vc_fraud_DF).transform(ngram_vc_fraud_DF)\\\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "result_ngrams_words_fraud_DF.printSchema()\n",
    "#\n",
    "#modelPCA_features_ngram_fraud_DF = pcaWords.fit(result_ngrams_words_fraud_DF).transform(result_ngrams_words_fraud_DF)\\\n",
    "#.persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "#modelPCA_features_ngram_fraud_DF.printSchema()\n",
    "#\n",
    "#modelPCA_features_ngram_fraud_DF.coalesce(1).write.json(output_file1)\n",
    "result_ngrams_words_fraud_DF.coalesce(1).write.json(output_file1)\n",
    "#\n",
    "print(\"Calculation of most frequent fraud_ngram notfraud_ngram - Start!\")\n",
    "#\n",
    "#  CALCULATE KL,KS COEF. Label Data fraud/not_fraud\n",
    "###################\n",
    "# Obtain the most frequent word on each position \n",
    "# Compose the standard_fraud_ngram from the most common positions\n",
    "#Â Calculate the standard_fraud_ngram\n",
    "#\n",
    "# https://stackoverflow.com/questions/35218882/find-maximum-row-per-group-in-spark-dataframe \n",
    "# Using struct ordering:\n",
    "#from pyspark.sql.functions import struct\n",
    "#\n",
    "#(cnts\n",
    "#  .groupBy(\"id_sa\")\n",
    "#  .agg(F.max(struct(col(\"cnt\"), col(\"id_sb\"))).alias(\"max\"))\n",
    "#  .select(col(\"id_sa\"), col(\"max.id_sb\")))\n",
    "#\n",
    "####################\n",
    "# FRAUD\n",
    "ngram7_fraud=sqlContext.read.json(input_file4).filter(\"fraud_label=1\")\n",
    "ngram7_fraud.printSchema()\n",
    "#\n",
    "most_frequent_fraud_df=ngram7_fraud\\\n",
    ".withColumn(\"value_sum\",F.explode(\"ngramscounts_7.values\"))\\\n",
    ".groupBy(\"hash_message\").agg(F.sum(\"value_sum\").alias('count'))\\\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "most_frequent_fraud_df.printSchema()\n",
    "#\n",
    "most_frequent_fraud_df.coalesce(1).write.json(output_most_frequent_fraud_df)\n",
    "#\n",
    "# The most Frequent would the the max\n",
    "standard_fraud_ngram=most_frequent_fraud_df.orderBy(col('count').desc()).select(col('hash_message')).limit(1).toPandas()\n",
    "#\n",
    "print(\"Value Print: standard_fraud_ngram=\")\n",
    "print(standard_fraud_ngram)\n",
    "#\n",
    "####################\n",
    "# NOT FRAUD\n",
    "ngram7_notfraud=sqlContext.read.json(input_file4).filter(\"fraud_label=0\")\n",
    "ngram7_notfraud.printSchema()\n",
    "#\n",
    "most_frequent_notfraud_df=ngram7_notfraud\\\n",
    ".withColumn(\"value_sum\",F.explode(\"ngramscounts_7.values\"))\\\n",
    ".groupBy(\"hash_message\").agg(F.sum(\"value_sum\").alias('count'))\\\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "most_frequent_notfraud_df.printSchema()\n",
    "#\n",
    "most_frequent_notfraud_df.coalesce(1).write.json(output_most_frequent_notfraud_df)\n",
    "#\n",
    "# The most Frequent would the the max\n",
    "standard_notfraud_ngram=most_frequent_notfraud_df.orderBy(col('count').desc()).select(col('hash_message')).limit(1).toPandas()\n",
    "#\n",
    "print(\"Value Print: standard_notfraud_ngram=\")\n",
    "print(standard_notfraud_ngram)\n",
    "#\n",
    "print(\"Calculation of most frequent fraud_ngram notfraud_ngram - Done!\")\n",
    "#\n",
    "sc.stop()\n",
    "#\n",
    "print(\"Preparation of Data Done!\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value UDF : standard_fraud_ngram=\n",
      "d71ec5039b409ec52b112cdf682cf2f4554329bc88207360175c3dd2d3ba04771d7e828aec45badeed2451c7dff5349bd17acf9045ea29d01ac02445d41e2387\n",
      "root\n",
      " |-- features_85: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- fraud_label: long (nullable = true)\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- ngrams_7: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ngramscounts_7: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- features_85: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- fraud_label: long (nullable = true)\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- ngrams_7: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ngramscounts_7: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- fraud_master_hash: string (nullable = false)\n",
      "\n",
      "Value UDF : standard_fraud_words=\n",
      "root15bfb256439d6820d1a0f97fc0bbe9735requestmethodgeturlhttpsislcadazncommislv2playbackassetid7ayxze86qe311h5f10m2oxt09eventidar95qdl7hjd8tv431zrcbg6zuformatmpegdashplayeriddazn001c3b59f4securetruelanguagecodeenlatitudenulllongitudenullplatformwebmanufacturermodelnullclientip509336132headersuseragentmozilla50macintoshintelmacosx10_12_6applewebkit605115khtmllikegeckoversion1112safari605115typeinresponsestatuscode200reasonphraseokduration62jwtviewerid7354fd69f36edeviceid8d1d417a8d2843b3a4a77354fd69f36e001c3b59f4userstatusactivepaid\n",
      "Value UDF : standard_notfraud_ngram=\n",
      "f55d39ad779e11e71a4133be7be5f3f5b65364e972f8e51ab71930c8a9842ed6065cc5d3580a65cbaf41a487a617f4719574ac869375dc9626b269b952131d56\n",
      "root\n",
      " |-- features_85: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- fraud_label: long (nullable = true)\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- ngrams_7: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ngramscounts_7: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- features_85: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- fraud_label: long (nullable = true)\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- ngrams_7: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ngramscounts_7: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- fraud_master_hash: string (nullable = false)\n",
      "\n",
      "Value UDF : standard_notfraud_words=\n",
      "root15c5a401af011a1be033ce92d8c26eb46c08a55e060c74eed9c1b2654c4dc649crequestmethodgeturlhttpsislcadazncommislv2playbackassetid6bsu76cb3lxn1fus03bst36dxeventidarticleid6bsu76cb3lxn1fus03bst36dxformatmpegdashplayeriddazn1ce8bbdb3585bcdesecuretruelanguagecodeenlatitudenulllongitudenullplatformandroidtvmanufacturersonymodelnullmtalanguagecodeenclientip745721994headersuseragentmozilla50linuxandroid800bravia4kgbbuildopr2170623027s16wvapplewebkit53736khtmllikegeckoversion40chrome710357899mobilesafari53736fev1420typeinresponsestatuscode200reasonphraseokduration125jwtvieweridd9f1c6460681deviceid9e1d1116c7654c3aa67ad9f1c64606811ce8bbdb3585bcdeuserstatusactivepaid\n",
      "root\n",
      " |-- features_85: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- fraud_label: long (nullable = true)\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- ngrams_7: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ngramscounts_7: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- features_85: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- fraud_label: long (nullable = true)\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- ngramscounts_7: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- words_conc: string (nullable = false)\n",
      " |-- kl_fraud_words: double (nullable = true)\n",
      " |-- ks_fraud_words: double (nullable = true)\n",
      " |-- entropy_fraud_words: double (nullable = true)\n",
      " |-- kl_notfraud_words: double (nullable = true)\n",
      " |-- ks_notfraud_words: double (nullable = true)\n",
      " |-- entropy_notfraud_words: double (nullable = true)\n",
      "\n",
      "Model Data KL,KS, Entropy Done! NGrams Vectors Data Done!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import findspark\n",
    "findspark.init()\n",
    "#\n",
    "import pyspark\n",
    "from pyspark.sql import functions as pfunc\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Window, types\n",
    "#\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from scipy.stats import kstest\n",
    "from scipy import stats\n",
    "#\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "#\n",
    "#import org.apache.spark.ml.feature.NGram\n",
    "from pyspark.ml.feature import NGram\n",
    "#\n",
    "from collections import Counter\n",
    "#\n",
    "from pyspark.ml.feature import NGram\n",
    "#\n",
    "from pyspark.ml.feature import NGram, CountVectorizer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "#\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector\n",
    "#\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "#\n",
    "#  FILTER with PySpark SQL Functions F.\n",
    "from pyspark.sql import functions as F\n",
    "# FUNCTIONS\n",
    "# \n",
    "##\n",
    "## Register Generic UDF Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "# KL : Kullback-Leibler Divergence\n",
    "# KS : Kolmogorov-Smirnov ( Sample sizes can be different)\n",
    "# -----------------------------------------------------------------------------    \n",
    "## NGRAM hash_mesage KL from URL TO Default NGRAM KL\n",
    "def func_kl_ngram_msg(var1,var_match):\n",
    "    ##Making sure the analysis of has_message SHA_512 is linear.\n",
    "    ##Making sure the analysis is over the words list on same size\n",
    "    def KL(P,Q):\n",
    "        epsilon = 0.00001\n",
    "        P = P+epsilon\n",
    "        Q = Q+epsilon\n",
    "        divergence = np.sum(P*np.log(P/Q))\n",
    "        return np.asscalar(divergence)\n",
    "    ##Making sure the analysis ignore trash\n",
    "    cleanvar=var1.strip('http://').strip('https://').strip('www.')\n",
    "    len_var1=len(cleanvar)\n",
    "    idx_var1=cleanvar[:4]# Use \"root1\" as baseline #cleanvar.split(\"/\")[0]#cleanvar[:8]\n",
    "    list_values1 = list(cleanvar)\n",
    "    list_of_ord_values1 = [ord(char) for char in list_values1]\n",
    "    values1 = np.asarray(list_of_ord_values1)\n",
    "    # URLS with at least 4166 characters\n",
    "    ## TODO : Complete this list with common Malware/phishing sites used in internal webtraffic\n",
    "    list_values2 = var_match\n",
    "    list_values2 += var_match\n",
    "    ## Making sure the analysis start on the same webdomain name\n",
    "    idx_var2 = list_values2.find(idx_var1)\n",
    "    start_values2 = list_values2[idx_var2:(idx_var2+len_var1)]\n",
    "    list_values2 = list(start_values2+list_values2)\n",
    "    list_values2 = list_values2[:len_var1]\n",
    "    list_of_ord_values2 = [ord(char) for char in list_values2]\n",
    "    values2 = np.asarray(list_of_ord_values2)\n",
    "    return KL(values1,values2)\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "## NGRAM hash_message \"Kolmogorov-Smirnov test KS pvalue= ks_2samp function8.\n",
    "def func_ks_ngram_msg(var1,var_match):\n",
    "    ##Making sure the analysis of has_message SHA_512 is linear.\n",
    "    ##Making sure the analysis is over the words list on same size\n",
    "    cleanvar=var1.strip('http://').strip('https://').strip('www.')\n",
    "    len_var1=len(cleanvar)\n",
    "    idx_var1=cleanvar[:4]# Use \"root1\" as baseline #cleanvar.split(\"/\")[0]#cleanvar[:8]\n",
    "    list_values1 = list(cleanvar)\n",
    "    list_of_ord_values1 = [ord(char) for char in list_values1]\n",
    "    values1 = np.asarray(list_of_ord_values1)\n",
    "    values1= np.sort(values1)\n",
    "    # URLS with up to least 4166 characters\n",
    "    list_values2 = var_match\n",
    "    list_values2 += var_match\n",
    "    ## Making sure the analysis start on the same messgae\n",
    "    idx_var2 = list_values2.find(idx_var1)\n",
    "    start_values2 = list_values2[idx_var2:(idx_var2+len_var1)]\n",
    "    list_values2 = list(start_values2+list_values2)\n",
    "    list_values2 = list_values2[:len_var1]\n",
    "    list_of_ord_values2 = [ord(char) for char in list_values2]\n",
    "    values2 = np.asarray(list_of_ord_values2)\n",
    "    values2=np.sort(values2)\n",
    "    (Darray,pvalue)=stats.ks_2samp(values1, values2)\n",
    "    return np.asscalar(pvalue)\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "## Entropy TO hash_message function3.\n",
    "def func_entropy_ngram_msg(var1,var_match):\n",
    "    ##Making sure the analysis of has_message SHA_512 is linear.\n",
    "    ##Making sure the analysis is over the words list ignore size\n",
    "    #\n",
    "    cleanvar=var1.strip('http://').strip('https://').strip('www.')\n",
    "    len_var1=len(cleanvar)\n",
    "    idx_var1=cleanvar[:4]# Use \"root1\" as baseline #cleanvar.split(\"/\")[0]#cleanvar[:8]\n",
    "    list_values1 = list(cleanvar)\n",
    "    list_of_ord_values1 = [ord(char) for char in list_values1]\n",
    "    values1 = np.asarray(list_of_ord_values1)\n",
    "    # URLS with at least 4166 characters\n",
    "    ## TODO : Complete this list with an NGRAM search\n",
    "    list_values2 = var_match\n",
    "    list_values2 += var_match\n",
    "    ## Making sure the analysis start on point\n",
    "    idx_var2 = list_values2.find(idx_var1)\n",
    "    start_values2 = list_values2[idx_var2:(idx_var2+len_var1)]\n",
    "    list_values2 = list(start_values2+list_values2)\n",
    "    list_values2 = list_values2[:len_var1]\n",
    "    list_of_ord_values2 = [ord(char) for char in list_values2]\n",
    "    values2 = np.asarray(list_of_ord_values2)\n",
    "    pvalue=stats.entropy(values1, values2)\n",
    "    scalar_pvalue=np.asscalar(pvalue)\n",
    "    return scalar_pvalue\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "func_kl_ngram_msg_udf = udf(func_kl_ngram_msg, FloatType())\n",
    "func_ks_ngram_msg_udf = udf(func_ks_ngram_msg, FloatType())\n",
    "func_entropy_ngram_msg_udf = udf(func_entropy_ngram_msg, FloatType())\n",
    "#\n",
    "# Arguments\n",
    "#\n",
    "#import argparse\n",
    "## Parse date_of execution\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--datev1\", help=\"Execution Date\")\n",
    "#args = parser.parse_args()\n",
    "#if args.datev1:\n",
    "#    processdate = args.datev1\n",
    "# GENERAL PREPARATION SCRIPT\n",
    "#\n",
    "#  Date in format YYYYMMDD\n",
    "#process_date = processdate\n",
    "#if not process_date:\n",
    "#    process_date = \"20190122\"\n",
    "#\n",
    "process_date = \"20190206\"\n",
    "#\n",
    "sc = pyspark.SparkContext(appName=\"FraudCanada-Model-NGrams-CountVectorizer-KL-KS-Entropy-Model-Labeling\")\n",
    "sqlContext = SQLContext(sc)\n",
    "#\n",
    "#\n",
    "input_most_frequent_fraud_df=\"hdfs:///data/staged/ott_dazn/advanced-model-data/the-most-frequent-fraud-hash_message/dt=\"+process_date\n",
    "#\n",
    "input_most_frequent_notfraud_df=\"hdfs:///data/staged/ott_dazn/advanced-model-data/the-most-frequent-notfraud-hash_message/dt=\"+process_date\n",
    "#\n",
    "input_fraud=\"hdfs:///data/staged/ott_dazn/advanced-model-data/fraud-notfraud-canada-tokenizedwords-ngrams-7-features-85/dt=\"+process_date\n",
    "#\n",
    "input_file1=\"hdfs:///data/staged/ott_dazn/advanced-model-data/fraud-notfraud-canada-tokenizedwords-ngrams-7-features-85/dt=\"+process_date\n",
    "#\n",
    "output_file1=\"hdfs:///data/staged/ott_dazn/advanced-model-data/label-fraud-notfraud-data-model/dt=\"+process_date\n",
    "#\n",
    "preserve_training_output_file=\"hdfs:///data/staged/ott_dazn/advanced-model-data/preserve-training-output-automl-clean/dt=\"+process_date\n",
    "#\n",
    "# The most Frequent would the ones with the max frequency of NGrams85 tokens\n",
    "pd.options.display.max_colwidth = 512\n",
    "#\n",
    "standard_fraud_ngram=sqlContext.read.json(input_most_frequent_fraud_df)\\\n",
    ".orderBy(col('count').desc()).select(col('hash_message')).limit(1).toPandas()['hash_message'][0]\n",
    "print(\"Value UDF : standard_fraud_ngram=\")\n",
    "print(standard_fraud_ngram)\n",
    "#\n",
    "# Select Tokens/words form the max frequency of NGrams85 tokens hash_message\n",
    "standard_fraud_ngram_words=sqlContext.read.json(input_fraud)\n",
    "standard_fraud_ngram_words.printSchema()\n",
    "#\n",
    "standard_fraud_words_search=standard_fraud_ngram_words\\\n",
    ".withColumn('fraud_master_hash',lit(standard_fraud_ngram).cast('string'))\\\n",
    ".filter(\" hash_message=fraud_master_hash \")\n",
    "standard_fraud_words_search.printSchema()\n",
    "#\n",
    "standard_fraud_words=standard_fraud_words_search\\\n",
    ".withColumn('words_conc',F.concat_ws('',col('words')).cast('string'))\\\n",
    ".select(col('words_conc')).limit(1).toPandas()['words_conc'][0] \n",
    "#\n",
    "print(\"Value UDF : standard_fraud_words=\")\n",
    "print(standard_fraud_words)\n",
    "#\n",
    "# Working now!\n",
    "# Needs to be bigger than message twice should be tested!\n",
    "#   Py4JJavaError: An error occurred while calling z:org.apache.spark.sql.functions.lit.\n",
    "#      : java.lang.RuntimeException: Unsupported literal type class java.util.HashMap \n",
    "# {0=root15c466c8e6e8f9d17adb73426cd55c70f72b9f18e39e3455c9043a18b86b122b6requestmethodgeturlhttpsislcadazncommislv2playbackassetidfg5oon8sl71n1nfwuegbo8npgeventidarticleidfg5oon8sl71n1nfwuegbo8npgformatmpegdashplayeriddaznf3874e050812a853securetruelanguagecodeenlatitudenulllongitudenullplatformandroidtvmanufacturernvidiamodelnullmtalanguagecodeenclientip50100225179headersuseragentmozilla50linuxandroid800shieldandroidtvbuildopr6170623010wvapplewebkit53736khtmllikegeckoversion40chrome710357899mobilesafari53736fev1420typeinresponsestatuscode200reasonphraseokduration47jwtvieweridc2ebc25d8085deviceid993bf365c72c4b0b9168c2ebc25d8085f3874e050812a853userstatusactivepaid}\n",
    "#\n",
    "standard_notfraud_ngram=sqlContext.read.json(input_most_frequent_notfraud_df)\\\n",
    ".orderBy(col('count').desc()).select(col('hash_message')).limit(1).toPandas()['hash_message'][0]\n",
    "print(\"Value UDF : standard_notfraud_ngram=\")\n",
    "print(standard_notfraud_ngram)\n",
    "#\n",
    "# Select Tokens/words form the max frequency of NGrams85 tokens hash_message\n",
    "standard_notfraud_ngram_words=sqlContext.read.json(input_fraud)\n",
    "standard_notfraud_ngram_words.printSchema()\n",
    "#\n",
    "standard_notfraud_words_search=standard_notfraud_ngram_words\\\n",
    ".withColumn('fraud_master_hash',lit(standard_notfraud_ngram).cast('string'))\\\n",
    ".filter(\" hash_message=fraud_master_hash \")\n",
    "standard_notfraud_words_search.printSchema()\n",
    "#\n",
    "standard_notfraud_words=standard_notfraud_words_search\\\n",
    ".withColumn('words_conc',F.concat_ws('',col('words')).cast('string'))\\\n",
    ".select(col('words_conc')).limit(1).toPandas()['words_conc'][0] \n",
    "#\n",
    "print(\"Value UDF : standard_notfraud_words=\")\n",
    "print(standard_notfraud_words)\n",
    "#\n",
    "ngram7_fraud=sqlContext.read.json(input_file1)\n",
    "ngram7_fraud.printSchema()\n",
    "#\n",
    "drop_phish_cols=['words','ngrams_7']\n",
    "#\n",
    "fraud_label_read_df=ngram7_fraud.filter(\"hash_message is not NULL\")\\\n",
    ".withColumn('words_conc',F.concat_ws('',col('words')).cast('string'))\\\n",
    ".drop(*drop_phish_cols)\\\n",
    ".withColumn('kl_fraud_words',func_kl_ngram_msg_udf(col('words_conc'),lit(standard_fraud_words).cast('string')).cast('double'))\\\n",
    ".withColumn('ks_fraud_words',func_ks_ngram_msg_udf(col('words_conc'),lit(standard_fraud_words).cast('string')).cast('double'))\\\n",
    ".withColumn('entropy_fraud_words',func_entropy_ngram_msg_udf(col('words_conc'),lit(standard_fraud_words).cast('string')).cast('double'))\\\n",
    ".withColumn('kl_notfraud_words',func_kl_ngram_msg_udf(col('words_conc'),lit(standard_notfraud_words).cast('string')).cast('double'))\\\n",
    ".withColumn('ks_notfraud_words',func_ks_ngram_msg_udf(col('words_conc'),lit(standard_notfraud_words).cast('string')).cast('double'))\\\n",
    ".withColumn('entropy_notfraud_words',func_entropy_ngram_msg_udf(col('words_conc'),lit(standard_notfraud_words).cast('string')).cast('double'))\\\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "fraud_label_read_df.printSchema()\n",
    "#\n",
    "fraud_label_read_df.coalesce(1).write.json(output_file1)\n",
    "#\n",
    "fraud_label_read_df.unpersist()\n",
    "#\n",
    "sc.stop()\n",
    "#\n",
    "print(\"Model Data KL,KS, Entropy Done! NGrams Vectors Data Done!\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: java version \"1.8.0_172\"; Java(TM) SE Runtime Environment (build 1.8.0_172-b11); Java HotSpot(TM) 64-Bit Server VM (build 25.172-b11, mixed mode)\n",
      "  Starting server from /opt/cloudera/parcels/Anaconda-5.1.0.1/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmp9s14p_qx\n",
      "  JVM stdout: /tmp/tmp9s14p_qx/h2o_siemanalyst_started_from_python.out\n",
      "  JVM stderr: /tmp/tmp9s14p_qx/h2o_siemanalyst_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n",
      "Warning: Your H2O cluster version is too old (3 months and 10 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.22.0.1</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>3 months and 10 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_siemanalyst_mw3hay</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>5.333 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>12</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>6</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.4 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         01 secs\n",
       "H2O cluster timezone:       UTC\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.22.0.1\n",
       "H2O cluster version age:    3 months and 10 days !!!\n",
       "H2O cluster name:           H2O_from_python_siemanalyst_mw3hay\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    5.333 Gb\n",
       "H2O cluster total cores:    12\n",
       "H2O cluster allowed cores:  6\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.4 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training Model NGrams Vectors KS KL Entropty\n",
      "root\n",
      " |-- entropy_fraud_words: double (nullable = true)\n",
      " |-- entropy_notfraud_words: double (nullable = true)\n",
      " |-- features_85: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- fraud_label: long (nullable = true)\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- kl_fraud_words: double (nullable = true)\n",
      " |-- kl_notfraud_words: double (nullable = true)\n",
      " |-- ks_fraud_words: double (nullable = true)\n",
      " |-- ks_notfraud_words: double (nullable = true)\n",
      " |-- ngramscounts_7: struct (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- size: long (nullable = true)\n",
      " |    |-- type: long (nullable = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- words_conc: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- kl_fraud_words: double (nullable = true)\n",
      " |-- ks_fraud_words: double (nullable = true)\n",
      " |-- entropy_fraud_words: double (nullable = true)\n",
      " |-- kl_notfraud_words: double (nullable = true)\n",
      " |-- ks_notfraud_words: double (nullable = true)\n",
      " |-- entropy_notfraud_words: double (nullable = true)\n",
      " |-- features85_type: long (nullable = true)\n",
      " |-- features85_size: long (nullable = true)\n",
      " |-- features85_indices: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- features85_values: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- ngramscounts7_type: long (nullable = true)\n",
      " |-- ngramscounts7_size: long (nullable = true)\n",
      " |-- ngramscounts7_indices: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- ngramscounts7_values: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- kl_fraud_words: double (nullable = true)\n",
      " |-- ks_fraud_words: double (nullable = true)\n",
      " |-- entropy_fraud_words: double (nullable = true)\n",
      " |-- kl_notfraud_words: double (nullable = true)\n",
      " |-- ks_notfraud_words: double (nullable = true)\n",
      " |-- entropy_notfraud_words: double (nullable = true)\n",
      " |-- features85_type: long (nullable = true)\n",
      " |-- features85_size: long (nullable = true)\n",
      " |-- features85_indices: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- features85_values: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- ngramscounts7_type: long (nullable = true)\n",
      " |-- ngramscounts7_size: long (nullable = true)\n",
      " |-- ngramscounts7_indices: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- ngramscounts7_values: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- kl_fraud_words: double (nullable = true)\n",
      " |-- ks_fraud_words: double (nullable = true)\n",
      " |-- entropy_fraud_words: double (nullable = true)\n",
      " |-- kl_notfraud_words: double (nullable = true)\n",
      " |-- ks_notfraud_words: double (nullable = true)\n",
      " |-- entropy_notfraud_words: double (nullable = true)\n",
      " |-- features85_type: long (nullable = true)\n",
      " |-- features85_size: long (nullable = true)\n",
      " |-- features85_indices: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- features85_values: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- ngramscounts7_type: long (nullable = true)\n",
      " |-- ngramscounts7_size: long (nullable = true)\n",
      " |-- ngramscounts7_indices: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- ngramscounts7_values: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n",
      "Parse progress: |âââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n",
      "Parse progress: |âââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n",
      "Parse progress: |âââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n",
      "Parse progress: |âââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n",
      "train\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>hash_message                                                                                                                    </th><th style=\"text-align: right;\">  fraud_label</th><th style=\"text-align: right;\">  kl_fraud_words</th><th style=\"text-align: right;\">  ks_fraud_words</th><th style=\"text-align: right;\">  entropy_fraud_words</th><th style=\"text-align: right;\">  kl_notfraud_words</th><th style=\"text-align: right;\">  ks_notfraud_words</th><th style=\"text-align: right;\">  entropy_notfraud_words</th><th style=\"text-align: right;\">  features85_type</th><th style=\"text-align: right;\">  features85_size</th><th style=\"text-align: right;\">  ngramscounts7_type</th><th style=\"text-align: right;\">  ngramscounts7_size</th><th>features85_list_indices  </th><th>features85_list_values  </th><th>ngramscounts7_list_indices  </th><th>ngramscounts7_list_values  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>45c8664c2595ec4eb69d2794510a6b2709cd4c7376424543ff7d1a856a9fd264dcf2acf79c229d1735ece2b0293a26e1f7f06977b09d503ee3ecbf79f41fcfe1</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2958.18</td><td style=\"text-align: right;\">        0.680909</td><td style=\"text-align: right;\">            0.0414306</td><td style=\"text-align: right;\">            4174.65</td><td style=\"text-align: right;\">           0.576074</td><td style=\"text-align: right;\">               0.0702108</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    164    165    166    295    544    545    546   1940   1956   1961\n",
       "   1962   3995  25796  28975  42401  54583  55521  57339  59314  60189\n",
       "  62079  64022  70148  98667 102744 104261 107736 108682 112715 156710\n",
       " 209253 213413]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>51fd93b4b44009b433bc5a23f288031224ea54e53126895967ce64385c261256352efd51bc8d8daabe1662b039b8c4777d86e9c5a792efabbbc3c86c3abce0cd</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2494.88</td><td style=\"text-align: right;\">        0.990999</td><td style=\"text-align: right;\">            0.0387373</td><td style=\"text-align: right;\">            3931.17</td><td style=\"text-align: right;\">           0.830355</td><td style=\"text-align: right;\">               0.0720945</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    707    842    843    844   3094  10390  10429  10458  10548  11708\n",
       "  21959  29074  68837  71482  78795  86039  90805  91804  92815  93940\n",
       "  95051  95231 100051 107944 112029 112432 135667 154706 198073 243058]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>a718ac829139f11b98d9480c89f8bdb0d245093ca42fb35383f5c863168388745996497a2739b53d376d929dd0cc1c2381c398c088f0c958614514c07ecc3050</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         3084.28</td><td style=\"text-align: right;\">        0.831228</td><td style=\"text-align: right;\">            0.0472219</td><td style=\"text-align: right;\">            3671.83</td><td style=\"text-align: right;\">           0.629505</td><td style=\"text-align: right;\">               0.0636913</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    164    165    166    295    707   2561   3630   3724   3753  14203\n",
       "  14267  15388  26219  26988  31068  47005  48996  49224  49470  52427\n",
       "  60632  61270  76692  83576  91789  93571  96977 104828 113156 251060]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>022f6b81a53eb05b037dbb0cad11d846daa9d5d35982585da98f1499f01eaddd442a465c91b3eba97cfa45ef374da9a0644f4b0808669ac92bd7f095a17d9872</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         3156.26</td><td style=\"text-align: right;\">        0.784173</td><td style=\"text-align: right;\">            0.0474292</td><td style=\"text-align: right;\">            3850.26</td><td style=\"text-align: right;\">           0.682076</td><td style=\"text-align: right;\">               0.0660141</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    164    165    166    295    607    608    609   1479   2602   2623\n",
       "   2639   3230   4829   4840   4857   4988   4992   5090   5121   5122\n",
       "  17967  19419  50604  52103  56467  57847  60870  66283  68374 119160\n",
       " 120926 161483 174050]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>d56bb4817065322929588e81c018d9deed1afb06eafacdf07fa70aec1f3d89114ace3f6c8ffe70662f92f0f01f7d4a351f9aacd334818e8e9f04bb49ce2e667a</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2413.15</td><td style=\"text-align: right;\">        1       </td><td style=\"text-align: right;\">            0.0464608</td><td style=\"text-align: right;\">            3265.06</td><td style=\"text-align: right;\">           0.947901</td><td style=\"text-align: right;\">               0.0643643</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    816    956    957    959    961    962    967   1502   1503   1509\n",
       "   4068   9378   9952  26504  34176  35386  39360  42901  43116  44939\n",
       "  45653  69750  72884  77314  94442 100242 101391 108129 173932 178104\n",
       " 181561 199157 231332 245371]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>ff74d13e4d3333f00aea89ab8863c5678dafe684abac29e40c1809f16cc7b776b75c627fca96e83a21d5af5c2f9e32a23092625768cc0c63c9b1f03f397d182a</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         3356.81</td><td style=\"text-align: right;\">        0.784173</td><td style=\"text-align: right;\">            0.0481377</td><td style=\"text-align: right;\">            3994.56</td><td style=\"text-align: right;\">           0.682076</td><td style=\"text-align: right;\">               0.0655579</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    607    608    609   2710   3089   5220   5279   5472   7503   8630\n",
       "   9713  34342  46644  47164  51476  52017  53102  55085  55189  56775\n",
       "  66021  68244  78771  92124  95114  99551 108687 108967 111745 157777\n",
       " 235095 256509]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>b01dc8c3adf8593c9db23a88e61177cfbc041a1a7c32e1e3f1f928f16650f8b9e58f4313e52046dbdce1f026f20b5c02d981ea3b304ca6dff99ac2ff40d79e83</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2706.61</td><td style=\"text-align: right;\">        0.873126</td><td style=\"text-align: right;\">            0.0369048</td><td style=\"text-align: right;\">            4099.7 </td><td style=\"text-align: right;\">           0.680909</td><td style=\"text-align: right;\">               0.069231 </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    842    843    844   1311   1313   1317   2008   2135   3760   8782\n",
       "  10257  10631  26393  28179  29261  30410  30640  31119  31902  37875\n",
       "  38126  38847  38932  44043  45609  46548  68517  82375 106990 151197]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>2cb9fff98bf842481b752665424454a9a44e7f8c5c2c6c62a3bd5119d09fd997b1edb4152e84c10dc9c23b04c10b71c852bc647288a0e73b564684ba90d3dd1e</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2974.66</td><td style=\"text-align: right;\">        0.999236</td><td style=\"text-align: right;\">            0.0497925</td><td style=\"text-align: right;\">            3647.62</td><td style=\"text-align: right;\">           0.751974</td><td style=\"text-align: right;\">               0.0637668</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    681    956    957    959    961    962    967   1616   1868   3712\n",
       "   3774   3788   3818   3827   3828   3829  10203  11369  11717  16102\n",
       "  18329  69207  69668  93188  93770  97752 105401 108418 137510 141636\n",
       " 191923 229533 239815]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>5906909ba80934995ceca933c96c1b0e8c9f4f0de55b148a2d7b6ed2b16fe03fd74c7567de969bedaa8c0ce70aa19f5d8dbacc9a1b8b5371531a9d3075c0cc37</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2439.39</td><td style=\"text-align: right;\">        0.98069 </td><td style=\"text-align: right;\">            0.0336585</td><td style=\"text-align: right;\">            3969.97</td><td style=\"text-align: right;\">           0.783181</td><td style=\"text-align: right;\">               0.0687975</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    570    571    572   1010   1012   1258   1259   1260   1262   1263\n",
       "   1266   1273   1735   2296   3001   3144   3159   3163   3179   3180\n",
       "   3222   3232  10492  11244  11575  12249  41457  42221  46607 138750\n",
       " 225290 255276]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>e914ef9af4936eed853cb4d10d52b502dc617e12f000b7cc2cd25e0bf0e4cc6069f9194b2f3515cba08b28a4df8b940f91400f9af043b94bc9d746fba7b800e1</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2325.07</td><td style=\"text-align: right;\">        0.941236</td><td style=\"text-align: right;\">            0.0402253</td><td style=\"text-align: right;\">            3454.34</td><td style=\"text-align: right;\">           0.682076</td><td style=\"text-align: right;\">               0.0677211</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    299    300    301    570    571    572    578   1279   1789  17507\n",
       "  19101  23530  26477  26807  29308  29486  29523  29752  31258  33975\n",
       "  40742  41227  42594  43395  45134  45260  55929  63691  64266 148676\n",
       " 211504]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>hash_message                                                                                                                    </th><th style=\"text-align: right;\">  fraud_label</th><th style=\"text-align: right;\">  kl_fraud_words</th><th style=\"text-align: right;\">  ks_fraud_words</th><th style=\"text-align: right;\">  entropy_fraud_words</th><th style=\"text-align: right;\">  kl_notfraud_words</th><th style=\"text-align: right;\">  ks_notfraud_words</th><th style=\"text-align: right;\">  entropy_notfraud_words</th><th style=\"text-align: right;\">  features85_type</th><th style=\"text-align: right;\">  features85_size</th><th style=\"text-align: right;\">  ngramscounts7_type</th><th style=\"text-align: right;\">  ngramscounts7_size</th><th>features85_list_indices  </th><th>features85_list_values  </th><th>ngramscounts7_list_indices  </th><th>ngramscounts7_list_values  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>101642031f7aa7d62d25fbc9dc190f6165b1efd53d20eea9a7405968fc32ba6da3db74ea520bd5910bc52db1acc8f3bb069e415572a344186ab52fb0aa0211d2</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2703.02</td><td style=\"text-align: right;\">        0.83296 </td><td style=\"text-align: right;\">            0.0480958</td><td style=\"text-align: right;\">            3235.29</td><td style=\"text-align: right;\">           0.736194</td><td style=\"text-align: right;\">               0.0631322</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "   1192   1193   1195   1972   3168   4048  20261  21615  22667  23930\n",
       "  24166  51800  53721  56262  60340  61638  63487  65066  71064  77803\n",
       "  80020  81356  82272  82280  86026  89190 165716 170455 187454 235691\n",
       " 255607]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>a7250368bd2f3d9e0ecbdd03354d7196b83a2a40013c748face8f747e0443fdf0fd6c22a588d1795c16b455c7039cf657ecf5bcdfde08fbe54669045033d24f1</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         3327.77</td><td style=\"text-align: right;\">        0.963607</td><td style=\"text-align: right;\">            0.0489203</td><td style=\"text-align: right;\">            4471.18</td><td style=\"text-align: right;\">           0.730739</td><td style=\"text-align: right;\">               0.0765263</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    842    843    844   2815   6052   6740   6792   6985  12328  12413\n",
       "  22038  23607  25439  27477  29093  31782  32434  32787  33325  70710\n",
       "  71694  78589  80537  87234 104781 105971 106868 108578 112657 211966\n",
       " 217414 242542]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>cf6aaf9aa72e0f8ebdbbab8db1e5e6c2d9ffac5bc954ae0e057099e0dd6464ee2136042556445063e81c1a4107c2d0a27234da61e648541391b2a904456a005a</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2434.66</td><td style=\"text-align: right;\">        0.980881</td><td style=\"text-align: right;\">            0.0417387</td><td style=\"text-align: right;\">            3292.28</td><td style=\"text-align: right;\">           0.784173</td><td style=\"text-align: right;\">               0.06374  </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    164    165    166    295    681   1296   1298   1299   1616   1868\n",
       "   3712   3774   3788   3797   3811   3818   3827   3828   3829   3843\n",
       "  17132  18494  36389  36416  36515  38711  41234  45850  46554 120634\n",
       " 124787 225931 242249]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>5971b987ced86878931381142fdc479e723acb6637f637cb30156e81f88f06d53cd0f9bbf7180b9d710380c699c05f6d58ab1c23e640e231d803d11dfd7d1cd1</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2424.23</td><td style=\"text-align: right;\">        0.964228</td><td style=\"text-align: right;\">            0.0339223</td><td style=\"text-align: right;\">            4167.72</td><td style=\"text-align: right;\">           0.732933</td><td style=\"text-align: right;\">               0.0733494</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    607    608    609    816   2640   4105   4159   4209   6418  10263\n",
       "  10543  10995  11252  11343  11344  11403  11711  17186  17318  36214\n",
       "  38184  41546  42908  43180  43737  44097 120764 132602 134109 214651\n",
       " 251957]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>824d5fca02966660c51d6bd8309e3caa8e61e934cc791ff7e7b9fb4ce353106cc141cf9c245e3ede6a74e517a52ec858be0804d43eb8394c605eb4019a1e4270</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         3343.64</td><td style=\"text-align: right;\">        0.874593</td><td style=\"text-align: right;\">            0.0505059</td><td style=\"text-align: right;\">            3803.02</td><td style=\"text-align: right;\">           0.785161</td><td style=\"text-align: right;\">               0.0643405</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    164    165    166    295    570    571    572   2141   2148   2152\n",
       "   2172   3034   6787   8100   8160   8244   8685   8730   8789   8855\n",
       "  25997  28927 121203 179164 186793 213852 231031 231652 244436 252102]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>c6651b9ecde7ba7e9314d46222d1b2138d627bc57cb531fb5b65d8963d13880a094b11c8cc3cd48a4508ee8b5f3c91a8473eaa94e53d8307c8165e6de623de99</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2610.08</td><td style=\"text-align: right;\">        0.627063</td><td style=\"text-align: right;\">            0.0335777</td><td style=\"text-align: right;\">            4312.02</td><td style=\"text-align: right;\">           0.523838</td><td style=\"text-align: right;\">               0.072228 </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    607    608    609   1071   1072   1075   2087   2369   3230  12075\n",
       "  12724  20788  21799  22081  24066  24595  24601  24887  39956  68716\n",
       "  68887  72674  88029  89841  94006 101079 137134 141428 170932 211251\n",
       " 227551 228281 229625]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>bd6ff0f84372e6ca986f1a5fbb38f6fe3d3a885973c5c4fb643108b8f95f4e5e21da53ad1f97fe4c4dc489b445504ab551244e7686024adc01b8dc9d2030bb17</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         3089.51</td><td style=\"text-align: right;\">        0.38286 </td><td style=\"text-align: right;\">            0.0349989</td><td style=\"text-align: right;\">            4674.31</td><td style=\"text-align: right;\">           0.303823</td><td style=\"text-align: right;\">               0.0712006</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 36 38 39 40 41 42 43 44 45 46 47 48\n",
       " 49 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    544    545    546    707   1656   3166   3217   3224   3751   4634\n",
       "  13797  13924  14085  14958  15143  15212  15656  16132  16234  36285\n",
       "  36664  36931  37579  40774  43661  46324  70793  81994  89918 194247\n",
       " 201925]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>207103be4212a074b3ef483fab2083bc7be04f273a953853b512dc5aac5911bcd68e350f3b75dccf1e0cf9b5521703fa2c6e069504656453fc261376bf4ce972</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         3435.29</td><td style=\"text-align: right;\">        0.780179</td><td style=\"text-align: right;\">            0.0461717</td><td style=\"text-align: right;\">            4561.62</td><td style=\"text-align: right;\">           0.624608</td><td style=\"text-align: right;\">               0.0733706</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[    0     1     2     3     4     5     6     7     8     9    10    11\n",
       "    13    14    15    48    49    50    51    52    90    91    92    93\n",
       "    94    95   115   124   125   136   137   138   139   140   141   142\n",
       "   143   144   145   146   147   148   155   156   157   158   159   160\n",
       "   161   162   842   843   844  1010  1012  1279  1941  3761  3764  3771\n",
       "  3792  3849  3883  3891  7949  8941  9473  9502  9811  9853  9905 10075\n",
       " 25389 26731 29016 57023]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>3557520b1295c0ed3541163e88d7bc43e7084444031e2fe487277ca211677941062d4f2a346792c45c68ce388a0341e99cca1dbc7f7288bc475de3ed26ed7587</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2992.53</td><td style=\"text-align: right;\">        0.964534</td><td style=\"text-align: right;\">            0.0430735</td><td style=\"text-align: right;\">            3852.81</td><td style=\"text-align: right;\">           0.831228</td><td style=\"text-align: right;\">               0.064967 </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    164    165    166    295    842    843    844   1354   2296   5120\n",
       "   8931   9339   9990  10548  11708  11817  12198  12231  12406  12760\n",
       "  13044  13318  35608  35985  37861  42834  43587  45675  45899 145600\n",
       " 150444 226786]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>503a11e7c1393852838272468342f4db14504f4fe077ada30e196dc2c15a8d99265dcdc73e86c33a7ea8eec1e2333a209997cd44bbfe8e5b6a5d8ff439e2f180</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2981.36</td><td style=\"text-align: right;\">        0.830355</td><td style=\"text-align: right;\">            0.0385299</td><td style=\"text-align: right;\">            4289.69</td><td style=\"text-align: right;\">           0.5251  </td><td style=\"text-align: right;\">               0.0690645</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    570    571    572   1841   3995   8015   8686   8767   9256  10412\n",
       "  10646  10721  10985  11177  11564  11578  11751  13427  18261  28918\n",
       "  29472  30375  31210  31285  31376  33150 126502 131074 158352 180321\n",
       " 250518]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AutoML progress: |ââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n",
      "AutoML Modeling Done!\n",
      "['GBM_grid_1_AutoML_20190206_120744_model_2', 'GBM_grid_1_AutoML_20190206_120744_model_8', 'GBM_grid_1_AutoML_20190206_120744_model_7', 'GBM_grid_1_AutoML_20190206_120744_model_9', 'GBM_grid_1_AutoML_20190206_120744_model_1', 'GBM_1_AutoML_20190206_120744', 'StackedEnsemble_AllModels_AutoML_20190206_120744', 'DeepLearning_grid_1_AutoML_20190206_120744_model_2', 'GBM_grid_1_AutoML_20190206_120744_model_6', 'StackedEnsemble_BestOfFamily_AutoML_20190206_120744', 'GBM_grid_1_AutoML_20190206_120744_model_10', 'GBM_grid_1_AutoML_20190206_120744_model_5', 'GBM_4_AutoML_20190206_120744', 'GBM_2_AutoML_20190206_120744', 'DeepLearning_1_AutoML_20190206_120744', 'GBM_3_AutoML_20190206_120744', 'GBM_grid_1_AutoML_20190206_120744_model_3', 'GBM_5_AutoML_20190206_120744', 'DeepLearning_grid_1_AutoML_20190206_120744_model_10', 'DeepLearning_grid_1_AutoML_20190206_120744_model_3', 'DeepLearning_grid_1_AutoML_20190206_120744_model_5', 'DeepLearning_grid_1_AutoML_20190206_120744_model_7', 'DeepLearning_grid_1_AutoML_20190206_120744_model_8', 'DeepLearning_grid_1_AutoML_20190206_120744_model_4', 'DeepLearning_grid_1_AutoML_20190206_120744_model_1', 'GBM_grid_1_AutoML_20190206_120744_model_4', 'DeepLearning_grid_1_AutoML_20190206_120744_model_6', 'DeepLearning_grid_1_AutoML_20190206_120744_model_9']\n",
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_AutoML_20190206_120744\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 3.695793677649106e-07\n",
      "RMSE: 0.0006079303971384476\n",
      "LogLoss: 0.00011449714348936241\n",
      "Null degrees of freedom: 4133\n",
      "Residual degrees of freedom: 4107\n",
      "Null deviance: 5187.6493944424765\n",
      "Residual deviance: 0.9466623823700534\n",
      "AIC: 54.94666238237005\n",
      "AUC: 1.0\n",
      "pr_auc: 0.6500754147812972\n",
      "Gini: 1.0\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.9992603541082276: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>2808.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/2808.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.0</td>\n",
       "<td>1326.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/1326.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>2808.0</td>\n",
       "<td>1326.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/4134.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1     Error    Rate\n",
       "-----  ----  ----  -------  ------------\n",
       "0      2808  0     0        (0.0/2808.0)\n",
       "1      0     1326  0        (0.0/1326.0)\n",
       "Total  2808  1326  0        (0.0/4134.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.9992604</td>\n",
       "<td>1.0</td>\n",
       "<td>246.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.9992604</td>\n",
       "<td>1.0</td>\n",
       "<td>246.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.9992604</td>\n",
       "<td>1.0</td>\n",
       "<td>246.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.9992604</td>\n",
       "<td>1.0</td>\n",
       "<td>246.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.9992604</td>\n",
       "<td>1.0</td>\n",
       "<td>246.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.9992604</td>\n",
       "<td>1.0</td>\n",
       "<td>246.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.9992604</td>\n",
       "<td>1.0</td>\n",
       "<td>246.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.9992604</td>\n",
       "<td>1.0</td>\n",
       "<td>246.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value    idx\n",
       "---------------------------  -----------  -------  -----\n",
       "max f1                       0.99926      1        246\n",
       "max f2                       0.99926      1        246\n",
       "max f0point5                 0.99926      1        246\n",
       "max accuracy                 0.99926      1        246\n",
       "max precision                0.999914     1        0\n",
       "max recall                   0.99926      1        246\n",
       "max specificity              0.999914     1        0\n",
       "max absolute_mcc             0.99926      1        246\n",
       "max min_per_class_accuracy   0.99926      1        246\n",
       "max mean_per_class_accuracy  0.99926      1        246"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 32.08 %, avg score: 32.08 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0101597</td>\n",
       "<td>0.9999139</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>0.0316742</td>\n",
       "<td>0.0316742</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200774</td>\n",
       "<td>0.9999139</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>0.0309201</td>\n",
       "<td>0.0625943</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0302371</td>\n",
       "<td>0.9999139</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>0.0316742</td>\n",
       "<td>0.0942685</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0401548</td>\n",
       "<td>0.9999139</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>0.0309201</td>\n",
       "<td>0.1251885</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500726</td>\n",
       "<td>0.9999139</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>0.0309201</td>\n",
       "<td>0.1561086</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1001451</td>\n",
       "<td>0.9999139</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>0.1561086</td>\n",
       "<td>0.3122172</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1502177</td>\n",
       "<td>0.9999137</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999138</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>0.1561086</td>\n",
       "<td>0.4683258</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2002903</td>\n",
       "<td>0.9999129</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999135</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999138</td>\n",
       "<td>0.1561086</td>\n",
       "<td>0.6244344</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3001935</td>\n",
       "<td>0.9997585</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998787</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999021</td>\n",
       "<td>0.3114630</td>\n",
       "<td>0.9358974</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.3998549</td>\n",
       "<td>0.0000678</td>\n",
       "<td>0.6432039</td>\n",
       "<td>2.5009074</td>\n",
       "<td>0.2063107</td>\n",
       "<td>0.2066093</td>\n",
       "<td>0.8021779</td>\n",
       "<td>0.8021788</td>\n",
       "<td>0.0641026</td>\n",
       "<td>1.0</td>\n",
       "<td>-35.6796117</td>\n",
       "<td>150.0907441</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.4997581</td>\n",
       "<td>0.0000673</td>\n",
       "<td>0.0</td>\n",
       "<td>2.0009681</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000674</td>\n",
       "<td>0.6418199</td>\n",
       "<td>0.6418341</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>100.0968054</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999032</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.0</td>\n",
       "<td>1.6669355</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000673</td>\n",
       "<td>0.5346774</td>\n",
       "<td>0.5347005</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>66.6935484</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6998065</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.0</td>\n",
       "<td>1.4289665</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.4583477</td>\n",
       "<td>0.4583771</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>42.8966471</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999516</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.0</td>\n",
       "<td>1.2500756</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.4009676</td>\n",
       "<td>0.4010018</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>25.0075597</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8998549</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1112903</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.3564516</td>\n",
       "<td>0.3564894</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1290323</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.3207547</td>\n",
       "<td>0.3207955</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0101597                   0.999914           3.11765   3.11765            1                0.999914     1                           0.999914            0.0316742       0.0316742                  211.765   211.765\n",
       "    2        0.0200774                   0.999914           3.11765   3.11765            1                0.999914     1                           0.999914            0.0309201       0.0625943                  211.765   211.765\n",
       "    3        0.0302371                   0.999914           3.11765   3.11765            1                0.999914     1                           0.999914            0.0316742       0.0942685                  211.765   211.765\n",
       "    4        0.0401548                   0.999914           3.11765   3.11765            1                0.999914     1                           0.999914            0.0309201       0.125189                   211.765   211.765\n",
       "    5        0.0500726                   0.999914           3.11765   3.11765            1                0.999914     1                           0.999914            0.0309201       0.156109                   211.765   211.765\n",
       "    6        0.100145                    0.999914           3.11765   3.11765            1                0.999914     1                           0.999914            0.156109        0.312217                   211.765   211.765\n",
       "    7        0.150218                    0.999914           3.11765   3.11765            1                0.999914     1                           0.999914            0.156109        0.468326                   211.765   211.765\n",
       "    8        0.20029                     0.999913           3.11765   3.11765            1                0.999913     1                           0.999914            0.156109        0.624434                   211.765   211.765\n",
       "    9        0.300194                    0.999758           3.11765   3.11765            1                0.999879     1                           0.999902            0.311463        0.935897                   211.765   211.765\n",
       "    10       0.399855                    6.77546e-05        0.643204  2.50091            0.206311         0.206609     0.802178                    0.802179            0.0641026       1                          -35.6796  150.091\n",
       "    11       0.499758                    6.72901e-05        0         2.00097            0                6.7403e-05   0.64182                     0.641834            0               1                          -100      100.097\n",
       "    12       0.599903                    6.72469e-05        0         1.66694            0                6.72637e-05  0.534677                    0.5347              0               1                          -100      66.6935\n",
       "    13       0.699806                    6.7235e-05         0         1.42897            0                6.72403e-05  0.458348                    0.458377            0               1                          -100      42.8966\n",
       "    14       0.799952                    6.72302e-05        0         1.25008            0                6.72323e-05  0.400968                    0.401002            0               1                          -100      25.0076\n",
       "    15       0.899855                    6.72277e-05        0         1.11129            0                6.72287e-05  0.356452                    0.356489            0               1                          -100      11.129\n",
       "    16       1                           6.72262e-05        0         1                  0                6.72269e-05  0.320755                    0.320795            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 7.3668186430933815e-06\n",
      "RMSE: 0.00271418839491539\n",
      "LogLoss: 0.00024109219178066542\n",
      "Null degrees of freedom: 435\n",
      "Residual degrees of freedom: 409\n",
      "Null deviance: 553.3540454708925\n",
      "Residual deviance: 0.21023239123274154\n",
      "AIC: 54.21023239123274\n",
      "AUC: 1.0\n",
      "pr_auc: 0.6527777777777778\n",
      "Gini: 1.0\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.9994226545407332: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>292.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/292.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.0</td>\n",
       "<td>144.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/144.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>292.0</td>\n",
       "<td>144.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/436.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  -----------\n",
       "0      292  0    0        (0.0/292.0)\n",
       "1      0    144  0        (0.0/144.0)\n",
       "Total  292  144  0        (0.0/436.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.9994227</td>\n",
       "<td>1.0</td>\n",
       "<td>62.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.9994227</td>\n",
       "<td>1.0</td>\n",
       "<td>62.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.9994227</td>\n",
       "<td>1.0</td>\n",
       "<td>62.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.9994227</td>\n",
       "<td>1.0</td>\n",
       "<td>62.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.9994227</td>\n",
       "<td>1.0</td>\n",
       "<td>62.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.9994227</td>\n",
       "<td>1.0</td>\n",
       "<td>62.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.9994227</td>\n",
       "<td>1.0</td>\n",
       "<td>62.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.9994227</td>\n",
       "<td>1.0</td>\n",
       "<td>62.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value    idx\n",
       "---------------------------  -----------  -------  -----\n",
       "max f1                       0.999423     1        62\n",
       "max f2                       0.999423     1        62\n",
       "max f0point5                 0.999423     1        62\n",
       "max accuracy                 0.999423     1        62\n",
       "max precision                0.999914     1        0\n",
       "max recall                   0.999423     1        62\n",
       "max specificity              0.999914     1        0\n",
       "max absolute_mcc             0.999423     1        62\n",
       "max min_per_class_accuracy   0.999423     1        62\n",
       "max mean_per_class_accuracy  0.999423     1        62"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 33.03 %, avg score: 33.04 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0114679</td>\n",
       "<td>0.9999139</td>\n",
       "<td>3.0277778</td>\n",
       "<td>3.0277778</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>0.0347222</td>\n",
       "<td>0.0347222</td>\n",
       "<td>202.7777778</td>\n",
       "<td>202.7777778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0206422</td>\n",
       "<td>0.9999139</td>\n",
       "<td>3.0277778</td>\n",
       "<td>3.0277778</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>0.0277778</td>\n",
       "<td>0.0625</td>\n",
       "<td>202.7777778</td>\n",
       "<td>202.7777778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0321101</td>\n",
       "<td>0.9999139</td>\n",
       "<td>3.0277778</td>\n",
       "<td>3.0277778</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>0.0347222</td>\n",
       "<td>0.0972222</td>\n",
       "<td>202.7777778</td>\n",
       "<td>202.7777778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0412844</td>\n",
       "<td>0.9999139</td>\n",
       "<td>3.0277778</td>\n",
       "<td>3.0277778</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>0.0277778</td>\n",
       "<td>0.125</td>\n",
       "<td>202.7777778</td>\n",
       "<td>202.7777778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0504587</td>\n",
       "<td>0.9999139</td>\n",
       "<td>3.0277778</td>\n",
       "<td>3.0277778</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>0.0277778</td>\n",
       "<td>0.1527778</td>\n",
       "<td>202.7777778</td>\n",
       "<td>202.7777778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1009174</td>\n",
       "<td>0.9999139</td>\n",
       "<td>3.0277778</td>\n",
       "<td>3.0277778</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>0.1527778</td>\n",
       "<td>0.3055556</td>\n",
       "<td>202.7777778</td>\n",
       "<td>202.7777778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1513761</td>\n",
       "<td>0.9999138</td>\n",
       "<td>3.0277778</td>\n",
       "<td>3.0277778</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999138</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999139</td>\n",
       "<td>0.1527778</td>\n",
       "<td>0.4583333</td>\n",
       "<td>202.7777778</td>\n",
       "<td>202.7777778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2018349</td>\n",
       "<td>0.9999133</td>\n",
       "<td>3.0277778</td>\n",
       "<td>3.0277778</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999135</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999138</td>\n",
       "<td>0.1527778</td>\n",
       "<td>0.6111111</td>\n",
       "<td>202.7777778</td>\n",
       "<td>202.7777778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3004587</td>\n",
       "<td>0.9998651</td>\n",
       "<td>3.0277778</td>\n",
       "<td>3.0277778</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999073</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999116</td>\n",
       "<td>0.2986111</td>\n",
       "<td>0.9097222</td>\n",
       "<td>202.7777778</td>\n",
       "<td>202.7777778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4013761</td>\n",
       "<td>0.0000692</td>\n",
       "<td>0.8945707</td>\n",
       "<td>2.4914286</td>\n",
       "<td>0.2954545</td>\n",
       "<td>0.2969491</td>\n",
       "<td>0.8228571</td>\n",
       "<td>0.8231668</td>\n",
       "<td>0.0902778</td>\n",
       "<td>1.0</td>\n",
       "<td>-10.5429293</td>\n",
       "<td>149.1428571</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0000673</td>\n",
       "<td>0.0</td>\n",
       "<td>2.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000677</td>\n",
       "<td>0.6605505</td>\n",
       "<td>0.6608124</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>100.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6009174</td>\n",
       "<td>0.0000673</td>\n",
       "<td>0.0</td>\n",
       "<td>1.6641221</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000673</td>\n",
       "<td>0.5496183</td>\n",
       "<td>0.5498476</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>66.4122137</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6995413</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.0</td>\n",
       "<td>1.4295082</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.4721311</td>\n",
       "<td>0.4723375</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>42.9508197</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8004587</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.0</td>\n",
       "<td>1.2492837</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.4126074</td>\n",
       "<td>0.4127963</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>24.9283668</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8990826</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1122449</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.3673469</td>\n",
       "<td>0.3675224</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.2244898</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000672</td>\n",
       "<td>0.3302752</td>\n",
       "<td>0.3304398</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0114679                   0.999914           3.02778   3.02778            1                0.999914     1                           0.999914            0.0347222       0.0347222                  202.778   202.778\n",
       "    2        0.0206422                   0.999914           3.02778   3.02778            1                0.999914     1                           0.999914            0.0277778       0.0625                     202.778   202.778\n",
       "    3        0.0321101                   0.999914           3.02778   3.02778            1                0.999914     1                           0.999914            0.0347222       0.0972222                  202.778   202.778\n",
       "    4        0.0412844                   0.999914           3.02778   3.02778            1                0.999914     1                           0.999914            0.0277778       0.125                      202.778   202.778\n",
       "    5        0.0504587                   0.999914           3.02778   3.02778            1                0.999914     1                           0.999914            0.0277778       0.152778                   202.778   202.778\n",
       "    6        0.100917                    0.999914           3.02778   3.02778            1                0.999914     1                           0.999914            0.152778        0.305556                   202.778   202.778\n",
       "    7        0.151376                    0.999914           3.02778   3.02778            1                0.999914     1                           0.999914            0.152778        0.458333                   202.778   202.778\n",
       "    8        0.201835                    0.999913           3.02778   3.02778            1                0.999913     1                           0.999914            0.152778        0.611111                   202.778   202.778\n",
       "    9        0.300459                    0.999865           3.02778   3.02778            1                0.999907     1                           0.999912            0.298611        0.909722                   202.778   202.778\n",
       "    10       0.401376                    6.92386e-05        0.894571  2.49143            0.295455         0.296949     0.822857                    0.823167            0.0902778       1                          -10.5429  149.143\n",
       "    11       0.5                         6.73167e-05        0         2                  0                6.76779e-05  0.66055                     0.660812            0               1                          -100      100\n",
       "    12       0.600917                    6.72501e-05        0         1.66412            0                6.72707e-05  0.549618                    0.549848            0               1                          -100      66.4122\n",
       "    13       0.699541                    6.72365e-05        0         1.42951            0                6.72426e-05  0.472131                    0.472338            0               1                          -100      42.9508\n",
       "    14       0.800459                    6.72309e-05        0         1.24928            0                6.72333e-05  0.412607                    0.412796            0               1                          -100      24.9284\n",
       "    15       0.899083                    6.72281e-05        0         1.11224            0                6.72294e-05  0.367347                    0.367522            0               1                          -100      11.2245\n",
       "    16       1                           6.72263e-05        0         1                  0                6.72272e-05  0.330275                    0.33044             0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.0002240260624873979\n",
      "RMSE: 0.01496750020836472\n",
      "LogLoss: 0.0008999462306909166\n",
      "Null degrees of freedom: 4133\n",
      "Residual degrees of freedom: 4107\n",
      "Null deviance: 5189.586167649082\n",
      "Residual deviance: 7.440755435352477\n",
      "AIC: 61.44075543535248\n",
      "AUC: 1.0\n",
      "pr_auc: 0.9019607843137255\n",
      "Gini: 1.0\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.043778129118590504: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>2808.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/2808.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.0</td>\n",
       "<td>1326.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/1326.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>2808.0</td>\n",
       "<td>1326.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/4134.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1     Error    Rate\n",
       "-----  ----  ----  -------  ------------\n",
       "0      2808  0     0        (0.0/2808.0)\n",
       "1      0     1326  0        (0.0/1326.0)\n",
       "Total  2808  1326  0        (0.0/4134.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.0437781</td>\n",
       "<td>1.0</td>\n",
       "<td>193.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0437781</td>\n",
       "<td>1.0</td>\n",
       "<td>193.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.0437781</td>\n",
       "<td>1.0</td>\n",
       "<td>193.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.0437781</td>\n",
       "<td>1.0</td>\n",
       "<td>193.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9998975</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0437781</td>\n",
       "<td>1.0</td>\n",
       "<td>193.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9998975</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.0437781</td>\n",
       "<td>1.0</td>\n",
       "<td>193.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.0437781</td>\n",
       "<td>1.0</td>\n",
       "<td>193.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.0437781</td>\n",
       "<td>1.0</td>\n",
       "<td>193.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value    idx\n",
       "---------------------------  -----------  -------  -----\n",
       "max f1                       0.0437781    1        193\n",
       "max f2                       0.0437781    1        193\n",
       "max f0point5                 0.0437781    1        193\n",
       "max accuracy                 0.0437781    1        193\n",
       "max precision                0.999898     1        0\n",
       "max recall                   0.0437781    1        193\n",
       "max specificity              0.999898     1        0\n",
       "max absolute_mcc             0.0437781    1        193\n",
       "max min_per_class_accuracy   0.0437781    1        193\n",
       "max mean_per_class_accuracy  0.0437781    1        193"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 32.08 %, avg score: 32.05 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0101597</td>\n",
       "<td>0.9998976</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998976</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998976</td>\n",
       "<td>0.0316742</td>\n",
       "<td>0.0316742</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200774</td>\n",
       "<td>0.9998975</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998975</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998976</td>\n",
       "<td>0.0309201</td>\n",
       "<td>0.0625943</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0302371</td>\n",
       "<td>0.9998975</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998975</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998975</td>\n",
       "<td>0.0316742</td>\n",
       "<td>0.0942685</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0401548</td>\n",
       "<td>0.9998970</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998974</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998975</td>\n",
       "<td>0.0309201</td>\n",
       "<td>0.1251885</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500726</td>\n",
       "<td>0.9998970</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998970</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998974</td>\n",
       "<td>0.0309201</td>\n",
       "<td>0.1561086</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1001451</td>\n",
       "<td>0.9998958</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998962</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998968</td>\n",
       "<td>0.1561086</td>\n",
       "<td>0.3122172</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1502177</td>\n",
       "<td>0.9998885</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998913</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998950</td>\n",
       "<td>0.1561086</td>\n",
       "<td>0.4683258</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2002903</td>\n",
       "<td>0.9998664</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998823</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998918</td>\n",
       "<td>0.1561086</td>\n",
       "<td>0.6244344</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3001935</td>\n",
       "<td>0.9998428</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998551</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998796</td>\n",
       "<td>0.3114630</td>\n",
       "<td>0.9358974</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4003387</td>\n",
       "<td>0.0000880</td>\n",
       "<td>0.6400966</td>\n",
       "<td>2.4978852</td>\n",
       "<td>0.2053140</td>\n",
       "<td>0.2029050</td>\n",
       "<td>0.8012085</td>\n",
       "<td>0.8005155</td>\n",
       "<td>0.0641026</td>\n",
       "<td>1.0</td>\n",
       "<td>-35.9903382</td>\n",
       "<td>149.7885196</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5002419</td>\n",
       "<td>0.0000833</td>\n",
       "<td>0.0</td>\n",
       "<td>1.9990329</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000873</td>\n",
       "<td>0.6411992</td>\n",
       "<td>0.6406621</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>99.9032882</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6003870</td>\n",
       "<td>0.0000831</td>\n",
       "<td>0.0</td>\n",
       "<td>1.6655923</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000831</td>\n",
       "<td>0.5342466</td>\n",
       "<td>0.5338129</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>66.5592264</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7002903</td>\n",
       "<td>0.0000826</td>\n",
       "<td>0.0</td>\n",
       "<td>1.4279793</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000827</td>\n",
       "<td>0.4580311</td>\n",
       "<td>0.4576711</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>42.7979275</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8004354</td>\n",
       "<td>0.0000749</td>\n",
       "<td>0.0</td>\n",
       "<td>1.2493200</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000773</td>\n",
       "<td>0.4007253</td>\n",
       "<td>0.4004200</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>24.9320036</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8998549</td>\n",
       "<td>0.0000733</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1112903</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000744</td>\n",
       "<td>0.3564516</td>\n",
       "<td>0.3561883</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1290323</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000732</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000732</td>\n",
       "<td>0.3207547</td>\n",
       "<td>0.3205251</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0101597                   0.999898           3.11765   3.11765            1                0.999898     1                           0.999898            0.0316742       0.0316742                  211.765   211.765\n",
       "    2        0.0200774                   0.999898           3.11765   3.11765            1                0.999898     1                           0.999898            0.0309201       0.0625943                  211.765   211.765\n",
       "    3        0.0302371                   0.999898           3.11765   3.11765            1                0.999898     1                           0.999898            0.0316742       0.0942685                  211.765   211.765\n",
       "    4        0.0401548                   0.999897           3.11765   3.11765            1                0.999897     1                           0.999898            0.0309201       0.125189                   211.765   211.765\n",
       "    5        0.0500726                   0.999897           3.11765   3.11765            1                0.999897     1                           0.999897            0.0309201       0.156109                   211.765   211.765\n",
       "    6        0.100145                    0.999896           3.11765   3.11765            1                0.999896     1                           0.999897            0.156109        0.312217                   211.765   211.765\n",
       "    7        0.150218                    0.999889           3.11765   3.11765            1                0.999891     1                           0.999895            0.156109        0.468326                   211.765   211.765\n",
       "    8        0.20029                     0.999866           3.11765   3.11765            1                0.999882     1                           0.999892            0.156109        0.624434                   211.765   211.765\n",
       "    9        0.300194                    0.999843           3.11765   3.11765            1                0.999855     1                           0.99988             0.311463        0.935897                   211.765   211.765\n",
       "    10       0.400339                    8.79951e-05        0.640097  2.49789            0.205314         0.202905     0.801208                    0.800516            0.0641026       1                          -35.9903  149.789\n",
       "    11       0.500242                    8.33224e-05        0         1.99903            0                8.73247e-05  0.641199                    0.640662            0               1                          -100      99.9033\n",
       "    12       0.600387                    8.31137e-05        0         1.66559            0                8.31486e-05  0.534247                    0.533813            0               1                          -100      66.5592\n",
       "    13       0.70029                     8.2597e-05         0         1.42798            0                8.26939e-05  0.458031                    0.457671            0               1                          -100      42.7979\n",
       "    14       0.800435                    7.48659e-05        0         1.24932            0                7.72877e-05  0.400725                    0.40042             0               1                          -100      24.932\n",
       "    15       0.899855                    7.32599e-05        0         1.11129            0                7.43947e-05  0.356452                    0.356188            0               1                          -100      11.129\n",
       "    16       1                           7.32306e-05        0         1                  0                7.32367e-05  0.320755                    0.320525            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "gbm prediction progress: |ââââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/cloudera/parcels/Anaconda-5.1.0.1/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'features85_list_indices' has levels not trained on: [[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 61 62 63 65 66 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 58 61 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 56 60 61 64 66 67 75 77], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 55 56 57 58 61 62 63 65 66 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 57 58 59 61 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 57 58 60 61 65 66 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 58 59 61 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 58 61 62 63 67], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 48 49 50 51 52\n",
      " 54 55 56 58 61 62 63 65 68 69 70 71 72 73 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 57 58 59 60 66 67 77], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 58 65 77 81], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 56 58 65 77 81 82], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 54 55 57 58 60 65 66 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 54 55 57 58 60 65 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 54 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 54 58 62 63 65\n",
      " 68 69 70 71 72 73 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 54 62 63 65 68\n",
      " 69 70 71 72 73 78], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 43 44 45 46 47 53 58 59 64 76\n",
      " 81], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 54 55 56 57 58 60 61 64 66 67 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 54 55 56 58 60 61 64 67 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 36 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 58 59 60 64 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 54 55 56 57 58 60 66 67 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 55 56 58 59 60 67 74 77], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 48 49 50 51 52 53 54 55\n",
      " 58 59 60 68 69 70 71 72 73 74 78], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 54 56 57 61 62 63 65 66], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 48 49 50 51 52 54\n",
      " 56 58 60 64 67 68 69 70 71 72 73 77], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 54 56 57 65 66 77 81 82], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 50 51 52 53 54 56\n",
      " 65 68 69 70 71 72 73 77 78 81 82], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 50 51 52 54 56 58\n",
      " 59 64 65 68 69 70 71 72 73 77 81], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 42 43 44 45 46 47 53 57 59 76 81], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 48 49 50 51 52 53 54 56 58\n",
      " 60 67 68 69 70 71 72 73 74], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 48 49 50 51 52 53 54 58 59\n",
      " 60 68 69 70 71 72 73 74 78], [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 42 43 44 45 46 47 53 55 57 58 62 63 64\n",
      " 66 67 74 75], [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 42 43 44 45 46 47 53 57 58 62 63 64 67\n",
      " 74 75], [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 43 44 45 46 47 53 57 58 62 63 64 67 74\n",
      " 75 82], [ 0  1  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 40 41 43 44 45 46 47 57 58 60 82], [ 0  1  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 40 41 43 44 45 46 47 57 60 82], [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 38 39 43 44 45 46 47 53 57 59 66 76\n",
      " 81], [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 42 43 44 45 46 47 53 59 76 81 82], [ 1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 38 39 43 44 45 46 47 53 57 58 59 66 76 81], [ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 40 41 42 43 44 45 46 47 55 57 58 60]]\n",
      "  warnings.warn(w)\n",
      "/opt/cloudera/parcels/Anaconda-5.1.0.1/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'features85_list_values' has levels not trained on: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 1.], [1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [1. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1.], [1. 2. 2. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1.], [1. 2. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1.], [1. 2. 3. 3. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1.], [1. 4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.\n",
      " 2. 1.], [1. 4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1.\n",
      " 2. 1.], [1. 4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.], [1. 4. 4. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 3.], [1. 4. 5. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 3.], [1. 4. 5. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [2. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 1. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 4. 2. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 3.], [2. 4. 3. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 4. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [3. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1.], [3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1.], [3. 2. 2. 3. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1.], [3. 2. 2. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1.], [3. 2. 2. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1.], [3. 2. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.], [3. 4. 2. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3. 1.], [3. 4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [3. 4. 3. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [4. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 2. 2.\n",
      " 1.], [4. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1.\n",
      " 1. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 2.\n",
      " 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 3. 1.\n",
      " 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 2.\n",
      " 1. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1.], [4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1.], [4. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 4. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2.\n",
      " 1.], [5. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [5. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [6. 2. 3. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.]]\n",
      "  warnings.warn(w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>hash_message                                                                                                                    </th><th style=\"text-align: right;\">  fraud_label</th><th style=\"text-align: right;\">  kl_fraud_words</th><th style=\"text-align: right;\">  ks_fraud_words</th><th style=\"text-align: right;\">  entropy_fraud_words</th><th style=\"text-align: right;\">  kl_notfraud_words</th><th style=\"text-align: right;\">  ks_notfraud_words</th><th style=\"text-align: right;\">  entropy_notfraud_words</th><th style=\"text-align: right;\">  features85_type</th><th style=\"text-align: right;\">  features85_size</th><th style=\"text-align: right;\">  ngramscounts7_type</th><th style=\"text-align: right;\">  ngramscounts7_size</th><th>features85_list_indices  </th><th>features85_list_values  </th><th>ngramscounts7_list_indices  </th><th>ngramscounts7_list_values  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>101642031f7aa7d62d25fbc9dc190f6165b1efd53d20eea9a7405968fc32ba6da3db74ea520bd5910bc52db1acc8f3bb069e415572a344186ab52fb0aa0211d2</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2703.02</td><td style=\"text-align: right;\">        0.83296 </td><td style=\"text-align: right;\">            0.0480958</td><td style=\"text-align: right;\">            3235.29</td><td style=\"text-align: right;\">           0.736194</td><td style=\"text-align: right;\">               0.0631322</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "   1192   1193   1195   1972   3168   4048  20261  21615  22667  23930\n",
       "  24166  51800  53721  56262  60340  61638  63487  65066  71064  77803\n",
       "  80020  81356  82272  82280  86026  89190 165716 170455 187454 235691\n",
       " 255607]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>a7250368bd2f3d9e0ecbdd03354d7196b83a2a40013c748face8f747e0443fdf0fd6c22a588d1795c16b455c7039cf657ecf5bcdfde08fbe54669045033d24f1</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         3327.77</td><td style=\"text-align: right;\">        0.963607</td><td style=\"text-align: right;\">            0.0489203</td><td style=\"text-align: right;\">            4471.18</td><td style=\"text-align: right;\">           0.730739</td><td style=\"text-align: right;\">               0.0765263</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    842    843    844   2815   6052   6740   6792   6985  12328  12413\n",
       "  22038  23607  25439  27477  29093  31782  32434  32787  33325  70710\n",
       "  71694  78589  80537  87234 104781 105971 106868 108578 112657 211966\n",
       " 217414 242542]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>cf6aaf9aa72e0f8ebdbbab8db1e5e6c2d9ffac5bc954ae0e057099e0dd6464ee2136042556445063e81c1a4107c2d0a27234da61e648541391b2a904456a005a</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2434.66</td><td style=\"text-align: right;\">        0.980881</td><td style=\"text-align: right;\">            0.0417387</td><td style=\"text-align: right;\">            3292.28</td><td style=\"text-align: right;\">           0.784173</td><td style=\"text-align: right;\">               0.06374  </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    164    165    166    295    681   1296   1298   1299   1616   1868\n",
       "   3712   3774   3788   3797   3811   3818   3827   3828   3829   3843\n",
       "  17132  18494  36389  36416  36515  38711  41234  45850  46554 120634\n",
       " 124787 225931 242249]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>5971b987ced86878931381142fdc479e723acb6637f637cb30156e81f88f06d53cd0f9bbf7180b9d710380c699c05f6d58ab1c23e640e231d803d11dfd7d1cd1</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2424.23</td><td style=\"text-align: right;\">        0.964228</td><td style=\"text-align: right;\">            0.0339223</td><td style=\"text-align: right;\">            4167.72</td><td style=\"text-align: right;\">           0.732933</td><td style=\"text-align: right;\">               0.0733494</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    607    608    609    816   2640   4105   4159   4209   6418  10263\n",
       "  10543  10995  11252  11343  11344  11403  11711  17186  17318  36214\n",
       "  38184  41546  42908  43180  43737  44097 120764 132602 134109 214651\n",
       " 251957]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>824d5fca02966660c51d6bd8309e3caa8e61e934cc791ff7e7b9fb4ce353106cc141cf9c245e3ede6a74e517a52ec858be0804d43eb8394c605eb4019a1e4270</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         3343.64</td><td style=\"text-align: right;\">        0.874593</td><td style=\"text-align: right;\">            0.0505059</td><td style=\"text-align: right;\">            3803.02</td><td style=\"text-align: right;\">           0.785161</td><td style=\"text-align: right;\">               0.0643405</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    164    165    166    295    570    571    572   2141   2148   2152\n",
       "   2172   3034   6787   8100   8160   8244   8685   8730   8789   8855\n",
       "  25997  28927 121203 179164 186793 213852 231031 231652 244436 252102]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>c6651b9ecde7ba7e9314d46222d1b2138d627bc57cb531fb5b65d8963d13880a094b11c8cc3cd48a4508ee8b5f3c91a8473eaa94e53d8307c8165e6de623de99</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2610.08</td><td style=\"text-align: right;\">        0.627063</td><td style=\"text-align: right;\">            0.0335777</td><td style=\"text-align: right;\">            4312.02</td><td style=\"text-align: right;\">           0.523838</td><td style=\"text-align: right;\">               0.072228 </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    607    608    609   1071   1072   1075   2087   2369   3230  12075\n",
       "  12724  20788  21799  22081  24066  24595  24601  24887  39956  68716\n",
       "  68887  72674  88029  89841  94006 101079 137134 141428 170932 211251\n",
       " 227551 228281 229625]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>bd6ff0f84372e6ca986f1a5fbb38f6fe3d3a885973c5c4fb643108b8f95f4e5e21da53ad1f97fe4c4dc489b445504ab551244e7686024adc01b8dc9d2030bb17</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         3089.51</td><td style=\"text-align: right;\">        0.38286 </td><td style=\"text-align: right;\">            0.0349989</td><td style=\"text-align: right;\">            4674.31</td><td style=\"text-align: right;\">           0.303823</td><td style=\"text-align: right;\">               0.0712006</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 36 38 39 40 41 42 43 44 45 46 47 48\n",
       " 49 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    544    545    546    707   1656   3166   3217   3224   3751   4634\n",
       "  13797  13924  14085  14958  15143  15212  15656  16132  16234  36285\n",
       "  36664  36931  37579  40774  43661  46324  70793  81994  89918 194247\n",
       " 201925]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>207103be4212a074b3ef483fab2083bc7be04f273a953853b512dc5aac5911bcd68e350f3b75dccf1e0cf9b5521703fa2c6e069504656453fc261376bf4ce972</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         3435.29</td><td style=\"text-align: right;\">        0.780179</td><td style=\"text-align: right;\">            0.0461717</td><td style=\"text-align: right;\">            4561.62</td><td style=\"text-align: right;\">           0.624608</td><td style=\"text-align: right;\">               0.0733706</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[    0     1     2     3     4     5     6     7     8     9    10    11\n",
       "    13    14    15    48    49    50    51    52    90    91    92    93\n",
       "    94    95   115   124   125   136   137   138   139   140   141   142\n",
       "   143   144   145   146   147   148   155   156   157   158   159   160\n",
       "   161   162   842   843   844  1010  1012  1279  1941  3761  3764  3771\n",
       "  3792  3849  3883  3891  7949  8941  9473  9502  9811  9853  9905 10075\n",
       " 25389 26731 29016 57023]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>3557520b1295c0ed3541163e88d7bc43e7084444031e2fe487277ca211677941062d4f2a346792c45c68ce388a0341e99cca1dbc7f7288bc475de3ed26ed7587</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2992.53</td><td style=\"text-align: right;\">        0.964534</td><td style=\"text-align: right;\">            0.0430735</td><td style=\"text-align: right;\">            3852.81</td><td style=\"text-align: right;\">           0.831228</td><td style=\"text-align: right;\">               0.064967 </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    164    165    166    295    842    843    844   1354   2296   5120\n",
       "   8931   9339   9990  10548  11708  11817  12198  12231  12406  12760\n",
       "  13044  13318  35608  35985  37861  42834  43587  45675  45899 145600\n",
       " 150444 226786]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>503a11e7c1393852838272468342f4db14504f4fe077ada30e196dc2c15a8d99265dcdc73e86c33a7ea8eec1e2333a209997cd44bbfe8e5b6a5d8ff439e2f180</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2981.36</td><td style=\"text-align: right;\">        0.830355</td><td style=\"text-align: right;\">            0.0385299</td><td style=\"text-align: right;\">            4289.69</td><td style=\"text-align: right;\">           0.5251  </td><td style=\"text-align: right;\">               0.0690645</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    570    571    572   1841   3995   8015   8686   8767   9256  10412\n",
       "  10646  10721  10985  11177  11564  11578  11751  13427  18261  28918\n",
       "  29472  30375  31210  31285  31376  33150 126502 131074 158352 180321\n",
       " 250518]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prediction\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">  p0</th><th style=\"text-align: right;\">  p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Save Model For Future Usage\n",
      "gbm prediction progress: |ââââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n",
      "test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>hash_message                                                                                                                    </th><th style=\"text-align: right;\">  fraud_label</th><th style=\"text-align: right;\">  kl_fraud_words</th><th style=\"text-align: right;\">  ks_fraud_words</th><th style=\"text-align: right;\">  entropy_fraud_words</th><th style=\"text-align: right;\">  kl_notfraud_words</th><th style=\"text-align: right;\">  ks_notfraud_words</th><th style=\"text-align: right;\">  entropy_notfraud_words</th><th style=\"text-align: right;\">  features85_type</th><th style=\"text-align: right;\">  features85_size</th><th style=\"text-align: right;\">  ngramscounts7_type</th><th style=\"text-align: right;\">  ngramscounts7_size</th><th>features85_list_indices  </th><th>features85_list_values  </th><th>ngramscounts7_list_indices  </th><th>ngramscounts7_list_values  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>0c983268c70fd7c560320a3a925c414fd448cfad5dbc1287f1f1234aae6b706bdc5ac0c1a43d67efea4f8d4d1c47a4795449ad8284a1d4eef377f2e407ecca15</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         3787.34</td><td style=\"text-align: right;\">       0.399077 </td><td style=\"text-align: right;\">            0.0795858</td><td style=\"text-align: right;\">            2450.78</td><td style=\"text-align: right;\">           0.722469</td><td style=\"text-align: right;\">               0.0547815</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
       " 51 52 53 54 55 57 58 59 60 64 74]                          </td><td>[3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     23     24     25     26     27     36     45     46     47     65\n",
       "     66     67     68     69     70     71     72     73     74     75\n",
       "     76     77     78     96     97     98     99    100    101    102\n",
       "    103    104    105    106    112    113    114    152    153    154\n",
       "   1455   2641  12207  36645  37149  38512  42844  43290  45175  45622\n",
       "  47181  47315  47754  51188  51421  53447  54992  55174  57563  58819\n",
       "  58824  58831  59045  59285  59421  60606  61074  64791  65510  66339\n",
       "  67109  67692 126259 227408 246021]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>751b0d4b88a1ad630b258c03596bf591c7d7d671fe8a54fc802599b80695ca85373e5b918307956930479bfcc5640b2fa91ccadf77c8afcf75dbbeef8d627689</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         3136.01</td><td style=\"text-align: right;\">       0.522336 </td><td style=\"text-align: right;\">            0.0757026</td><td style=\"text-align: right;\">            2208.72</td><td style=\"text-align: right;\">           0.316835</td><td style=\"text-align: right;\">               0.0628293</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
       " 51 52 53 54 57 59 60 64 74]                          </td><td>[3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     36     45     46     47     68     69     70     71     72     73\n",
       "     74     75     76     77     78     96     97     98     99    100\n",
       "    101    102    103    104    105    106    112    113    114   1070\n",
       "   1101   1103   1116   1172   1587   1853   1864   1866   7423   7552\n",
       "   7969  23657  46798  59908  60463  61747  64797  70099  70310  78128\n",
       "  79052  81104  81253  87607  88791  90640  91462  92107  96001  97569\n",
       "  98487 100447 106278 107445 111550 141147 141207 155918 163518 198047\n",
       " 237709 239794 259091]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>cbc22468fac4860040971cbe031ea19f8227f966d3a27884a9466a08b869d926f8ce43bf9ba88ee4691958b5a30794f0039b68fdc45f99d3dfc515727c2118d7</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         3183.96</td><td style=\"text-align: right;\">       0.522336 </td><td style=\"text-align: right;\">            0.0760335</td><td style=\"text-align: right;\">            2247.62</td><td style=\"text-align: right;\">           0.569637</td><td style=\"text-align: right;\">               0.0630055</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
       " 51 52 53 54 59 60 64 74]                          </td><td>[3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     36     45     46     47     68     69     70     71     72     73\n",
       "     74     75     76     77     78     96     97     98     99    100\n",
       "    101    102    103    104    105    106    112    113    114    302\n",
       "    303    304    928    944    948    952   1624   1625   1626   1657\n",
       "   2654  16970  20440  23125  23858  24125  24348  33711  34189  34569\n",
       "  34699  35707  35765  36001  37044  37939  38432  39438  39543  41952\n",
       "  42951  43753  44569  44846  45642 103813 104899 174276 189763 191089\n",
       " 219811 223729]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>647a1a86d4a231f6ea53f9d03cc4065768bac9e94533264d52a474613af4af4f6efa50f598c09acf943be604b22714ee7b5d6cfe5e8d05a2044a5650ca247c07</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         5527.58</td><td style=\"text-align: right;\">       0.177085 </td><td style=\"text-align: right;\">            0.0803395</td><td style=\"text-align: right;\">            5264.12</td><td style=\"text-align: right;\">           0.116322</td><td style=\"text-align: right;\">               0.0787753</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 48 49 50 51 52\n",
       " 54 56 61 62 63 67 68 69 70 71 72 73 78]                          </td><td>[4. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      5      9     10     11     12\n",
       "     16     17     18     19     20     21     28     29     30     33\n",
       "    256    286    307    308    309    310    311    312    313   1219\n",
       "   1220   1223   1224   1225   1226   1227   1229   1230   1231   1232\n",
       "   1233   1234   1235   1236   1238   1239   1240   1242   1243   1244\n",
       "   1980   2731   2763   2771   2786   3779   3850   4619   4632   4648\n",
       "   4669   4694   4779   4795   6003   6390   6420   6532   6593   6595\n",
       "  14261  16245  24905 121248 137943 222694 226564]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>89a21086ca9f0e18e588f36c1574d965f4ed483108208b11d6f52e229eed99a0bcf8d68fb5da24f50727aa167f9158dd7b9de41227a296230fa389d4c0213449</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         2096.24</td><td style=\"text-align: right;\">       0.157862 </td><td style=\"text-align: right;\">            0.0773763</td><td style=\"text-align: right;\">            2316.33</td><td style=\"text-align: right;\">           0.447932</td><td style=\"text-align: right;\">               0.073537 </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
       " 25 26 27 28 29 30 31 32 33 34 35 40 41 43 44 45 46 47 57 58 60 66]                          </td><td>[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      6      7      8    231    232\n",
       "    233    234    262    791    795    799    846    848    852    853\n",
       "    855    856    858    861    863    864    866    868    869    871\n",
       "   1680   9659  25669  27021  27885  28074  28822  29327  29745  30347\n",
       "  30873  31037  31355  31649  31886 115134 128855 140406 161067 162703\n",
       " 202211 218577 234361 257753]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>f019d9a53ea6317b9c4271fa4ea14d7e7107ba35f871473ce9f5c207ec7526bade74f60bcfae76841ce1b5c0954faa73961fb718a6ce4a1551ed57dd629e032e</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         4828.7 </td><td style=\"text-align: right;\">       0.626082 </td><td style=\"text-align: right;\">            0.068934 </td><td style=\"text-align: right;\">            4118.7 </td><td style=\"text-align: right;\">           0.975112</td><td style=\"text-align: right;\">               0.0714218</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
       " 25 26 27 28 29 30 31 32 33 34 35 42 43 44 45 46 47 53 57 58 62 63 64 67\n",
       " 74 75]                          </td><td>[2. 4. 5. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1.\n",
       " 1. 3.]                         </td><td>[     0      1      2      3      4      6      7      8    202    203\n",
       "    204    205    208    209    210    211    229    230    247   1664\n",
       "   2234   2358   2590   3059   3064   3130  13549  13715  13831  13928\n",
       "  13958  14158  14873  16251  25616  27420  30666  32414  32673  34075\n",
       "  34193  34993  35675  35677  35828  36695  36815  36833  36839  37319\n",
       "  37759  37851  38268  38275  38436  38813  38955  39046  39200  39204\n",
       "  39362  39813  39905  40114  40756  41907  42203  42689  42879  43301\n",
       "  43819  44054  44135  44840  45065  45553  45712  45802  45849  46315\n",
       "  46491 124741 156589 165472 190325 196092 197240]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>5497dc4dc2abc0cf7c5a7e4b09b058cbd8deed23eabc446057911df6d6b3858e5f5854ae73f53cc02761185c7e02c0da166b17cf1f500903bef7f10e4a70fd25</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         4345.49</td><td style=\"text-align: right;\">       0.978829 </td><td style=\"text-align: right;\">            0.0747982</td><td style=\"text-align: right;\">            3607.71</td><td style=\"text-align: right;\">           0.673018</td><td style=\"text-align: right;\">               0.056413 </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
       " 48 49 50 51 52 54 56 61 62 63 64 67 75 77]                          </td><td>[6. 2. 3. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     36    149    150    151    236    244    248    249\n",
       "    250    261    448    449    450    451    452    453    454    683\n",
       "    697    718    761    762    763    764    765    766    769    770\n",
       "    771    773    955    958    960    963    965    966    968    984\n",
       "    985    987    990    991    993    994    995    997    998   1046\n",
       "   1059   1092   2238   2569   2904   2905   2946   3647   4692   4695\n",
       "   8366  13581  13694  13905  14220  14271  14712  14859  15019  15151\n",
       "  15392  15437  15522  15549  15560  15598  15889  16004  16182  16267\n",
       "  16359  20730  20771  20785  20843  22875  24480  24577  24852  53828\n",
       " 126816 132228 149513 157143 223244 230468]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>7a26b41f0501bf1a91fab3c66f48efa09bdc582dc213b9576d772e855188741da14096f3506daee11a591c6582df456d9620f119ea3a43decca1deec5e217bb5</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         4890.76</td><td style=\"text-align: right;\">       0.989061 </td><td style=\"text-align: right;\">            0.0830742</td><td style=\"text-align: right;\">            2499.05</td><td style=\"text-align: right;\">           0.833535</td><td style=\"text-align: right;\">               0.0577531</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
       " 48 49 50 51 52 54 56 57 58 60 61 65 66 79 80 83 84]                          </td><td>[3. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     12     13     14     15     22     31     32     34\n",
       "     35     40     41     43     44     63     64     81     82     83\n",
       "     84    116    117    118    119    120    121    122    123    439\n",
       "    440    441    442    443    444    445    446   1009   1105   1113\n",
       "   1115   1117   1631   1648   1651   1995   2018   2042   2043   2046\n",
       "   3952  20759  21665  22274  23669  23838  24241  35472  36610  36770\n",
       "  37958  40184  41068  42712  42791  46046  49260  56168  64717 116078\n",
       " 131231 135097 140555 148227 159879 160558 166006 197239 205224]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>5383d9b8598680fcd978ce6238d65a0805eb63fa59e4b45b68486f97d3e70f149ca706a0be06be8e569543a63ae756d12219b6786d1ddf1424e912f6ee37201a</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         2159.28</td><td style=\"text-align: right;\">       0.0817671</td><td style=\"text-align: right;\">            0.0734992</td><td style=\"text-align: right;\">            1616.69</td><td style=\"text-align: right;\">           0.110837</td><td style=\"text-align: right;\">               0.0671171</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
       " 25 26 27 28 29 30 31 32 33 34 35 38 39 42 43 44 45 46 47 53 57 59 76 81]                          </td><td>[4. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 2. 2. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8     85\n",
       "     86     87     88    107   1064   1071   1072   1075   1110   1209\n",
       "   1210   1684   1849   1850   1851   2215   4202   5237   5401   7782\n",
       "  10454  10491  10643  10729  10838  11677  15369  68749  69787  74124\n",
       "  74497  75192  76197  77532  79903  80283  80548  82333  82510  83513\n",
       "  83794  86834  87132  87626  88160  90265  92572  93100  93159  93522\n",
       "  95128  95539  96911  99690 101130 103184 103602 105018 106722 108018\n",
       " 109202 109477 112092 112607 112670 113234 113509 119438 127234 153204\n",
       " 211147 213309]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>f17d2f1ac2307d595763c3422ecd432cabf485c700895111519c7952f583164ae27e400504a50a7a7e73dbe709a3434f023729ab306484da388a1080e674feca</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         2702.7 </td><td style=\"text-align: right;\">       0.984483 </td><td style=\"text-align: right;\">            0.0511013</td><td style=\"text-align: right;\">            3574.73</td><td style=\"text-align: right;\">           0.993018</td><td style=\"text-align: right;\">               0.0714935</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
       " 25 26 27 28 29 30 31 32 33 34 35 43 44 45 46 47 53 57 62 63 64 67 74 75]                          </td><td>[1. 4. 4. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 3.]                         </td><td>[     0      1      2      3      4      6      7      8    164    165\n",
       "    166    202    203    204    205    208    209    210    211    229\n",
       "    230    463    520    788    886    890    964   1174   1176   1177\n",
       "   1387   1460   1477   2295   2302   2306   2311   2322   2349   2594\n",
       "   2596   2605   2607   2608   2613   2644   2650   2663   3198   3946\n",
       "   4040   6150  12735  12951  13201  13331  13447  26320  47125  48118\n",
       "  48271  48651  49934  50964  52100  52279  52435  52678  54577  55736\n",
       "  56712  56718  58360  58963  59206  65094  65175 117790 121041 140200\n",
       " 152718 221068 224277 237713 248531]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prediction\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">  p0</th><th style=\"text-align: right;\">         p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">3.25879e-18</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm prediction progress: |ââââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n",
      "Export File progress: | (failed)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Job with key $03017f00000132d4ffffffff$_b77a6ef0518b8070fc08c93ab8344ca2 failed with an exception: java.lang.RuntimeException: java.io.FileNotFoundException: ./projects/logs-archive-production/fraud-canada-tokenizedwords/notebooks/product_model_prediction/file_prediction_20190206.csv (No such file or directory)\nstacktrace: \njava.lang.RuntimeException: java.io.FileNotFoundException: ./projects/logs-archive-production/fraud-canada-tokenizedwords/notebooks/product_model_prediction/file_prediction_20190206.csv (No such file or directory)\n\tat water.persist.PersistManager.create(PersistManager.java:586)\n\tat water.util.FrameUtils$ExportTaskDriver.exportCSVStream(FrameUtils.java:460)\n\tat water.util.FrameUtils$ExportTaskDriver.compute2(FrameUtils.java:362)\n\tat water.H2O$H2OCountedCompleter.compute(H2O.java:1310)\n\tat jsr166y.CountedCompleter.exec(CountedCompleter.java:468)\n\tat jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)\n\tat jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)\n\tat jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)\n\tat jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\nCaused by: java.io.FileNotFoundException: ./projects/logs-archive-production/fraud-canada-tokenizedwords/notebooks/product_model_prediction/file_prediction_20190206.csv (No such file or directory)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:101)\n\tat water.persist.PersistManager.create(PersistManager.java:583)\n\t... 8 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-60d10a65e7f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0mpreds_over_all_hf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0mpath_out_file1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./projects/logs-archive-production/fraud-canada-tokenizedwords/notebooks/product_model_prediction/file_prediction_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mprocess_date\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m \u001b[0moutput_pred_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds_over_all_hf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_out_file1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0mtrain_over_all_hf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda-5.1.0.1/lib/python3.6/site-packages/h2o/h2o.py\u001b[0m in \u001b[0;36mexport_file\u001b[0;34m(frame, path, force, parts)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0massert_is_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     H2OJob(api(\"POST /3/Frames/%s/export\" % (frame.frame_id), data={\"path\": path, \"num_parts\": parts, \"force\": force}),\n\u001b[0;32m-> 1022\u001b[0;31m            \"Export File\").poll()\n\u001b[0m\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda-5.1.0.1/lib/python3.6/site-packages/h2o/job.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, verbose_model_scoring_history)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"stacktrace\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 raise EnvironmentError(\"Job with key {} failed with an exception: {}\\nstacktrace: \"\n\u001b[0;32m---> 77\u001b[0;31m                                        \"\\n{}\".format(self.job_key, self.exception, self.job[\"stacktrace\"]))\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job with key %s failed with an exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Job with key $03017f00000132d4ffffffff$_b77a6ef0518b8070fc08c93ab8344ca2 failed with an exception: java.lang.RuntimeException: java.io.FileNotFoundException: ./projects/logs-archive-production/fraud-canada-tokenizedwords/notebooks/product_model_prediction/file_prediction_20190206.csv (No such file or directory)\nstacktrace: \njava.lang.RuntimeException: java.io.FileNotFoundException: ./projects/logs-archive-production/fraud-canada-tokenizedwords/notebooks/product_model_prediction/file_prediction_20190206.csv (No such file or directory)\n\tat water.persist.PersistManager.create(PersistManager.java:586)\n\tat water.util.FrameUtils$ExportTaskDriver.exportCSVStream(FrameUtils.java:460)\n\tat water.util.FrameUtils$ExportTaskDriver.compute2(FrameUtils.java:362)\n\tat water.H2O$H2OCountedCompleter.compute(H2O.java:1310)\n\tat jsr166y.CountedCompleter.exec(CountedCompleter.java:468)\n\tat jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)\n\tat jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)\n\tat jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)\n\tat jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\nCaused by: java.io.FileNotFoundException: ./projects/logs-archive-production/fraud-canada-tokenizedwords/notebooks/product_model_prediction/file_prediction_20190206.csv (No such file or directory)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:270)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:101)\n\tat water.persist.PersistManager.create(PersistManager.java:583)\n\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import findspark\n",
    "findspark.init()\n",
    "#\n",
    "import pyspark\n",
    "from pyspark.sql import functions as pfunc\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Window, types\n",
    "#\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from scipy.stats import kstest\n",
    "from scipy import stats\n",
    "#\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "#\n",
    "#import org.apache.spark.ml.feature.NGram\n",
    "from pyspark.ml.feature import NGram\n",
    "#\n",
    "from collections import Counter\n",
    "#\n",
    "from pyspark.ml.feature import NGram\n",
    "#\n",
    "from pyspark.ml.feature import NGram, CountVectorizer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "#\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector\n",
    "#\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "#\n",
    "#  FILTER with PySpark SQL Functions F.\n",
    "#\n",
    "# Arguments\n",
    "#\n",
    "import argparse\n",
    "## Parse date_of execution\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--datev1\", help=\"Execution Date\")\n",
    "#args = parser.parse_args()\n",
    "#if args.datev1:\n",
    "#    processdate = args.datev1\n",
    "# GENERAL PREPARATION SCRIPT\n",
    "#\n",
    "#  Date in format YYYYMMDD\n",
    "#process_date = processdate\n",
    "#if not process_date:\n",
    "#    process_date = \"20181231\"\n",
    "#\n",
    "process_date=\"20190206\"\n",
    "#\n",
    "sc = pyspark.SparkContext(appName=\"FraudCanada-AUTOML-Model-NGrams-CountVectorizer-KL-KS-Entropy\")\n",
    "sqlContext = SQLContext(sc)\n",
    "#\n",
    "input_most_frequent_df=\"hdfs:///data/staged/ott_dazn/advanced-model-data/the-most-frequent-fraud-hash_message/dt=\"+process_date\n",
    "input_fraud=\"hdfs:///data/staged/ott_dazn/advanced-model-data/fraud-notfraud-canada-tokenizedwords-ngrams-5-features-85/dt=\"+process_date\n",
    "#\n",
    "input_file1=\"hdfs:///data/staged/ott_dazn/advanced-model-data/fraud-notfraud-canada-tokenizedwords-ngrams-5-features-85/dt=\"+process_date\n",
    "output_file1=\"hdfs:///data/staged/ott_dazn/advanced-model-data/label-fraud-notfraud-data-model/dt=\"+process_date\n",
    "preserve_training_input_file=\"hdfs:///data/staged/ott_dazn/advanced-model-data/preserve-training-output-automl-clean/dt=\"+process_date\n",
    "#\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "#\n",
    "import subprocess\n",
    "subprocess.run('unset http_proxy', shell=True)\n",
    "#\n",
    "# Start an H2O virtual cluster that uses 6 gigs of RAM and 6 cores\n",
    "h2o.init(ip=\"localhost\",port=54321,max_mem_size = \"6g\", nthreads = 6) \n",
    "#\n",
    "# Clean up the h2o cluster just in case\n",
    "h2o.remove_all()\n",
    "#\n",
    "#  TRAINING PROCESS\n",
    "#\n",
    "print(\"Start Training Model NGrams Vectors KS KL Entropty\")\n",
    "#\n",
    "# Horrible code :: close your eyes, is ugly\n",
    "#\n",
    "fraud_label_read_file=sqlContext.read.json(output_file1)\n",
    "fraud_label_read_file.printSchema()\n",
    "#\n",
    "fraud_label_read_df=fraud_label_read_file\\\n",
    ".select(col('hash_message').cast('string'),col('fraud_label').cast('int'),\\\n",
    "        col('kl_fraud_words').cast('double'),col('ks_fraud_words').cast('double'),\\\n",
    "        col('entropy_fraud_words').cast('double'),\\\n",
    "        col('kl_notfraud_words').cast('double'), col('ks_notfraud_words').cast('double'),\\\n",
    "        col('entropy_notfraud_words').cast('double'),\\\n",
    "        col('features_85.type').alias('features85_type').cast('long'),\\\n",
    "        col('features_85.size').alias('features85_size').cast('long'),\\\n",
    "        col('features_85.indices').alias('features85_indices'),\\\n",
    "        col('features_85.values').alias('features85_values'),\\\n",
    "        col('ngramscounts_7.type').alias('ngramscounts7_type').cast('long'),\\\n",
    "        col('ngramscounts_7.size').alias('ngramscounts7_size').cast('long'),\\\n",
    "        col('ngramscounts_7.indices').alias('ngramscounts7_indices'),\\\n",
    "        col('ngramscounts_7.values').alias('ngramscounts7_values'))\n",
    "fraud_label_read_df.printSchema()\n",
    "#\n",
    "# ABOVE ARE CASE ISSUES on struct Struct of features_85 and ngramscounts_7 \n",
    "# Both cares conversion to DF valide type list\n",
    "# Flat vars for each, individually and seperately from the original struct\n",
    "#\n",
    "# https://stackoverflow.com/questions/47401418/pyspark-conversion-to-array-types?rq=1 \n",
    "#\n",
    "#\n",
    "fraud_fraud_label_read1_df=fraud_label_read_df.filter(\"fraud_label=1\")\\\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "notfraud_fraud_label_read1_df=fraud_label_read_df.filter(\"fraud_label=0\")\\\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "#\n",
    "fraud_fraud_label_read1_df.printSchema()\n",
    "notfraud_fraud_label_read1_df.printSchema()\n",
    "#\n",
    "drop_list_cols=['features85_indices','features85_values','ngramscounts7_indices','ngramscounts7_values']\n",
    "#\n",
    "### 1.) https://stackoverflow.com/questions/38610559/convert-spark-dataframe-column-to-python-list\n",
    "###    list(spark_df.select('mvv').toPandas()['mvv'])\n",
    "### 2.) http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html\n",
    "###    df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32,temp_k=lambda x: (x['temp_f'] +  459.67) * 5 / 9)\n",
    "### 3.) https://stackoverflow.com/questions/43216411/pandas-flatten-a-list-of-list-within-a-column\n",
    "###    df['var2'] = df['var2'].apply(np.ravel)\n",
    "### 4.) Random xxx rows\n",
    "###    df.orderBy(rand()).limit(n)\n",
    "from pyspark.sql.functions import rand\n",
    "#\n",
    "fraud_label_train_pd_rand=fraud_fraud_label_read1_df.limit(10000)\\\n",
    ".orderBy(rand()).persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "#\n",
    "fraud_label_train_pd=fraud_label_train_pd_rand.limit(3100).toPandas()\\\n",
    ".assign(features85_list_indices=lambda x: x['features85_indices'].apply(np.ravel),\\\n",
    "        features85_list_values=lambda x: x['features85_values'].apply(np.ravel),\\\n",
    "        ngramscounts7_list_indices=lambda x: x['ngramscounts7_indices'].apply(np.ravel),\\\n",
    "        ngramscounts7_list_values=lambda x: x['ngramscounts7_values'].apply(np.ravel))\\\n",
    ".drop(drop_list_cols, axis=1, inplace=False)\n",
    "#\n",
    "fraud_label_test_pd_rand=fraud_fraud_label_read1_df.limit(10000)\\\n",
    ".orderBy(rand()).persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "#\n",
    "fraud_label_test_pd=fraud_label_test_pd_rand.limit(500).toPandas()\\\n",
    ".assign(features85_list_indices=lambda x: x['features85_indices'].apply(np.ravel),\\\n",
    "        features85_list_values=lambda x: x['features85_values'].apply(np.ravel),\\\n",
    "        ngramscounts7_list_indices=lambda x: x['ngramscounts7_indices'].apply(np.ravel),\\\n",
    "        ngramscounts7_list_values=lambda x: x['ngramscounts7_values'].apply(np.ravel))\\\n",
    ".drop(drop_list_cols, axis=1, inplace=False)\n",
    "#\n",
    "fraud_label_train=h2o.H2OFrame(fraud_label_train_pd)\n",
    "fraud_label_test=h2o.H2OFrame(fraud_label_test_pd)\n",
    "#\n",
    "not_fraud_label_train_pd_rand=notfraud_fraud_label_read1_df.limit(10000)\\\n",
    ".orderBy(rand()).persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "#\n",
    "not_fraud_label_train_pd=not_fraud_label_train_pd_rand.limit(3100).toPandas()\\\n",
    ".assign(features85_list_indices=lambda x: x['features85_indices'].apply(np.ravel),\\\n",
    "        features85_list_values=lambda x: x['features85_values'].apply(np.ravel),\\\n",
    "        ngramscounts7_list_indices=lambda x: x['ngramscounts7_indices'].apply(np.ravel),\\\n",
    "        ngramscounts7_list_values=lambda x: x['ngramscounts7_values'].apply(np.ravel))\\\n",
    ".drop(drop_list_cols, axis=1, inplace=False)\n",
    "#\n",
    "not_fraud_label_test_pd_rand=notfraud_fraud_label_read1_df.limit(10000)\\\n",
    ".orderBy(rand()).persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "#\n",
    "not_fraud_label_test_pd=not_fraud_label_test_pd_rand.limit(800).toPandas()\\\n",
    ".assign(features85_list_indices=lambda x: x['features85_indices'].apply(np.ravel),\\\n",
    "        features85_list_values=lambda x: x['features85_values'].apply(np.ravel),\\\n",
    "        ngramscounts7_list_indices=lambda x: x['ngramscounts7_indices'].apply(np.ravel),\\\n",
    "        ngramscounts7_list_values=lambda x: x['ngramscounts7_values'].apply(np.ravel))\\\n",
    ".drop(drop_list_cols, axis=1, inplace=False)\n",
    "#.orderBy(rand())\\\n",
    "#.sort(notfraud_fraud_label_read1_df.kl_notfraud_words.desc())\\\n",
    "#\n",
    "not_fraud_label_train=h2o.H2OFrame(not_fraud_label_train_pd)\n",
    "not_fraud_label_test=h2o.H2OFrame(not_fraud_label_test_pd)\n",
    "#\n",
    "################# Use Two DataFrames ##################### - rbind() H2o Frames issue\n",
    "#\n",
    "#\n",
    "###### TRAINING PROCESS ############\n",
    "# RBIND \"Merge\" all of vars internal subset of data with fraud and with not_fraud\n",
    "# function merge() doesn't work if both H2O/dataframes have same variables\n",
    "#\n",
    "train = fraud_label_train.rbind(not_fraud_label_train)\n",
    "test = fraud_label_test.rbind(not_fraud_label_test)\n",
    "#\n",
    "print(\"train\")\n",
    "print(train.head(10))\n",
    "print(\"test\")\n",
    "print(test.head(10))\n",
    "#\n",
    "# Identify predictors and response\n",
    "x = train.columns\n",
    "#\n",
    "#Â Fraud Label to be learned in the model from the atrributes of the ngram85 learned words\n",
    "#\n",
    "y= 'fraud_label'\n",
    "x.remove(y)\n",
    "#\n",
    "# For binary classification, response should be a factor\n",
    "train[y] = train[y].asfactor()\n",
    "test[y] = test[y].asfactor()\n",
    "#\n",
    "# http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html\n",
    "#Â Balance Classes to compensate unbalanced data\n",
    "# Run AutoML for 25 base models (limited to 1 hour max runtime by default)\n",
    "aml = H2OAutoML(max_models=30, seed=1999, exclude_algos=[\"DRF\",\"GLM\"])\n",
    "aml.train(x=x, y=y, training_frame=train)\n",
    "#\n",
    "#preserve_training_output.write.json(preserve_training_output_file)\n",
    "#\n",
    "print(\"AutoML Modeling Done!\")\n",
    "#\n",
    "# View the AutoML Leaderboard\n",
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)\n",
    "#\n",
    "# The leader model is stored here\n",
    "aml.leader\n",
    "#\n",
    "# Get model ids for all models in the AutoML Leaderboard\n",
    "model_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])\n",
    "print(model_ids)\n",
    "# Get the \"All Models\" Stacked Ensemble model\n",
    "se = h2o.get_model([mid for mid in model_ids if \"StackedEnsemble_AllModels\" in mid][0])\n",
    "print(se)\n",
    "# Get the Stacked Ensemble metalearner model\n",
    "#metalearner = h2o.get_model(aml.leader.metalearner()['name'])\n",
    "#metalearner.coef_norm()\n",
    "#%matplotlib inline\n",
    "#metalearner.std_coef_plot()\n",
    "# If you need to generate predictions on a test set, you can make\n",
    "# predictions directly on the `\"H2OAutoML\"` object, or on the leader\n",
    "# model object directly\n",
    "\n",
    "#preds = aml.predict(test)\n",
    "# or:\n",
    "preds = aml.leader.predict(test)\n",
    "print(\"test\")\n",
    "print(test.head(10))\n",
    "print(\"prediction\")\n",
    "print(preds.head(10))\n",
    "#\n",
    "#\n",
    "print(\"Save Model For Future Usage\")\n",
    "aml.leader.download_mojo(path = \"./projects/logs-archive-production/fraud-canada-tokenizedwords/notebooks/product_model_bin/ngrams7_features85_m30/v\"+process_date+\"/mojo\", get_genmodel_jar = True)\n",
    "# If you need to generate predictions on a test set, you can make\n",
    "# predictions directly on the `\"H2OAutoML\"` object, or on the leader\n",
    "# model object directly\n",
    "\n",
    "#preds = aml.predict(test)\n",
    "# or:\n",
    "preds = aml.leader.predict(test)\n",
    "print(\"test\")\n",
    "print(test.tail(10))\n",
    "print(\"prediction\")\n",
    "print(preds.tail(10))\n",
    "# If you need to generate predictions on a test set, you can make\n",
    "# predictions directly on the `\"H2OAutoML\"` object, or on the leader\n",
    "# model object directly\n",
    "\n",
    "#preds = aml.predict(test)\n",
    "# or:\n",
    "preds_over_all_hf = aml.leader.predict(train)\n",
    "path_out_file1=\"./projects/logs-archive-production/fraud-canada-tokenizedwords/notebooks/product_model_prediction/file_prediction_\"+process_date+\".csv\"\n",
    "output_pred_file=h2o.export_file(frame=preds_over_all_hf, path=path_out_file1, force=False)\n",
    "#\n",
    "train_over_all_hf = train\n",
    "path_out_file2=\"./projects/logs-archive-production/fraud-canada-tokenizedwords/notebooks/product_model_prediction/file_train_\"+process_date+\".csv\"\n",
    "output_pred_file=h2o.export_file(frame=train_over_all_hf, path=path_out_file2, force=False)\n",
    "#\n",
    "sc.stop()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                           </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">    logloss</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">      rmse</th><th style=\"text-align: right;\">        mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>GBM_grid_1_AutoML_20190206_120744_model_2          </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.00206106 </td><td style=\"text-align: right;\">           0          </td><td style=\"text-align: right;\">0.0155499 </td><td style=\"text-align: right;\">0.0002418  </td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190206_120744_model_8          </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.00222091 </td><td style=\"text-align: right;\">           0          </td><td style=\"text-align: right;\">0.0155514 </td><td style=\"text-align: right;\">0.000241847</td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190206_120744_model_7          </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.00201646 </td><td style=\"text-align: right;\">           0          </td><td style=\"text-align: right;\">0.0177354 </td><td style=\"text-align: right;\">0.000314545</td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190206_120744_model_9          </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.00117409 </td><td style=\"text-align: right;\">           0          </td><td style=\"text-align: right;\">0.0152584 </td><td style=\"text-align: right;\">0.00023282 </td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190206_120744_model_1          </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.000805172</td><td style=\"text-align: right;\">           0          </td><td style=\"text-align: right;\">0.0148946 </td><td style=\"text-align: right;\">0.000221848</td></tr>\n",
       "<tr><td>GBM_1_AutoML_20190206_120744                       </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.00230928 </td><td style=\"text-align: right;\">           0          </td><td style=\"text-align: right;\">0.0200727 </td><td style=\"text-align: right;\">0.000402911</td></tr>\n",
       "<tr><td>StackedEnsemble_AllModels_AutoML_20190206_120744   </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.000899946</td><td style=\"text-align: right;\">           0          </td><td style=\"text-align: right;\">0.0149675 </td><td style=\"text-align: right;\">0.000224026</td></tr>\n",
       "<tr><td>DeepLearning_grid_1_AutoML_20190206_120744_model_2 </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.000246943</td><td style=\"text-align: right;\">           0          </td><td style=\"text-align: right;\">0.0064663 </td><td style=\"text-align: right;\">4.1813e-05 </td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190206_120744_model_6          </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.00108807 </td><td style=\"text-align: right;\">           0          </td><td style=\"text-align: right;\">0.0148655 </td><td style=\"text-align: right;\">0.000220985</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_AutoML_20190206_120744</td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.000510137</td><td style=\"text-align: right;\">           0          </td><td style=\"text-align: right;\">0.00936266</td><td style=\"text-align: right;\">8.76594e-05</td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190206_120744_model_10         </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.00145199 </td><td style=\"text-align: right;\">           0          </td><td style=\"text-align: right;\">0.0155145 </td><td style=\"text-align: right;\">0.000240701</td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190206_120744_model_5          </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.00196495 </td><td style=\"text-align: right;\">           0          </td><td style=\"text-align: right;\">0.0173824 </td><td style=\"text-align: right;\">0.000302149</td></tr>\n",
       "<tr><td>GBM_4_AutoML_20190206_120744                       </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.0020286  </td><td style=\"text-align: right;\">           0.000178063</td><td style=\"text-align: right;\">0.0168805 </td><td style=\"text-align: right;\">0.000284952</td></tr>\n",
       "<tr><td>GBM_2_AutoML_20190206_120744                       </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.00234997 </td><td style=\"text-align: right;\">           0.000178063</td><td style=\"text-align: right;\">0.0203544 </td><td style=\"text-align: right;\">0.0004143  </td></tr>\n",
       "<tr><td>DeepLearning_1_AutoML_20190206_120744              </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.000785098</td><td style=\"text-align: right;\">           0.000178063</td><td style=\"text-align: right;\">0.0152178 </td><td style=\"text-align: right;\">0.000231582</td></tr>\n",
       "<tr><td>GBM_3_AutoML_20190206_120744                       </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.00192596 </td><td style=\"text-align: right;\">           0.000178063</td><td style=\"text-align: right;\">0.0156542 </td><td style=\"text-align: right;\">0.000245055</td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190206_120744_model_3          </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.00194118 </td><td style=\"text-align: right;\">           0.000178063</td><td style=\"text-align: right;\">0.0165038 </td><td style=\"text-align: right;\">0.000272376</td></tr>\n",
       "<tr><td>GBM_5_AutoML_20190206_120744                       </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">0.00220945 </td><td style=\"text-align: right;\">           0.000178063</td><td style=\"text-align: right;\">0.0188732 </td><td style=\"text-align: right;\">0.000356199</td></tr>\n",
       "<tr><td>DeepLearning_grid_1_AutoML_20190206_120744_model_10</td><td style=\"text-align: right;\">0.999994</td><td style=\"text-align: right;\">0.00671421 </td><td style=\"text-align: right;\">           0.000377074</td><td style=\"text-align: right;\">0.0238686 </td><td style=\"text-align: right;\">0.00056971 </td></tr>\n",
       "<tr><td>DeepLearning_grid_1_AutoML_20190206_120744_model_3 </td><td style=\"text-align: right;\">0.999992</td><td style=\"text-align: right;\">0.0048511  </td><td style=\"text-align: right;\">           0.00148735 </td><td style=\"text-align: right;\">0.0353717 </td><td style=\"text-align: right;\">0.00125116 </td></tr>\n",
       "<tr><td>DeepLearning_grid_1_AutoML_20190206_120744_model_5 </td><td style=\"text-align: right;\">0.99999 </td><td style=\"text-align: right;\">0.00483422 </td><td style=\"text-align: right;\">           0.00126739 </td><td style=\"text-align: right;\">0.0398346 </td><td style=\"text-align: right;\">0.0015868  </td></tr>\n",
       "<tr><td>DeepLearning_grid_1_AutoML_20190206_120744_model_7 </td><td style=\"text-align: right;\">0.999988</td><td style=\"text-align: right;\">0.0146387  </td><td style=\"text-align: right;\">           0.00148735 </td><td style=\"text-align: right;\">0.0542272 </td><td style=\"text-align: right;\">0.00294059 </td></tr>\n",
       "<tr><td>DeepLearning_grid_1_AutoML_20190206_120744_model_8 </td><td style=\"text-align: right;\">0.999974</td><td style=\"text-align: right;\">0.00845242 </td><td style=\"text-align: right;\">           0.00239861 </td><td style=\"text-align: right;\">0.0461237 </td><td style=\"text-align: right;\">0.0021274  </td></tr>\n",
       "<tr><td>DeepLearning_grid_1_AutoML_20190206_120744_model_4 </td><td style=\"text-align: right;\">0.999951</td><td style=\"text-align: right;\">0.0121876  </td><td style=\"text-align: right;\">           0.00326797 </td><td style=\"text-align: right;\">0.0607309 </td><td style=\"text-align: right;\">0.00368824 </td></tr>\n",
       "<tr><td>DeepLearning_grid_1_AutoML_20190206_120744_model_1 </td><td style=\"text-align: right;\">0.999933</td><td style=\"text-align: right;\">0.0171685  </td><td style=\"text-align: right;\">           0.00404307 </td><td style=\"text-align: right;\">0.0659124 </td><td style=\"text-align: right;\">0.00434445 </td></tr>\n",
       "<tr><td>GBM_grid_1_AutoML_20190206_120744_model_4          </td><td style=\"text-align: right;\">0.999925</td><td style=\"text-align: right;\">0.00266054 </td><td style=\"text-align: right;\">           0.000377074</td><td style=\"text-align: right;\">0.020666  </td><td style=\"text-align: right;\">0.000427082</td></tr>\n",
       "<tr><td>DeepLearning_grid_1_AutoML_20190206_120744_model_6 </td><td style=\"text-align: right;\">0.999178</td><td style=\"text-align: right;\">0.0913167  </td><td style=\"text-align: right;\">           0.00809661 </td><td style=\"text-align: right;\">0.110779  </td><td style=\"text-align: right;\">0.012272   </td></tr>\n",
       "<tr><td>DeepLearning_grid_1_AutoML_20190206_120744_model_9 </td><td style=\"text-align: right;\">0.997658</td><td style=\"text-align: right;\">0.0592709  </td><td style=\"text-align: right;\">           0.00560374 </td><td style=\"text-align: right;\">0.0800222 </td><td style=\"text-align: right;\">0.00640356 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the AutoML Leaderboard\n",
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OGradientBoostingEstimator :  Gradient Boosting Machine\n",
      "Model Key:  GBM_grid_1_AutoML_20190206_120744_model_2\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.0\n",
      "RMSE: 0.0\n",
      "LogLoss: 0.0\n",
      "Mean Per-Class Error: 0.0\n",
      "AUC: 1.0\n",
      "pr_auc: 0.0\n",
      "Gini: 1.0\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 1.0: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>2808.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/2808.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.0</td>\n",
       "<td>1326.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/1326.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>2808.0</td>\n",
       "<td>1326.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/4134.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1     Error    Rate\n",
       "-----  ----  ----  -------  ------------\n",
       "0      2808  0     0        (0.0/2808.0)\n",
       "1      0     1326  0        (0.0/1326.0)\n",
       "Total  2808  1326  0        (0.0/4134.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value    idx\n",
       "---------------------------  -----------  -------  -----\n",
       "max f1                       1            1        0\n",
       "max f2                       1            1        0\n",
       "max f0point5                 1            1        0\n",
       "max accuracy                 1            1        0\n",
       "max precision                1            1        0\n",
       "max recall                   1            1        0\n",
       "max specificity              1            1        0\n",
       "max absolute_mcc             1            1        0\n",
       "max min_per_class_accuracy   1            1        0\n",
       "max mean_per_class_accuracy  1            1        0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 32.08 %, avg score: 32.08 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.3207547</td>\n",
       "<td>1.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>1.0</td>\n",
       "<td>1e-19</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.3207547</td>\n",
       "<td>0.3207547</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.3207547</td>\n",
       "<td>0.3207547</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift     cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain     cumulative_gain\n",
       "--  -------  --------------------------  -----------------  -------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  -------  -----------------\n",
       "    1        0.320755                    1                  3.11765  3.11765            1                1            1                           1                   1               1                          211.765  211.765\n",
       "    2        1                           1e-19              0        1                  0                1.04759e-19  0.320755                    0.320755            0               1                          -100     0\n",
       "    3        1                           0                  0        1                  0                0            0.320755                    0.320755            0               1                          -100     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 5.015211034453588e-26\n",
      "RMSE: 2.2394666852743284e-13\n",
      "LogLoss: 1.4487391918149767e-14\n",
      "Mean Per-Class Error: 0.0\n",
      "AUC: 1.0\n",
      "pr_auc: 0.0\n",
      "Gini: 1.0\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.9999999999999561: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>292.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/292.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.0</td>\n",
       "<td>144.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/144.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>292.0</td>\n",
       "<td>144.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/436.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  -----------\n",
       "0      292  0    0        (0.0/292.0)\n",
       "1      0    144  0        (0.0/144.0)\n",
       "Total  292  144  0        (0.0/436.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value    idx\n",
       "---------------------------  -----------  -------  -----\n",
       "max f1                       1            1        0\n",
       "max f2                       1            1        0\n",
       "max f0point5                 1            1        0\n",
       "max accuracy                 1            1        0\n",
       "max precision                1            1        0\n",
       "max recall                   1            1        0\n",
       "max specificity              1            1        0\n",
       "max absolute_mcc             1            1        0\n",
       "max min_per_class_accuracy   1            1        0\n",
       "max mean_per_class_accuracy  1            1        0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 33.03 %, avg score: 33.03 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.3211009</td>\n",
       "<td>1.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>3.0277778</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9722222</td>\n",
       "<td>0.9722222</td>\n",
       "<td>202.7777778</td>\n",
       "<td>202.7777778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>1.0</td>\n",
       "<td>1e-19</td>\n",
       "<td>0.0409159</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0135135</td>\n",
       "<td>0.0135135</td>\n",
       "<td>0.3302752</td>\n",
       "<td>0.3302752</td>\n",
       "<td>0.0277778</td>\n",
       "<td>1.0</td>\n",
       "<td>-95.9084084</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.321101                    1                  3.02778    3.02778            1                1          1                           1                   0.972222        0.972222                   202.778   202.778\n",
       "    2        1                           1e-19              0.0409159  1                  0.0135135        0.0135135  0.330275                    0.330275            0.0277778       1                          -95.9084  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.00024180003124528305\n",
      "RMSE: 0.015549920618616772\n",
      "LogLoss: 0.002061060670068686\n",
      "Mean Per-Class Error: 0.0\n",
      "AUC: 1.0\n",
      "pr_auc: 0.003016591251885359\n",
      "Gini: 1.0\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.00019935528931665344: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>2808.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/2808.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.0</td>\n",
       "<td>1326.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/1326.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>2808.0</td>\n",
       "<td>1326.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/4134.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1     Error    Rate\n",
       "-----  ----  ----  -------  ------------\n",
       "0      2808  0     0        (0.0/2808.0)\n",
       "1      0     1326  0        (0.0/1326.0)\n",
       "Total  2808  1326  0        (0.0/4134.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value    idx\n",
       "---------------------------  -----------  -------  -----\n",
       "max f1                       0.000199355  1        4\n",
       "max f2                       0.000199355  1        4\n",
       "max f0point5                 0.000199355  1        4\n",
       "max accuracy                 0.000199355  1        4\n",
       "max precision                1            1        0\n",
       "max recall                   0.000199355  1        4\n",
       "max specificity              1            1        0\n",
       "max absolute_mcc             0.000199355  1        4\n",
       "max min_per_class_accuracy   0.000199355  1        4\n",
       "max mean_per_class_accuracy  0.000199355  1        4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 32.08 %, avg score: 32.05 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.2472182</td>\n",
       "<td>1.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7707391</td>\n",
       "<td>0.7707391</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.3040639</td>\n",
       "<td>1.0000000</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0000000</td>\n",
       "<td>0.1772247</td>\n",
       "<td>0.9479638</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.4511369</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.3538119</td>\n",
       "<td>2.2166220</td>\n",
       "<td>0.1134868</td>\n",
       "<td>0.1118424</td>\n",
       "<td>0.7109920</td>\n",
       "<td>0.7104559</td>\n",
       "<td>0.0520362</td>\n",
       "<td>1.0</td>\n",
       "<td>-64.6188080</td>\n",
       "<td>121.6621984</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>1.0</td>\n",
       "<td>1e-19</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.3207547</td>\n",
       "<td>0.3205129</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.247218                    1                  3.11765   3.11765            1                1            1                           1                   0.770739        0.770739                   211.765   211.765\n",
       "    2        0.304064                    1                  3.11765   3.11765            1                1            1                           1                   0.177225        0.947964                   211.765   211.765\n",
       "    3        0.451137                    1.8349e-13         0.353812  2.21662            0.113487         0.111842     0.710992                    0.710456            0.0520362       1                          -64.6188  121.662\n",
       "    4        1                           1e-19              0         1                  0                2.17978e-15  0.320755                    0.320513            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Metrics Summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>mean</b></td>\n",
       "<td><b>sd</b></td>\n",
       "<td><b>cv_1_valid</b></td>\n",
       "<td><b>cv_2_valid</b></td>\n",
       "<td><b>cv_3_valid</b></td>\n",
       "<td><b>cv_4_valid</b></td>\n",
       "<td><b>cv_5_valid</b></td></tr>\n",
       "<tr><td>accuracy</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>auc</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>err</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>err_count</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>f0point5</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>f1</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>f2</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>lift_top_group</td>\n",
       "<td>3.1176536</td>\n",
       "<td>0.0032218</td>\n",
       "<td>3.1090226</td>\n",
       "<td>3.1207547</td>\n",
       "<td>3.1207547</td>\n",
       "<td>3.1207547</td>\n",
       "<td>3.116981</td></tr>\n",
       "<tr><td>logloss</td>\n",
       "<td>0.0020606</td>\n",
       "<td>0.0029141</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0103028</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td></tr>\n",
       "<tr><td>max_per_class_error</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>mcc</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>mean_per_class_accuracy</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>mean_per_class_error</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>mse</td>\n",
       "<td>0.0002417</td>\n",
       "<td>0.0003419</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0012087</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td></tr>\n",
       "<tr><td>precision</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>r2</td>\n",
       "<td>0.9988899</td>\n",
       "<td>0.0015700</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9944493</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>recall</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>rmse</td>\n",
       "<td>0.0069533</td>\n",
       "<td>0.0098334</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0347665</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000001</td></tr>\n",
       "<tr><td>specificity</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr></table></div>"
      ],
      "text/plain": [
       "                         mean         sd           cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n",
       "-----------------------  -----------  -----------  ------------  ------------  ------------  ------------  ------------\n",
       "accuracy                 1            0            1             1             1             1             1\n",
       "auc                      1            0            1             1             1             1             1\n",
       "err                      0            0            0             0             0             0             0\n",
       "err_count                0            0            0             0             0             0             0\n",
       "f0point5                 1            0            1             1             1             1             1\n",
       "f1                       1            0            1             1             1             1             1\n",
       "f2                       1            0            1             1             1             1             1\n",
       "lift_top_group           3.11765      0.00322177   3.10902       3.12075       3.12075       3.12075       3.11698\n",
       "logloss                  0.00206056   0.00291407   6.11898e-16   0.0103028     8.12732e-16   3.45306e-10   2.31957e-09\n",
       "max_per_class_error      0            0            0             0             0             0             0\n",
       "mcc                      1            0            1             1             1             1             1\n",
       "mean_per_class_accuracy  1            0            1             1             1             1             1\n",
       "mean_per_class_error     0            0            0             0             0             0             0\n",
       "mse                      0.000241742  0.000341874  2.83048e-28   0.00120871    4.16074e-28   9.86047e-17   4.43994e-15\n",
       "precision                1            0            1             1             1             1             1\n",
       "r2                       0.99889      0.00156998   1             0.994449      1             1             1\n",
       "recall                   1            0            1             1             1             1             1\n",
       "rmse                     0.00695331   0.00983344   1.6824e-14    0.0347665     2.03979e-14   9.92999e-09   6.66329e-08\n",
       "specificity              1            0            1             1             1             1             1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_pr_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_logloss</b></td>\n",
       "<td><b>validation_auc</b></td>\n",
       "<td><b>validation_pr_auc</b></td>\n",
       "<td><b>validation_lift</b></td>\n",
       "<td><b>validation_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 11.906 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>0.4667667</td>\n",
       "<td>0.6274370</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.6792453</td>\n",
       "<td>0.4704085</td>\n",
       "<td>0.6345803</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.6697248</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 11.924 sec</td>\n",
       "<td>5.0</td>\n",
       "<td>0.0051481</td>\n",
       "<td>0.0051366</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0007541</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0051382</td>\n",
       "<td>0.0051386</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 11.947 sec</td>\n",
       "<td>10.0</td>\n",
       "<td>0.0000994</td>\n",
       "<td>0.0000960</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9962293</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0001229</td>\n",
       "<td>0.0001007</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9722222</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 11.993 sec</td>\n",
       "<td>15.0</td>\n",
       "<td>0.0000021</td>\n",
       "<td>0.0000018</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9947210</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000032</td>\n",
       "<td>0.0000020</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0347222</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.040 sec</td>\n",
       "<td>20.0</td>\n",
       "<td>0.0000001</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0316742</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000034</td>\n",
       "<td>0.0000002</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0277778</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.098 sec</td>\n",
       "<td>25.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000001</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0208333</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.155 sec</td>\n",
       "<td>30.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0069444</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.209 sec</td>\n",
       "<td>35.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.262 sec</td>\n",
       "<td>40.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.306 sec</td>\n",
       "<td>45.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.337 sec</td>\n",
       "<td>50.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.359 sec</td>\n",
       "<td>55.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.379 sec</td>\n",
       "<td>60.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.398 sec</td>\n",
       "<td>65.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.416 sec</td>\n",
       "<td>70.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:02</td>\n",
       "<td> 1 min 12.434 sec</td>\n",
       "<td>75.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:02</td>\n",
       "<td> 1 min 12.447 sec</td>\n",
       "<td>77.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration          number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
       "--  -------------------  ----------------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
       "    2019-02-06 12:10:01  1 min 11.906 sec  0                  0.466767         0.627437            0.5             0                  1                0.679245                         0.470408           0.63458               0.5               0                    1                  0.669725\n",
       "    2019-02-06 12:10:01  1 min 11.924 sec  5                  0.00514813       0.00513664          1               0.000754148        3.11765          0                                0.00513816         0.00513856            1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 11.947 sec  10                 9.94457e-05      9.59938e-05         1               0.996229           3.11765          0                                0.000122915        0.000100685           1                 0.972222             3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 11.993 sec  15                 2.12199e-06      1.79244e-06         1               0.994721           3.11765          0                                3.22373e-06        1.95473e-06           1                 0.0347222            3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.040 sec  20                 8.61559e-08      3.7072e-08          1               0.0316742          3.11765          0                                3.43066e-06        2.11577e-07           1                 0.0277778            3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.098 sec  25                 1.58077e-09      6.82417e-10         1               0                  3.11765          0                                6.28394e-08        3.88416e-09           1                 0.0208333            3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.155 sec  30                 3.94149e-11      1.3919e-11          1               0                  3.11765          0                                5.71013e-09        3.16675e-10           1                 0.00694444           3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.209 sec  35                 1.17942e-12      3.00574e-13         1               0                  3.11765          0                                5.19531e-10        2.85585e-11           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.262 sec  40                 2.17793e-14      5.61885e-15         1               0                  3.11765          0                                9.58097e-12        5.56681e-13           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.306 sec  45                 7.78488e-16      1.56543e-16         1               0                  3.11765          0                                2.42509e-12        1.51623e-13           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.337 sec  50                 4.90222e-17      5.21005e-18         1               0                  3.11765          0                                2.90144e-13        1.89033e-14           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.359 sec  55                 1.35963e-17      8.59389e-19         1               0                  3.11765          0                                7.57417e-13        4.29116e-14           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.379 sec  60                 2.18416e-17      2.14847e-18         1               0                  3.11765          0                                1.2903e-12         8.57031e-14           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.398 sec  65                 9.76787e-18      4.29694e-19         1               0                  3.11765          0                                7.30362e-13        4.91824e-14           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.416 sec  70                 4.88394e-18      1.07424e-19         1               0                  3.11765          0                                4.54014e-13        2.81258e-14           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:02  1 min 12.434 sec  75                 4.88394e-18      1.07424e-19         1               0                  3.11765          0                                2.1285e-13         1.31859e-14           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:02  1 min 12.447 sec  77                 0                0                   1               0                  3.11765          0                                2.23947e-13        1.44874e-14           1                 0                    3.02778            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>features85_list_indices</td>\n",
       "<td>900.5152588</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9144560</td></tr>\n",
       "<tr><td>features85_list_values</td>\n",
       "<td>84.0486832</td>\n",
       "<td>0.0933340</td>\n",
       "<td>0.0853498</td></tr>\n",
       "<tr><td>ngramscounts7_list_values</td>\n",
       "<td>0.1673868</td>\n",
       "<td>0.0001859</td>\n",
       "<td>0.0001700</td></tr>\n",
       "<tr><td>entropy_fraud_words</td>\n",
       "<td>0.0234560</td>\n",
       "<td>0.0000260</td>\n",
       "<td>0.0000238</td></tr>\n",
       "<tr><td>kl_notfraud_words</td>\n",
       "<td>0.0003148</td>\n",
       "<td>0.0000003</td>\n",
       "<td>0.0000003</td></tr>\n",
       "<tr><td>kl_fraud_words</td>\n",
       "<td>0.0000837</td>\n",
       "<td>0.0000001</td>\n",
       "<td>0.0000001</td></tr>\n",
       "<tr><td>ks_fraud_words</td>\n",
       "<td>0.0000086</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td></tr>\n",
       "<tr><td>entropy_notfraud_words</td>\n",
       "<td>0.0000024</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td></tr>\n",
       "<tr><td>ks_notfraud_words</td>\n",
       "<td>0.0000011</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td></tr></table></div>"
      ],
      "text/plain": [
       "variable                   relative_importance    scaled_importance    percentage\n",
       "-------------------------  ---------------------  -------------------  ------------\n",
       "features85_list_indices    900.515                1                    0.914456\n",
       "features85_list_values     84.0487                0.093334             0.0853498\n",
       "ngramscounts7_list_values  0.167387               0.000185879          0.000169978\n",
       "entropy_fraud_words        0.023456               2.60473e-05          2.38191e-05\n",
       "kl_notfraud_words          0.000314826            3.49606e-07          3.19699e-07\n",
       "kl_fraud_words             8.36856e-05            9.29308e-08          8.49811e-08\n",
       "ks_fraud_words             8.59057e-06            9.53962e-09          8.72356e-09\n",
       "entropy_notfraud_words     2.35438e-06            2.61448e-09          2.39083e-09\n",
       "ks_notfraud_words          1.09123e-06            1.21178e-09          1.10812e-09"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The leader model is stored here\n",
    "aml.leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm prediction progress: |ââââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/cloudera/parcels/Anaconda-5.1.0.1/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'features85_list_indices' has levels not trained on: [[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 61 62 63 65 66 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 58 61 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 56 60 61 64 66 67 75 77], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 55 56 57 58 61 62 63 65 66 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 57 58 59 61 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 57 58 60 61 65 66 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 58 59 61 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 58 61 62 63 67], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 48 49 50 51 52\n",
      " 54 55 56 58 61 62 63 65 68 69 70 71 72 73 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 57 58 59 60 66 67 77], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 58 65 77 81], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 56 58 65 77 81 82], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 54 55 57 58 60 65 66 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 54 55 57 58 60 65 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 54 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 54 58 62 63 65\n",
      " 68 69 70 71 72 73 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 54 62 63 65 68\n",
      " 69 70 71 72 73 78], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 43 44 45 46 47 53 58 59 64 76\n",
      " 81], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 54 55 56 57 58 60 61 64 66 67 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 54 55 56 58 60 61 64 67 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 36 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 58 59 60 64 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 54 55 56 57 58 60 66 67 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 55 56 58 59 60 67 74 77], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 48 49 50 51 52 53 54 55\n",
      " 58 59 60 68 69 70 71 72 73 74 78], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 54 56 57 61 62 63 65 66], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 48 49 50 51 52 54\n",
      " 56 58 60 64 67 68 69 70 71 72 73 77], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 54 56 57 65 66 77 81 82], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 50 51 52 53 54 56\n",
      " 65 68 69 70 71 72 73 77 78 81 82], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 50 51 52 54 56 58\n",
      " 59 64 65 68 69 70 71 72 73 77 81], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 42 43 44 45 46 47 53 57 59 76 81], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 48 49 50 51 52 53 54 56 58\n",
      " 60 67 68 69 70 71 72 73 74], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 48 49 50 51 52 53 54 58 59\n",
      " 60 68 69 70 71 72 73 74 78], [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 42 43 44 45 46 47 53 55 57 58 62 63 64\n",
      " 66 67 74 75], [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 42 43 44 45 46 47 53 57 58 62 63 64 67\n",
      " 74 75], [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 43 44 45 46 47 53 57 58 62 63 64 67 74\n",
      " 75 82], [ 0  1  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 40 41 43 44 45 46 47 57 58 60 82], [ 0  1  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 40 41 43 44 45 46 47 57 60 82], [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 38 39 43 44 45 46 47 53 57 59 66 76\n",
      " 81], [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 42 43 44 45 46 47 53 59 76 81 82], [ 1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 38 39 43 44 45 46 47 53 57 58 59 66 76 81], [ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 40 41 42 43 44 45 46 47 55 57 58 60]]\n",
      "  warnings.warn(w)\n",
      "/opt/cloudera/parcels/Anaconda-5.1.0.1/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'features85_list_values' has levels not trained on: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 1.], [1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [1. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1.], [1. 2. 2. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1.], [1. 2. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1.], [1. 2. 3. 3. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1.], [1. 4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.\n",
      " 2. 1.], [1. 4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1.\n",
      " 2. 1.], [1. 4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.], [1. 4. 4. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 3.], [1. 4. 5. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 3.], [1. 4. 5. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [2. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 1. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 4. 2. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 3.], [2. 4. 3. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 4. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [3. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1.], [3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1.], [3. 2. 2. 3. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1.], [3. 2. 2. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1.], [3. 2. 2. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1.], [3. 2. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.], [3. 4. 2. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3. 1.], [3. 4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [3. 4. 3. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [4. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 2. 2.\n",
      " 1.], [4. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1.\n",
      " 1. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 2.\n",
      " 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 3. 1.\n",
      " 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 2.\n",
      " 1. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1.], [4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1.], [4. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 4. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2.\n",
      " 1.], [5. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [5. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [6. 2. 3. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.]]\n",
      "  warnings.warn(w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>hash_message                                                                                                                    </th><th style=\"text-align: right;\">  fraud_label</th><th style=\"text-align: right;\">  kl_fraud_words</th><th style=\"text-align: right;\">  ks_fraud_words</th><th style=\"text-align: right;\">  entropy_fraud_words</th><th style=\"text-align: right;\">  kl_notfraud_words</th><th style=\"text-align: right;\">  ks_notfraud_words</th><th style=\"text-align: right;\">  entropy_notfraud_words</th><th style=\"text-align: right;\">  features85_type</th><th style=\"text-align: right;\">  features85_size</th><th style=\"text-align: right;\">  ngramscounts7_type</th><th style=\"text-align: right;\">  ngramscounts7_size</th><th>features85_list_indices  </th><th>features85_list_values  </th><th>ngramscounts7_list_indices  </th><th>ngramscounts7_list_values  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>101642031f7aa7d62d25fbc9dc190f6165b1efd53d20eea9a7405968fc32ba6da3db74ea520bd5910bc52db1acc8f3bb069e415572a344186ab52fb0aa0211d2</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2703.02</td><td style=\"text-align: right;\">        0.83296 </td><td style=\"text-align: right;\">            0.0480958</td><td style=\"text-align: right;\">            3235.29</td><td style=\"text-align: right;\">           0.736194</td><td style=\"text-align: right;\">               0.0631322</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "   1192   1193   1195   1972   3168   4048  20261  21615  22667  23930\n",
       "  24166  51800  53721  56262  60340  61638  63487  65066  71064  77803\n",
       "  80020  81356  82272  82280  86026  89190 165716 170455 187454 235691\n",
       " 255607]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>a7250368bd2f3d9e0ecbdd03354d7196b83a2a40013c748face8f747e0443fdf0fd6c22a588d1795c16b455c7039cf657ecf5bcdfde08fbe54669045033d24f1</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         3327.77</td><td style=\"text-align: right;\">        0.963607</td><td style=\"text-align: right;\">            0.0489203</td><td style=\"text-align: right;\">            4471.18</td><td style=\"text-align: right;\">           0.730739</td><td style=\"text-align: right;\">               0.0765263</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    842    843    844   2815   6052   6740   6792   6985  12328  12413\n",
       "  22038  23607  25439  27477  29093  31782  32434  32787  33325  70710\n",
       "  71694  78589  80537  87234 104781 105971 106868 108578 112657 211966\n",
       " 217414 242542]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>cf6aaf9aa72e0f8ebdbbab8db1e5e6c2d9ffac5bc954ae0e057099e0dd6464ee2136042556445063e81c1a4107c2d0a27234da61e648541391b2a904456a005a</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2434.66</td><td style=\"text-align: right;\">        0.980881</td><td style=\"text-align: right;\">            0.0417387</td><td style=\"text-align: right;\">            3292.28</td><td style=\"text-align: right;\">           0.784173</td><td style=\"text-align: right;\">               0.06374  </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    164    165    166    295    681   1296   1298   1299   1616   1868\n",
       "   3712   3774   3788   3797   3811   3818   3827   3828   3829   3843\n",
       "  17132  18494  36389  36416  36515  38711  41234  45850  46554 120634\n",
       " 124787 225931 242249]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>5971b987ced86878931381142fdc479e723acb6637f637cb30156e81f88f06d53cd0f9bbf7180b9d710380c699c05f6d58ab1c23e640e231d803d11dfd7d1cd1</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2424.23</td><td style=\"text-align: right;\">        0.964228</td><td style=\"text-align: right;\">            0.0339223</td><td style=\"text-align: right;\">            4167.72</td><td style=\"text-align: right;\">           0.732933</td><td style=\"text-align: right;\">               0.0733494</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    607    608    609    816   2640   4105   4159   4209   6418  10263\n",
       "  10543  10995  11252  11343  11344  11403  11711  17186  17318  36214\n",
       "  38184  41546  42908  43180  43737  44097 120764 132602 134109 214651\n",
       " 251957]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>824d5fca02966660c51d6bd8309e3caa8e61e934cc791ff7e7b9fb4ce353106cc141cf9c245e3ede6a74e517a52ec858be0804d43eb8394c605eb4019a1e4270</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         3343.64</td><td style=\"text-align: right;\">        0.874593</td><td style=\"text-align: right;\">            0.0505059</td><td style=\"text-align: right;\">            3803.02</td><td style=\"text-align: right;\">           0.785161</td><td style=\"text-align: right;\">               0.0643405</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    164    165    166    295    570    571    572   2141   2148   2152\n",
       "   2172   3034   6787   8100   8160   8244   8685   8730   8789   8855\n",
       "  25997  28927 121203 179164 186793 213852 231031 231652 244436 252102]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>c6651b9ecde7ba7e9314d46222d1b2138d627bc57cb531fb5b65d8963d13880a094b11c8cc3cd48a4508ee8b5f3c91a8473eaa94e53d8307c8165e6de623de99</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2610.08</td><td style=\"text-align: right;\">        0.627063</td><td style=\"text-align: right;\">            0.0335777</td><td style=\"text-align: right;\">            4312.02</td><td style=\"text-align: right;\">           0.523838</td><td style=\"text-align: right;\">               0.072228 </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    607    608    609   1071   1072   1075   2087   2369   3230  12075\n",
       "  12724  20788  21799  22081  24066  24595  24601  24887  39956  68716\n",
       "  68887  72674  88029  89841  94006 101079 137134 141428 170932 211251\n",
       " 227551 228281 229625]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>bd6ff0f84372e6ca986f1a5fbb38f6fe3d3a885973c5c4fb643108b8f95f4e5e21da53ad1f97fe4c4dc489b445504ab551244e7686024adc01b8dc9d2030bb17</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         3089.51</td><td style=\"text-align: right;\">        0.38286 </td><td style=\"text-align: right;\">            0.0349989</td><td style=\"text-align: right;\">            4674.31</td><td style=\"text-align: right;\">           0.303823</td><td style=\"text-align: right;\">               0.0712006</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 36 38 39 40 41 42 43 44 45 46 47 48\n",
       " 49 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    544    545    546    707   1656   3166   3217   3224   3751   4634\n",
       "  13797  13924  14085  14958  15143  15212  15656  16132  16234  36285\n",
       "  36664  36931  37579  40774  43661  46324  70793  81994  89918 194247\n",
       " 201925]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>207103be4212a074b3ef483fab2083bc7be04f273a953853b512dc5aac5911bcd68e350f3b75dccf1e0cf9b5521703fa2c6e069504656453fc261376bf4ce972</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         3435.29</td><td style=\"text-align: right;\">        0.780179</td><td style=\"text-align: right;\">            0.0461717</td><td style=\"text-align: right;\">            4561.62</td><td style=\"text-align: right;\">           0.624608</td><td style=\"text-align: right;\">               0.0733706</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[    0     1     2     3     4     5     6     7     8     9    10    11\n",
       "    13    14    15    48    49    50    51    52    90    91    92    93\n",
       "    94    95   115   124   125   136   137   138   139   140   141   142\n",
       "   143   144   145   146   147   148   155   156   157   158   159   160\n",
       "   161   162   842   843   844  1010  1012  1279  1941  3761  3764  3771\n",
       "  3792  3849  3883  3891  7949  8941  9473  9502  9811  9853  9905 10075\n",
       " 25389 26731 29016 57023]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>3557520b1295c0ed3541163e88d7bc43e7084444031e2fe487277ca211677941062d4f2a346792c45c68ce388a0341e99cca1dbc7f7288bc475de3ed26ed7587</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2992.53</td><td style=\"text-align: right;\">        0.964534</td><td style=\"text-align: right;\">            0.0430735</td><td style=\"text-align: right;\">            3852.81</td><td style=\"text-align: right;\">           0.831228</td><td style=\"text-align: right;\">               0.064967 </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    164    165    166    295    842    843    844   1354   2296   5120\n",
       "   8931   9339   9990  10548  11708  11817  12198  12231  12406  12760\n",
       "  13044  13318  35608  35985  37861  42834  43587  45675  45899 145600\n",
       " 150444 226786]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>503a11e7c1393852838272468342f4db14504f4fe077ada30e196dc2c15a8d99265dcdc73e86c33a7ea8eec1e2333a209997cd44bbfe8e5b6a5d8ff439e2f180</td><td style=\"text-align: right;\">            1</td><td style=\"text-align: right;\">         2981.36</td><td style=\"text-align: right;\">        0.830355</td><td style=\"text-align: right;\">            0.0385299</td><td style=\"text-align: right;\">            4289.69</td><td style=\"text-align: right;\">           0.5251  </td><td style=\"text-align: right;\">               0.0690645</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
       " 50 51 52 53 56 57 62 63 65 66 77 82]                          </td><td>[1. 2. 3. 4. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     13     14     15     48     49     50     51     52\n",
       "     90     91     92     93     94     95    115    124    125    136\n",
       "    137    138    139    140    141    142    143    144    145    146\n",
       "    147    148    155    156    157    158    159    160    161    162\n",
       "    570    571    572   1841   3995   8015   8686   8767   9256  10412\n",
       "  10646  10721  10985  11177  11564  11578  11751  13427  18261  28918\n",
       "  29472  30375  31210  31285  31376  33150 126502 131074 158352 180321\n",
       " 250518]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prediction\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">  p0</th><th style=\"text-align: right;\">  p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# If you need to generate predictions on a test set, you can make\n",
    "# predictions directly on the `\"H2OAutoML\"` object, or on the leader\n",
    "# model object directly\n",
    "\n",
    "#preds = aml.predict(test)\n",
    "# or:\n",
    "preds = aml.leader.predict(test)\n",
    "print(\"test\")\n",
    "print(test.head(10))\n",
    "print(\"prediction\")\n",
    "print(preds.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GBM_grid_1_AutoML_20190206_120744_model_2', 'GBM_grid_1_AutoML_20190206_120744_model_8', 'GBM_grid_1_AutoML_20190206_120744_model_7', 'GBM_grid_1_AutoML_20190206_120744_model_9', 'GBM_grid_1_AutoML_20190206_120744_model_1', 'GBM_1_AutoML_20190206_120744', 'StackedEnsemble_AllModels_AutoML_20190206_120744', 'DeepLearning_grid_1_AutoML_20190206_120744_model_2', 'GBM_grid_1_AutoML_20190206_120744_model_6', 'StackedEnsemble_BestOfFamily_AutoML_20190206_120744', 'GBM_grid_1_AutoML_20190206_120744_model_10', 'GBM_grid_1_AutoML_20190206_120744_model_5', 'GBM_4_AutoML_20190206_120744', 'GBM_2_AutoML_20190206_120744', 'DeepLearning_1_AutoML_20190206_120744', 'GBM_3_AutoML_20190206_120744', 'GBM_grid_1_AutoML_20190206_120744_model_3', 'GBM_5_AutoML_20190206_120744', 'DeepLearning_grid_1_AutoML_20190206_120744_model_10', 'DeepLearning_grid_1_AutoML_20190206_120744_model_3', 'DeepLearning_grid_1_AutoML_20190206_120744_model_5', 'DeepLearning_grid_1_AutoML_20190206_120744_model_7', 'DeepLearning_grid_1_AutoML_20190206_120744_model_8', 'DeepLearning_grid_1_AutoML_20190206_120744_model_4', 'DeepLearning_grid_1_AutoML_20190206_120744_model_1', 'GBM_grid_1_AutoML_20190206_120744_model_4', 'DeepLearning_grid_1_AutoML_20190206_120744_model_6', 'DeepLearning_grid_1_AutoML_20190206_120744_model_9']\n",
      "Model Details\n",
      "=============\n",
      "H2OGradientBoostingEstimator :  Gradient Boosting Machine\n",
      "Model Key:  GBM_grid_1_AutoML_20190206_120744_model_2\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.0\n",
      "RMSE: 0.0\n",
      "LogLoss: 0.0\n",
      "Mean Per-Class Error: 0.0\n",
      "AUC: 1.0\n",
      "pr_auc: 0.0\n",
      "Gini: 1.0\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 1.0: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>2808.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/2808.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.0</td>\n",
       "<td>1326.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/1326.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>2808.0</td>\n",
       "<td>1326.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/4134.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1     Error    Rate\n",
       "-----  ----  ----  -------  ------------\n",
       "0      2808  0     0        (0.0/2808.0)\n",
       "1      0     1326  0        (0.0/1326.0)\n",
       "Total  2808  1326  0        (0.0/4134.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value    idx\n",
       "---------------------------  -----------  -------  -----\n",
       "max f1                       1            1        0\n",
       "max f2                       1            1        0\n",
       "max f0point5                 1            1        0\n",
       "max accuracy                 1            1        0\n",
       "max precision                1            1        0\n",
       "max recall                   1            1        0\n",
       "max specificity              1            1        0\n",
       "max absolute_mcc             1            1        0\n",
       "max min_per_class_accuracy   1            1        0\n",
       "max mean_per_class_accuracy  1            1        0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 32.08 %, avg score: 32.08 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.3207547</td>\n",
       "<td>1.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>1.0</td>\n",
       "<td>1e-19</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.3207547</td>\n",
       "<td>0.3207547</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.3207547</td>\n",
       "<td>0.3207547</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift     cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain     cumulative_gain\n",
       "--  -------  --------------------------  -----------------  -------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  -------  -----------------\n",
       "    1        0.320755                    1                  3.11765  3.11765            1                1            1                           1                   1               1                          211.765  211.765\n",
       "    2        1                           1e-19              0        1                  0                1.04759e-19  0.320755                    0.320755            0               1                          -100     0\n",
       "    3        1                           0                  0        1                  0                0            0.320755                    0.320755            0               1                          -100     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 5.015211034453588e-26\n",
      "RMSE: 2.2394666852743284e-13\n",
      "LogLoss: 1.4487391918149767e-14\n",
      "Mean Per-Class Error: 0.0\n",
      "AUC: 1.0\n",
      "pr_auc: 0.0\n",
      "Gini: 1.0\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.9999999999999561: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>292.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/292.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.0</td>\n",
       "<td>144.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/144.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>292.0</td>\n",
       "<td>144.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/436.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  -----------\n",
       "0      292  0    0        (0.0/292.0)\n",
       "1      0    144  0        (0.0/144.0)\n",
       "Total  292  144  0        (0.0/436.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value    idx\n",
       "---------------------------  -----------  -------  -----\n",
       "max f1                       1            1        0\n",
       "max f2                       1            1        0\n",
       "max f0point5                 1            1        0\n",
       "max accuracy                 1            1        0\n",
       "max precision                1            1        0\n",
       "max recall                   1            1        0\n",
       "max specificity              1            1        0\n",
       "max absolute_mcc             1            1        0\n",
       "max min_per_class_accuracy   1            1        0\n",
       "max mean_per_class_accuracy  1            1        0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 33.03 %, avg score: 33.03 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.3211009</td>\n",
       "<td>1.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>3.0277778</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9722222</td>\n",
       "<td>0.9722222</td>\n",
       "<td>202.7777778</td>\n",
       "<td>202.7777778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>1.0</td>\n",
       "<td>1e-19</td>\n",
       "<td>0.0409159</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0135135</td>\n",
       "<td>0.0135135</td>\n",
       "<td>0.3302752</td>\n",
       "<td>0.3302752</td>\n",
       "<td>0.0277778</td>\n",
       "<td>1.0</td>\n",
       "<td>-95.9084084</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.321101                    1                  3.02778    3.02778            1                1          1                           1                   0.972222        0.972222                   202.778   202.778\n",
       "    2        1                           1e-19              0.0409159  1                  0.0135135        0.0135135  0.330275                    0.330275            0.0277778       1                          -95.9084  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.00024180003124528305\n",
      "RMSE: 0.015549920618616772\n",
      "LogLoss: 0.002061060670068686\n",
      "Mean Per-Class Error: 0.0\n",
      "AUC: 1.0\n",
      "pr_auc: 0.003016591251885359\n",
      "Gini: 1.0\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.00019935528931665344: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>2808.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/2808.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.0</td>\n",
       "<td>1326.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/1326.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>2808.0</td>\n",
       "<td>1326.0</td>\n",
       "<td>0.0</td>\n",
       "<td> (0.0/4134.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1     Error    Rate\n",
       "-----  ----  ----  -------  ------------\n",
       "0      2808  0     0        (0.0/2808.0)\n",
       "1      0     1326  0        (0.0/1326.0)\n",
       "Total  2808  1326  0        (0.0/4134.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.0001994</td>\n",
       "<td>1.0</td>\n",
       "<td>4.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value    idx\n",
       "---------------------------  -----------  -------  -----\n",
       "max f1                       0.000199355  1        4\n",
       "max f2                       0.000199355  1        4\n",
       "max f0point5                 0.000199355  1        4\n",
       "max accuracy                 0.000199355  1        4\n",
       "max precision                1            1        0\n",
       "max recall                   0.000199355  1        4\n",
       "max specificity              1            1        0\n",
       "max absolute_mcc             0.000199355  1        4\n",
       "max min_per_class_accuracy   0.000199355  1        4\n",
       "max mean_per_class_accuracy  0.000199355  1        4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 32.08 %, avg score: 32.05 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.2472182</td>\n",
       "<td>1.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7707391</td>\n",
       "<td>0.7707391</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.3040639</td>\n",
       "<td>1.0000000</td>\n",
       "<td>3.1176471</td>\n",
       "<td>3.1176471</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0000000</td>\n",
       "<td>0.1772247</td>\n",
       "<td>0.9479638</td>\n",
       "<td>211.7647059</td>\n",
       "<td>211.7647059</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.4511369</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.3538119</td>\n",
       "<td>2.2166220</td>\n",
       "<td>0.1134868</td>\n",
       "<td>0.1118424</td>\n",
       "<td>0.7109920</td>\n",
       "<td>0.7104559</td>\n",
       "<td>0.0520362</td>\n",
       "<td>1.0</td>\n",
       "<td>-64.6188080</td>\n",
       "<td>121.6621984</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>1.0</td>\n",
       "<td>1e-19</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.3207547</td>\n",
       "<td>0.3205129</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.247218                    1                  3.11765   3.11765            1                1            1                           1                   0.770739        0.770739                   211.765   211.765\n",
       "    2        0.304064                    1                  3.11765   3.11765            1                1            1                           1                   0.177225        0.947964                   211.765   211.765\n",
       "    3        0.451137                    1.8349e-13         0.353812  2.21662            0.113487         0.111842     0.710992                    0.710456            0.0520362       1                          -64.6188  121.662\n",
       "    4        1                           1e-19              0         1                  0                2.17978e-15  0.320755                    0.320513            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Metrics Summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>mean</b></td>\n",
       "<td><b>sd</b></td>\n",
       "<td><b>cv_1_valid</b></td>\n",
       "<td><b>cv_2_valid</b></td>\n",
       "<td><b>cv_3_valid</b></td>\n",
       "<td><b>cv_4_valid</b></td>\n",
       "<td><b>cv_5_valid</b></td></tr>\n",
       "<tr><td>accuracy</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>auc</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>err</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>err_count</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>f0point5</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>f1</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>f2</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>lift_top_group</td>\n",
       "<td>3.1176536</td>\n",
       "<td>0.0032218</td>\n",
       "<td>3.1090226</td>\n",
       "<td>3.1207547</td>\n",
       "<td>3.1207547</td>\n",
       "<td>3.1207547</td>\n",
       "<td>3.116981</td></tr>\n",
       "<tr><td>logloss</td>\n",
       "<td>0.0020606</td>\n",
       "<td>0.0029141</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0103028</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td></tr>\n",
       "<tr><td>max_per_class_error</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>mcc</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>mean_per_class_accuracy</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>mean_per_class_error</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>mse</td>\n",
       "<td>0.0002417</td>\n",
       "<td>0.0003419</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0012087</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td></tr>\n",
       "<tr><td>precision</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>r2</td>\n",
       "<td>0.9988899</td>\n",
       "<td>0.0015700</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9944493</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>recall</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>rmse</td>\n",
       "<td>0.0069533</td>\n",
       "<td>0.0098334</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0347665</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000001</td></tr>\n",
       "<tr><td>specificity</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td></tr></table></div>"
      ],
      "text/plain": [
       "                         mean         sd           cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n",
       "-----------------------  -----------  -----------  ------------  ------------  ------------  ------------  ------------\n",
       "accuracy                 1            0            1             1             1             1             1\n",
       "auc                      1            0            1             1             1             1             1\n",
       "err                      0            0            0             0             0             0             0\n",
       "err_count                0            0            0             0             0             0             0\n",
       "f0point5                 1            0            1             1             1             1             1\n",
       "f1                       1            0            1             1             1             1             1\n",
       "f2                       1            0            1             1             1             1             1\n",
       "lift_top_group           3.11765      0.00322177   3.10902       3.12075       3.12075       3.12075       3.11698\n",
       "logloss                  0.00206056   0.00291407   6.11898e-16   0.0103028     8.12732e-16   3.45306e-10   2.31957e-09\n",
       "max_per_class_error      0            0            0             0             0             0             0\n",
       "mcc                      1            0            1             1             1             1             1\n",
       "mean_per_class_accuracy  1            0            1             1             1             1             1\n",
       "mean_per_class_error     0            0            0             0             0             0             0\n",
       "mse                      0.000241742  0.000341874  2.83048e-28   0.00120871    4.16074e-28   9.86047e-17   4.43994e-15\n",
       "precision                1            0            1             1             1             1             1\n",
       "r2                       0.99889      0.00156998   1             0.994449      1             1             1\n",
       "recall                   1            0            1             1             1             1             1\n",
       "rmse                     0.00695331   0.00983344   1.6824e-14    0.0347665     2.03979e-14   9.92999e-09   6.66329e-08\n",
       "specificity              1            0            1             1             1             1             1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_pr_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_logloss</b></td>\n",
       "<td><b>validation_auc</b></td>\n",
       "<td><b>validation_pr_auc</b></td>\n",
       "<td><b>validation_lift</b></td>\n",
       "<td><b>validation_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 11.906 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>0.4667667</td>\n",
       "<td>0.6274370</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.6792453</td>\n",
       "<td>0.4704085</td>\n",
       "<td>0.6345803</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.6697248</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 11.924 sec</td>\n",
       "<td>5.0</td>\n",
       "<td>0.0051481</td>\n",
       "<td>0.0051366</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0007541</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0051382</td>\n",
       "<td>0.0051386</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 11.947 sec</td>\n",
       "<td>10.0</td>\n",
       "<td>0.0000994</td>\n",
       "<td>0.0000960</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9962293</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0001229</td>\n",
       "<td>0.0001007</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9722222</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 11.993 sec</td>\n",
       "<td>15.0</td>\n",
       "<td>0.0000021</td>\n",
       "<td>0.0000018</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9947210</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000032</td>\n",
       "<td>0.0000020</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0347222</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.040 sec</td>\n",
       "<td>20.0</td>\n",
       "<td>0.0000001</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0316742</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000034</td>\n",
       "<td>0.0000002</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0277778</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.098 sec</td>\n",
       "<td>25.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000001</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0208333</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.155 sec</td>\n",
       "<td>30.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0069444</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.209 sec</td>\n",
       "<td>35.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.262 sec</td>\n",
       "<td>40.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.306 sec</td>\n",
       "<td>45.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.337 sec</td>\n",
       "<td>50.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.359 sec</td>\n",
       "<td>55.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.379 sec</td>\n",
       "<td>60.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.398 sec</td>\n",
       "<td>65.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:01</td>\n",
       "<td> 1 min 12.416 sec</td>\n",
       "<td>70.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:02</td>\n",
       "<td> 1 min 12.434 sec</td>\n",
       "<td>75.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-06 12:10:02</td>\n",
       "<td> 1 min 12.447 sec</td>\n",
       "<td>77.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.1176471</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>3.0277778</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration          number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
       "--  -------------------  ----------------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
       "    2019-02-06 12:10:01  1 min 11.906 sec  0                  0.466767         0.627437            0.5             0                  1                0.679245                         0.470408           0.63458               0.5               0                    1                  0.669725\n",
       "    2019-02-06 12:10:01  1 min 11.924 sec  5                  0.00514813       0.00513664          1               0.000754148        3.11765          0                                0.00513816         0.00513856            1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 11.947 sec  10                 9.94457e-05      9.59938e-05         1               0.996229           3.11765          0                                0.000122915        0.000100685           1                 0.972222             3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 11.993 sec  15                 2.12199e-06      1.79244e-06         1               0.994721           3.11765          0                                3.22373e-06        1.95473e-06           1                 0.0347222            3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.040 sec  20                 8.61559e-08      3.7072e-08          1               0.0316742          3.11765          0                                3.43066e-06        2.11577e-07           1                 0.0277778            3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.098 sec  25                 1.58077e-09      6.82417e-10         1               0                  3.11765          0                                6.28394e-08        3.88416e-09           1                 0.0208333            3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.155 sec  30                 3.94149e-11      1.3919e-11          1               0                  3.11765          0                                5.71013e-09        3.16675e-10           1                 0.00694444           3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.209 sec  35                 1.17942e-12      3.00574e-13         1               0                  3.11765          0                                5.19531e-10        2.85585e-11           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.262 sec  40                 2.17793e-14      5.61885e-15         1               0                  3.11765          0                                9.58097e-12        5.56681e-13           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.306 sec  45                 7.78488e-16      1.56543e-16         1               0                  3.11765          0                                2.42509e-12        1.51623e-13           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.337 sec  50                 4.90222e-17      5.21005e-18         1               0                  3.11765          0                                2.90144e-13        1.89033e-14           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.359 sec  55                 1.35963e-17      8.59389e-19         1               0                  3.11765          0                                7.57417e-13        4.29116e-14           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.379 sec  60                 2.18416e-17      2.14847e-18         1               0                  3.11765          0                                1.2903e-12         8.57031e-14           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.398 sec  65                 9.76787e-18      4.29694e-19         1               0                  3.11765          0                                7.30362e-13        4.91824e-14           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:01  1 min 12.416 sec  70                 4.88394e-18      1.07424e-19         1               0                  3.11765          0                                4.54014e-13        2.81258e-14           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:02  1 min 12.434 sec  75                 4.88394e-18      1.07424e-19         1               0                  3.11765          0                                2.1285e-13         1.31859e-14           1                 0                    3.02778            0\n",
       "    2019-02-06 12:10:02  1 min 12.447 sec  77                 0                0                   1               0                  3.11765          0                                2.23947e-13        1.44874e-14           1                 0                    3.02778            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>features85_list_indices</td>\n",
       "<td>900.5152588</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9144560</td></tr>\n",
       "<tr><td>features85_list_values</td>\n",
       "<td>84.0486832</td>\n",
       "<td>0.0933340</td>\n",
       "<td>0.0853498</td></tr>\n",
       "<tr><td>ngramscounts7_list_values</td>\n",
       "<td>0.1673868</td>\n",
       "<td>0.0001859</td>\n",
       "<td>0.0001700</td></tr>\n",
       "<tr><td>entropy_fraud_words</td>\n",
       "<td>0.0234560</td>\n",
       "<td>0.0000260</td>\n",
       "<td>0.0000238</td></tr>\n",
       "<tr><td>kl_notfraud_words</td>\n",
       "<td>0.0003148</td>\n",
       "<td>0.0000003</td>\n",
       "<td>0.0000003</td></tr>\n",
       "<tr><td>kl_fraud_words</td>\n",
       "<td>0.0000837</td>\n",
       "<td>0.0000001</td>\n",
       "<td>0.0000001</td></tr>\n",
       "<tr><td>ks_fraud_words</td>\n",
       "<td>0.0000086</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td></tr>\n",
       "<tr><td>entropy_notfraud_words</td>\n",
       "<td>0.0000024</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td></tr>\n",
       "<tr><td>ks_notfraud_words</td>\n",
       "<td>0.0000011</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0000000</td></tr></table></div>"
      ],
      "text/plain": [
       "variable                   relative_importance    scaled_importance    percentage\n",
       "-------------------------  ---------------------  -------------------  ------------\n",
       "features85_list_indices    900.515                1                    0.914456\n",
       "features85_list_values     84.0487                0.093334             0.0853498\n",
       "ngramscounts7_list_values  0.167387               0.000185879          0.000169978\n",
       "entropy_fraud_words        0.023456               2.60473e-05          2.38191e-05\n",
       "kl_notfraud_words          0.000314826            3.49606e-07          3.19699e-07\n",
       "kl_fraud_words             8.36856e-05            9.29308e-08          8.49811e-08\n",
       "ks_fraud_words             8.59057e-06            9.53962e-09          8.72356e-09\n",
       "entropy_notfraud_words     2.35438e-06            2.61448e-09          2.39083e-09\n",
       "ks_notfraud_words          1.09123e-06            1.21178e-09          1.10812e-09"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'H2OGradientBoostingEstimator' has no attribute 'metalearner'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-401d11ca7ede>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Get the Stacked Ensemble metalearner model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmetalearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetalearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmetalearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda-5.1.0.1/lib/python3.6/site-packages/h2o/utils/backward_compatibility.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bcin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Make sure that we look up any names not found on the instance also in the class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda-5.1.0.1/lib/python3.6/site-packages/h2o/utils/backward_compatibility.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;31m# print(\"Warning: Method %s in class %s is deprecated.\" % (name, cls.__name__))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sm\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'H2OGradientBoostingEstimator' has no attribute 'metalearner'"
     ]
    }
   ],
   "source": [
    "# Get model ids for all models in the AutoML Leaderboard\n",
    "model_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])\n",
    "\n",
    "print(model_ids)\n",
    "# Get the \"All Models\" Stacked Ensemble model\n",
    "se = h2o.get_model([mid for mid in model_ids if \"GBM\" in mid][0])\n",
    "print(se)\n",
    "\n",
    "# Get the Stacked Ensemble metalearner model\n",
    "metalearner = h2o.get_model(aml.leader.metalearner()['name'])\n",
    "metalearner.coef_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metalearner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-5a4538925986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetalearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd_coef_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'metalearner' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "metalearner.std_coef_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/siemanalyst/projects/logs-archive-production/fraud-canada-tokenizedwords/notebooks/product_model_bin/ngrams7_features85_vectors_m25/v8/mojo/GBM_grid_1_AutoML_20190206_120744_model_2.zip'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.download_mojo(path = \"./product_model_bin/ngrams7_features85_vectors_m25/v8/mojo\", get_genmodel_jar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm prediction progress: |ââââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/cloudera/parcels/Anaconda-5.1.0.1/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'features85_list_indices' has levels not trained on: [[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 61 62 63 65 66 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 58 61 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 56 60 61 64 66 67 75 77], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 55 56 57 58 61 62 63 65 66 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 57 58 59 61 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 57 58 60 61 65 66 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 58 59 61 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 58 61 62 63 67], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 48 49 50 51 52\n",
      " 54 55 56 58 61 62 63 65 68 69 70 71 72 73 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 57 58 59 60 66 67 77], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 58 65 77 81], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 56 58 65 77 81 82], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 54 55 57 58 60 65 66 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 54 55 57 58 60 65 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 54 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 54 58 62 63 65\n",
      " 68 69 70 71 72 73 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 54 62 63 65 68\n",
      " 69 70 71 72 73 78], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 43 44 45 46 47 53 58 59 64 76\n",
      " 81], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 54 55 56 57 58 60 61 64 66 67 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 54 55 56 58 60 61 64 67 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 36 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 58 59 60 64 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 54 55 56 57 58 60 66 67 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 55 56 58 59 60 67 74 77], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 48 49 50 51 52 53 54 55\n",
      " 58 59 60 68 69 70 71 72 73 74 78], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 54 56 57 61 62 63 65 66], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 48 49 50 51 52 54\n",
      " 56 58 60 64 67 68 69 70 71 72 73 77], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 54 56 57 65 66 77 81 82], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 50 51 52 53 54 56\n",
      " 65 68 69 70 71 72 73 77 78 81 82], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 50 51 52 54 56 58\n",
      " 59 64 65 68 69 70 71 72 73 77 81], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 42 43 44 45 46 47 53 57 59 76 81], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 48 49 50 51 52 53 54 56 58\n",
      " 60 67 68 69 70 71 72 73 74], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 48 49 50 51 52 53 54 58 59\n",
      " 60 68 69 70 71 72 73 74 78], [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 42 43 44 45 46 47 53 55 57 58 62 63 64\n",
      " 66 67 74 75], [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 42 43 44 45 46 47 53 57 58 62 63 64 67\n",
      " 74 75], [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 43 44 45 46 47 53 57 58 62 63 64 67 74\n",
      " 75 82], [ 0  1  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 40 41 43 44 45 46 47 57 58 60 82], [ 0  1  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 40 41 43 44 45 46 47 57 60 82], [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 38 39 43 44 45 46 47 53 57 59 66 76\n",
      " 81], [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 42 43 44 45 46 47 53 59 76 81 82], [ 1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 38 39 43 44 45 46 47 53 57 58 59 66 76 81], [ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 40 41 42 43 44 45 46 47 55 57 58 60]]\n",
      "  warnings.warn(w)\n",
      "/opt/cloudera/parcels/Anaconda-5.1.0.1/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'features85_list_values' has levels not trained on: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 1.], [1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [1. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1.], [1. 2. 2. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1.], [1. 2. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1.], [1. 2. 3. 3. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1.], [1. 4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.\n",
      " 2. 1.], [1. 4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1.\n",
      " 2. 1.], [1. 4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.], [1. 4. 4. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 3.], [1. 4. 5. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 3.], [1. 4. 5. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [2. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 1. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 4. 2. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 3.], [2. 4. 3. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 4. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [3. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1.], [3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1.], [3. 2. 2. 3. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1.], [3. 2. 2. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1.], [3. 2. 2. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1.], [3. 2. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.], [3. 4. 2. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3. 1.], [3. 4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [3. 4. 3. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [4. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 2. 2.\n",
      " 1.], [4. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1.\n",
      " 1. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 2.\n",
      " 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 3. 1.\n",
      " 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 2.\n",
      " 1. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1.], [4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1.], [4. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 4. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2.\n",
      " 1.], [5. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [5. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [6. 2. 3. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.]]\n",
      "  warnings.warn(w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>hash_message                                                                                                                    </th><th style=\"text-align: right;\">  fraud_label</th><th style=\"text-align: right;\">  kl_fraud_words</th><th style=\"text-align: right;\">  ks_fraud_words</th><th style=\"text-align: right;\">  entropy_fraud_words</th><th style=\"text-align: right;\">  kl_notfraud_words</th><th style=\"text-align: right;\">  ks_notfraud_words</th><th style=\"text-align: right;\">  entropy_notfraud_words</th><th style=\"text-align: right;\">  features85_type</th><th style=\"text-align: right;\">  features85_size</th><th style=\"text-align: right;\">  ngramscounts7_type</th><th style=\"text-align: right;\">  ngramscounts7_size</th><th>features85_list_indices  </th><th>features85_list_values  </th><th>ngramscounts7_list_indices  </th><th>ngramscounts7_list_values  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>0c983268c70fd7c560320a3a925c414fd448cfad5dbc1287f1f1234aae6b706bdc5ac0c1a43d67efea4f8d4d1c47a4795449ad8284a1d4eef377f2e407ecca15</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         3787.34</td><td style=\"text-align: right;\">       0.399077 </td><td style=\"text-align: right;\">            0.0795858</td><td style=\"text-align: right;\">            2450.78</td><td style=\"text-align: right;\">           0.722469</td><td style=\"text-align: right;\">               0.0547815</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
       " 51 52 53 54 55 57 58 59 60 64 74]                          </td><td>[3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     23     24     25     26     27     36     45     46     47     65\n",
       "     66     67     68     69     70     71     72     73     74     75\n",
       "     76     77     78     96     97     98     99    100    101    102\n",
       "    103    104    105    106    112    113    114    152    153    154\n",
       "   1455   2641  12207  36645  37149  38512  42844  43290  45175  45622\n",
       "  47181  47315  47754  51188  51421  53447  54992  55174  57563  58819\n",
       "  58824  58831  59045  59285  59421  60606  61074  64791  65510  66339\n",
       "  67109  67692 126259 227408 246021]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>751b0d4b88a1ad630b258c03596bf591c7d7d671fe8a54fc802599b80695ca85373e5b918307956930479bfcc5640b2fa91ccadf77c8afcf75dbbeef8d627689</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         3136.01</td><td style=\"text-align: right;\">       0.522336 </td><td style=\"text-align: right;\">            0.0757026</td><td style=\"text-align: right;\">            2208.72</td><td style=\"text-align: right;\">           0.316835</td><td style=\"text-align: right;\">               0.0628293</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
       " 51 52 53 54 57 59 60 64 74]                          </td><td>[3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     36     45     46     47     68     69     70     71     72     73\n",
       "     74     75     76     77     78     96     97     98     99    100\n",
       "    101    102    103    104    105    106    112    113    114   1070\n",
       "   1101   1103   1116   1172   1587   1853   1864   1866   7423   7552\n",
       "   7969  23657  46798  59908  60463  61747  64797  70099  70310  78128\n",
       "  79052  81104  81253  87607  88791  90640  91462  92107  96001  97569\n",
       "  98487 100447 106278 107445 111550 141147 141207 155918 163518 198047\n",
       " 237709 239794 259091]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>cbc22468fac4860040971cbe031ea19f8227f966d3a27884a9466a08b869d926f8ce43bf9ba88ee4691958b5a30794f0039b68fdc45f99d3dfc515727c2118d7</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         3183.96</td><td style=\"text-align: right;\">       0.522336 </td><td style=\"text-align: right;\">            0.0760335</td><td style=\"text-align: right;\">            2247.62</td><td style=\"text-align: right;\">           0.569637</td><td style=\"text-align: right;\">               0.0630055</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
       " 51 52 53 54 59 60 64 74]                          </td><td>[3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     36     45     46     47     68     69     70     71     72     73\n",
       "     74     75     76     77     78     96     97     98     99    100\n",
       "    101    102    103    104    105    106    112    113    114    302\n",
       "    303    304    928    944    948    952   1624   1625   1626   1657\n",
       "   2654  16970  20440  23125  23858  24125  24348  33711  34189  34569\n",
       "  34699  35707  35765  36001  37044  37939  38432  39438  39543  41952\n",
       "  42951  43753  44569  44846  45642 103813 104899 174276 189763 191089\n",
       " 219811 223729]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>647a1a86d4a231f6ea53f9d03cc4065768bac9e94533264d52a474613af4af4f6efa50f598c09acf943be604b22714ee7b5d6cfe5e8d05a2044a5650ca247c07</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         5527.58</td><td style=\"text-align: right;\">       0.177085 </td><td style=\"text-align: right;\">            0.0803395</td><td style=\"text-align: right;\">            5264.12</td><td style=\"text-align: right;\">           0.116322</td><td style=\"text-align: right;\">               0.0787753</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 48 49 50 51 52\n",
       " 54 56 61 62 63 67 68 69 70 71 72 73 78]                          </td><td>[4. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      5      9     10     11     12\n",
       "     16     17     18     19     20     21     28     29     30     33\n",
       "    256    286    307    308    309    310    311    312    313   1219\n",
       "   1220   1223   1224   1225   1226   1227   1229   1230   1231   1232\n",
       "   1233   1234   1235   1236   1238   1239   1240   1242   1243   1244\n",
       "   1980   2731   2763   2771   2786   3779   3850   4619   4632   4648\n",
       "   4669   4694   4779   4795   6003   6390   6420   6532   6593   6595\n",
       "  14261  16245  24905 121248 137943 222694 226564]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>89a21086ca9f0e18e588f36c1574d965f4ed483108208b11d6f52e229eed99a0bcf8d68fb5da24f50727aa167f9158dd7b9de41227a296230fa389d4c0213449</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         2096.24</td><td style=\"text-align: right;\">       0.157862 </td><td style=\"text-align: right;\">            0.0773763</td><td style=\"text-align: right;\">            2316.33</td><td style=\"text-align: right;\">           0.447932</td><td style=\"text-align: right;\">               0.073537 </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
       " 25 26 27 28 29 30 31 32 33 34 35 40 41 43 44 45 46 47 57 58 60 66]                          </td><td>[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      6      7      8    231    232\n",
       "    233    234    262    791    795    799    846    848    852    853\n",
       "    855    856    858    861    863    864    866    868    869    871\n",
       "   1680   9659  25669  27021  27885  28074  28822  29327  29745  30347\n",
       "  30873  31037  31355  31649  31886 115134 128855 140406 161067 162703\n",
       " 202211 218577 234361 257753]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>f019d9a53ea6317b9c4271fa4ea14d7e7107ba35f871473ce9f5c207ec7526bade74f60bcfae76841ce1b5c0954faa73961fb718a6ce4a1551ed57dd629e032e</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         4828.7 </td><td style=\"text-align: right;\">       0.626082 </td><td style=\"text-align: right;\">            0.068934 </td><td style=\"text-align: right;\">            4118.7 </td><td style=\"text-align: right;\">           0.975112</td><td style=\"text-align: right;\">               0.0714218</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
       " 25 26 27 28 29 30 31 32 33 34 35 42 43 44 45 46 47 53 57 58 62 63 64 67\n",
       " 74 75]                          </td><td>[2. 4. 5. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1.\n",
       " 1. 3.]                         </td><td>[     0      1      2      3      4      6      7      8    202    203\n",
       "    204    205    208    209    210    211    229    230    247   1664\n",
       "   2234   2358   2590   3059   3064   3130  13549  13715  13831  13928\n",
       "  13958  14158  14873  16251  25616  27420  30666  32414  32673  34075\n",
       "  34193  34993  35675  35677  35828  36695  36815  36833  36839  37319\n",
       "  37759  37851  38268  38275  38436  38813  38955  39046  39200  39204\n",
       "  39362  39813  39905  40114  40756  41907  42203  42689  42879  43301\n",
       "  43819  44054  44135  44840  45065  45553  45712  45802  45849  46315\n",
       "  46491 124741 156589 165472 190325 196092 197240]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>5497dc4dc2abc0cf7c5a7e4b09b058cbd8deed23eabc446057911df6d6b3858e5f5854ae73f53cc02761185c7e02c0da166b17cf1f500903bef7f10e4a70fd25</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         4345.49</td><td style=\"text-align: right;\">       0.978829 </td><td style=\"text-align: right;\">            0.0747982</td><td style=\"text-align: right;\">            3607.71</td><td style=\"text-align: right;\">           0.673018</td><td style=\"text-align: right;\">               0.056413 </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
       " 48 49 50 51 52 54 56 61 62 63 64 67 75 77]                          </td><td>[6. 2. 3. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     36    149    150    151    236    244    248    249\n",
       "    250    261    448    449    450    451    452    453    454    683\n",
       "    697    718    761    762    763    764    765    766    769    770\n",
       "    771    773    955    958    960    963    965    966    968    984\n",
       "    985    987    990    991    993    994    995    997    998   1046\n",
       "   1059   1092   2238   2569   2904   2905   2946   3647   4692   4695\n",
       "   8366  13581  13694  13905  14220  14271  14712  14859  15019  15151\n",
       "  15392  15437  15522  15549  15560  15598  15889  16004  16182  16267\n",
       "  16359  20730  20771  20785  20843  22875  24480  24577  24852  53828\n",
       " 126816 132228 149513 157143 223244 230468]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>7a26b41f0501bf1a91fab3c66f48efa09bdc582dc213b9576d772e855188741da14096f3506daee11a591c6582df456d9620f119ea3a43decca1deec5e217bb5</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         4890.76</td><td style=\"text-align: right;\">       0.989061 </td><td style=\"text-align: right;\">            0.0830742</td><td style=\"text-align: right;\">            2499.05</td><td style=\"text-align: right;\">           0.833535</td><td style=\"text-align: right;\">               0.0577531</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
       " 48 49 50 51 52 54 56 57 58 60 61 65 66 79 80 83 84]                          </td><td>[3. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     12     13     14     15     22     31     32     34\n",
       "     35     40     41     43     44     63     64     81     82     83\n",
       "     84    116    117    118    119    120    121    122    123    439\n",
       "    440    441    442    443    444    445    446   1009   1105   1113\n",
       "   1115   1117   1631   1648   1651   1995   2018   2042   2043   2046\n",
       "   3952  20759  21665  22274  23669  23838  24241  35472  36610  36770\n",
       "  37958  40184  41068  42712  42791  46046  49260  56168  64717 116078\n",
       " 131231 135097 140555 148227 159879 160558 166006 197239 205224]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>5383d9b8598680fcd978ce6238d65a0805eb63fa59e4b45b68486f97d3e70f149ca706a0be06be8e569543a63ae756d12219b6786d1ddf1424e912f6ee37201a</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         2159.28</td><td style=\"text-align: right;\">       0.0817671</td><td style=\"text-align: right;\">            0.0734992</td><td style=\"text-align: right;\">            1616.69</td><td style=\"text-align: right;\">           0.110837</td><td style=\"text-align: right;\">               0.0671171</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
       " 25 26 27 28 29 30 31 32 33 34 35 38 39 42 43 44 45 46 47 53 57 59 76 81]                          </td><td>[4. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 2. 2. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8     85\n",
       "     86     87     88    107   1064   1071   1072   1075   1110   1209\n",
       "   1210   1684   1849   1850   1851   2215   4202   5237   5401   7782\n",
       "  10454  10491  10643  10729  10838  11677  15369  68749  69787  74124\n",
       "  74497  75192  76197  77532  79903  80283  80548  82333  82510  83513\n",
       "  83794  86834  87132  87626  88160  90265  92572  93100  93159  93522\n",
       "  95128  95539  96911  99690 101130 103184 103602 105018 106722 108018\n",
       " 109202 109477 112092 112607 112670 113234 113509 119438 127234 153204\n",
       " 211147 213309]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>f17d2f1ac2307d595763c3422ecd432cabf485c700895111519c7952f583164ae27e400504a50a7a7e73dbe709a3434f023729ab306484da388a1080e674feca</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         2702.7 </td><td style=\"text-align: right;\">       0.984483 </td><td style=\"text-align: right;\">            0.0511013</td><td style=\"text-align: right;\">            3574.73</td><td style=\"text-align: right;\">           0.993018</td><td style=\"text-align: right;\">               0.0714935</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
       " 25 26 27 28 29 30 31 32 33 34 35 43 44 45 46 47 53 57 62 63 64 67 74 75]                          </td><td>[1. 4. 4. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 3.]                         </td><td>[     0      1      2      3      4      6      7      8    164    165\n",
       "    166    202    203    204    205    208    209    210    211    229\n",
       "    230    463    520    788    886    890    964   1174   1176   1177\n",
       "   1387   1460   1477   2295   2302   2306   2311   2322   2349   2594\n",
       "   2596   2605   2607   2608   2613   2644   2650   2663   3198   3946\n",
       "   4040   6150  12735  12951  13201  13331  13447  26320  47125  48118\n",
       "  48271  48651  49934  50964  52100  52279  52435  52678  54577  55736\n",
       "  56712  56718  58360  58963  59206  65094  65175 117790 121041 140200\n",
       " 152718 221068 224277 237713 248531]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prediction\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">  p0</th><th style=\"text-align: right;\">         p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">3.25879e-18</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# If you need to generate predictions on a test set, you can make\n",
    "# predictions directly on the `\"H2OAutoML\"` object, or on the leader\n",
    "# model object directly\n",
    "\n",
    "#preds = aml.predict(test)\n",
    "# or:\n",
    "preds = aml.leader.predict(test)\n",
    "print(\"test\")\n",
    "print(test.tail(10))\n",
    "print(\"prediction\")\n",
    "print(preds.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm prediction progress: |ââââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/cloudera/parcels/Anaconda-5.1.0.1/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'features85_list_indices' has levels not trained on: [[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 61 62 63 65 66 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 58 61 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 56 60 61 64 66 67 75 77], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 55 56 57 58 61 62 63 65 66 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 57 58 59 61 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 57 58 60 61 65 66 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 58 59 61 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 56 58 61 62 63 67], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 48 49 50 51 52\n",
      " 54 55 56 58 61 62 63 65 68 69 70 71 72 73 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 57 58 59 60 66 67 77], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 58 65 77 81], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 56 58 65 77 81 82], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 54 55 57 58 60 65 66 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 54 55 57 58 60 65 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 54 62 63 65 79 80 83 84], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 54 58 62 63 65\n",
      " 68 69 70 71 72 73 79 80], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 54 62 63 65 68\n",
      " 69 70 71 72 73 78], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 38 39 43 44 45 46 47 53 58 59 64 76\n",
      " 81], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 54 55 56 57 58 60 61 64 66 67 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 54 55 56 58 60 61 64 67 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 36 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 58 59 60 64 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 54 55 56 57 58 60 66 67 74], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 55 56 58 59 60 67 74 77], [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 48 49 50 51 52 53 54 55\n",
      " 58 59 60 68 69 70 71 72 73 74 78], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 54 56 57 61 62 63 65 66], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 48 49 50 51 52 54\n",
      " 56 58 60 64 67 68 69 70 71 72 73 77], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      " 51 52 53 54 56 57 65 66 77 81 82], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 50 51 52 53 54 56\n",
      " 65 68 69 70 71 72 73 77 78 81 82], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 40 41 42 48 49 50 51 52 54 56 58\n",
      " 59 64 65 68 69 70 71 72 73 77 81], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 42 43 44 45 46 47 53 57 59 76 81], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 48 49 50 51 52 53 54 56 58\n",
      " 60 67 68 69 70 71 72 73 74], [ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 48 49 50 51 52 53 54 58 59\n",
      " 60 68 69 70 71 72 73 74 78], [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 42 43 44 45 46 47 53 55 57 58 62 63 64\n",
      " 66 67 74 75], [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 42 43 44 45 46 47 53 57 58 62 63 64 67\n",
      " 74 75], [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 43 44 45 46 47 53 57 58 62 63 64 67 74\n",
      " 75 82], [ 0  1  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 40 41 43 44 45 46 47 57 58 60 82], [ 0  1  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 40 41 43 44 45 46 47 57 60 82], [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 38 39 43 44 45 46 47 53 57 59 66 76\n",
      " 81], [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 38 39 42 43 44 45 46 47 53 59 76 81 82], [ 1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 38 39 43 44 45 46 47 53 57 58 59 66 76 81], [ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32 33 34 35 40 41 42 43 44 45 46 47 55 57 58 60]]\n",
      "  warnings.warn(w)\n",
      "/opt/cloudera/parcels/Anaconda-5.1.0.1/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'features85_list_values' has levels not trained on: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 1.], [1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [1. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1.], [1. 2. 2. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1.], [1. 2. 3. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1.], [1. 2. 3. 3. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1.], [1. 4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.\n",
      " 2. 1.], [1. 4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1.\n",
      " 2. 1.], [1. 4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.], [1. 4. 4. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 3.], [1. 4. 5. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 3.], [1. 4. 5. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [2. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 1. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 2. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1.], [2. 4. 2. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 3.], [2. 4. 3. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 4. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [3. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1.], [3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1.], [3. 2. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1.], [3. 2. 2. 3. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1.], [3. 2. 2. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1.], [3. 2. 2. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1.], [3. 2. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [3. 2. 3. 4. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.], [3. 4. 2. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3. 1.], [3. 4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [3. 4. 3. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 3.], [4. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 2. 2.\n",
      " 1.], [4. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1.\n",
      " 1. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 2.\n",
      " 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 3. 1.\n",
      " 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 2.\n",
      " 1. 2. 1.], [4. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1.], [4. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1.], [4. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 3. 3. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [4. 2. 4. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2.\n",
      " 1.], [5. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [5. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.], [6. 2. 3. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.]]\n",
      "  warnings.warn(w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>hash_message                                                                                                                    </th><th style=\"text-align: right;\">  fraud_label</th><th style=\"text-align: right;\">  kl_fraud_words</th><th style=\"text-align: right;\">  ks_fraud_words</th><th style=\"text-align: right;\">  entropy_fraud_words</th><th style=\"text-align: right;\">  kl_notfraud_words</th><th style=\"text-align: right;\">  ks_notfraud_words</th><th style=\"text-align: right;\">  entropy_notfraud_words</th><th style=\"text-align: right;\">  features85_type</th><th style=\"text-align: right;\">  features85_size</th><th style=\"text-align: right;\">  ngramscounts7_type</th><th style=\"text-align: right;\">  ngramscounts7_size</th><th>features85_list_indices  </th><th>features85_list_values  </th><th>ngramscounts7_list_indices  </th><th>ngramscounts7_list_values  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>0c983268c70fd7c560320a3a925c414fd448cfad5dbc1287f1f1234aae6b706bdc5ac0c1a43d67efea4f8d4d1c47a4795449ad8284a1d4eef377f2e407ecca15</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         3787.34</td><td style=\"text-align: right;\">       0.399077 </td><td style=\"text-align: right;\">            0.0795858</td><td style=\"text-align: right;\">            2450.78</td><td style=\"text-align: right;\">           0.722469</td><td style=\"text-align: right;\">               0.0547815</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
       " 51 52 53 54 55 57 58 59 60 64 74]                          </td><td>[3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     23     24     25     26     27     36     45     46     47     65\n",
       "     66     67     68     69     70     71     72     73     74     75\n",
       "     76     77     78     96     97     98     99    100    101    102\n",
       "    103    104    105    106    112    113    114    152    153    154\n",
       "   1455   2641  12207  36645  37149  38512  42844  43290  45175  45622\n",
       "  47181  47315  47754  51188  51421  53447  54992  55174  57563  58819\n",
       "  58824  58831  59045  59285  59421  60606  61074  64791  65510  66339\n",
       "  67109  67692 126259 227408 246021]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>751b0d4b88a1ad630b258c03596bf591c7d7d671fe8a54fc802599b80695ca85373e5b918307956930479bfcc5640b2fa91ccadf77c8afcf75dbbeef8d627689</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         3136.01</td><td style=\"text-align: right;\">       0.522336 </td><td style=\"text-align: right;\">            0.0757026</td><td style=\"text-align: right;\">            2208.72</td><td style=\"text-align: right;\">           0.316835</td><td style=\"text-align: right;\">               0.0628293</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
       " 51 52 53 54 57 59 60 64 74]                          </td><td>[3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     36     45     46     47     68     69     70     71     72     73\n",
       "     74     75     76     77     78     96     97     98     99    100\n",
       "    101    102    103    104    105    106    112    113    114   1070\n",
       "   1101   1103   1116   1172   1587   1853   1864   1866   7423   7552\n",
       "   7969  23657  46798  59908  60463  61747  64797  70099  70310  78128\n",
       "  79052  81104  81253  87607  88791  90640  91462  92107  96001  97569\n",
       "  98487 100447 106278 107445 111550 141147 141207 155918 163518 198047\n",
       " 237709 239794 259091]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>cbc22468fac4860040971cbe031ea19f8227f966d3a27884a9466a08b869d926f8ce43bf9ba88ee4691958b5a30794f0039b68fdc45f99d3dfc515727c2118d7</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         3183.96</td><td style=\"text-align: right;\">       0.522336 </td><td style=\"text-align: right;\">            0.0760335</td><td style=\"text-align: right;\">            2247.62</td><td style=\"text-align: right;\">           0.569637</td><td style=\"text-align: right;\">               0.0630055</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
       " 51 52 53 54 59 60 64 74]                          </td><td>[3. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     36     45     46     47     68     69     70     71     72     73\n",
       "     74     75     76     77     78     96     97     98     99    100\n",
       "    101    102    103    104    105    106    112    113    114    302\n",
       "    303    304    928    944    948    952   1624   1625   1626   1657\n",
       "   2654  16970  20440  23125  23858  24125  24348  33711  34189  34569\n",
       "  34699  35707  35765  36001  37044  37939  38432  39438  39543  41952\n",
       "  42951  43753  44569  44846  45642 103813 104899 174276 189763 191089\n",
       " 219811 223729]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>647a1a86d4a231f6ea53f9d03cc4065768bac9e94533264d52a474613af4af4f6efa50f598c09acf943be604b22714ee7b5d6cfe5e8d05a2044a5650ca247c07</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         5527.58</td><td style=\"text-align: right;\">       0.177085 </td><td style=\"text-align: right;\">            0.0803395</td><td style=\"text-align: right;\">            5264.12</td><td style=\"text-align: right;\">           0.116322</td><td style=\"text-align: right;\">               0.0787753</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 48 49 50 51 52\n",
       " 54 56 61 62 63 67 68 69 70 71 72 73 78]                          </td><td>[4. 2. 3. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      5      9     10     11     12\n",
       "     16     17     18     19     20     21     28     29     30     33\n",
       "    256    286    307    308    309    310    311    312    313   1219\n",
       "   1220   1223   1224   1225   1226   1227   1229   1230   1231   1232\n",
       "   1233   1234   1235   1236   1238   1239   1240   1242   1243   1244\n",
       "   1980   2731   2763   2771   2786   3779   3850   4619   4632   4648\n",
       "   4669   4694   4779   4795   6003   6390   6420   6532   6593   6595\n",
       "  14261  16245  24905 121248 137943 222694 226564]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>89a21086ca9f0e18e588f36c1574d965f4ed483108208b11d6f52e229eed99a0bcf8d68fb5da24f50727aa167f9158dd7b9de41227a296230fa389d4c0213449</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         2096.24</td><td style=\"text-align: right;\">       0.157862 </td><td style=\"text-align: right;\">            0.0773763</td><td style=\"text-align: right;\">            2316.33</td><td style=\"text-align: right;\">           0.447932</td><td style=\"text-align: right;\">               0.073537 </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
       " 25 26 27 28 29 30 31 32 33 34 35 40 41 43 44 45 46 47 57 58 60 66]                          </td><td>[1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      6      7      8    231    232\n",
       "    233    234    262    791    795    799    846    848    852    853\n",
       "    855    856    858    861    863    864    866    868    869    871\n",
       "   1680   9659  25669  27021  27885  28074  28822  29327  29745  30347\n",
       "  30873  31037  31355  31649  31886 115134 128855 140406 161067 162703\n",
       " 202211 218577 234361 257753]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>f019d9a53ea6317b9c4271fa4ea14d7e7107ba35f871473ce9f5c207ec7526bade74f60bcfae76841ce1b5c0954faa73961fb718a6ce4a1551ed57dd629e032e</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         4828.7 </td><td style=\"text-align: right;\">       0.626082 </td><td style=\"text-align: right;\">            0.068934 </td><td style=\"text-align: right;\">            4118.7 </td><td style=\"text-align: right;\">           0.975112</td><td style=\"text-align: right;\">               0.0714218</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
       " 25 26 27 28 29 30 31 32 33 34 35 42 43 44 45 46 47 53 57 58 62 63 64 67\n",
       " 74 75]                          </td><td>[2. 4. 5. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1.\n",
       " 1. 3.]                         </td><td>[     0      1      2      3      4      6      7      8    202    203\n",
       "    204    205    208    209    210    211    229    230    247   1664\n",
       "   2234   2358   2590   3059   3064   3130  13549  13715  13831  13928\n",
       "  13958  14158  14873  16251  25616  27420  30666  32414  32673  34075\n",
       "  34193  34993  35675  35677  35828  36695  36815  36833  36839  37319\n",
       "  37759  37851  38268  38275  38436  38813  38955  39046  39200  39204\n",
       "  39362  39813  39905  40114  40756  41907  42203  42689  42879  43301\n",
       "  43819  44054  44135  44840  45065  45553  45712  45802  45849  46315\n",
       "  46491 124741 156589 165472 190325 196092 197240]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>5497dc4dc2abc0cf7c5a7e4b09b058cbd8deed23eabc446057911df6d6b3858e5f5854ae73f53cc02761185c7e02c0da166b17cf1f500903bef7f10e4a70fd25</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         4345.49</td><td style=\"text-align: right;\">       0.978829 </td><td style=\"text-align: right;\">            0.0747982</td><td style=\"text-align: right;\">            3607.71</td><td style=\"text-align: right;\">           0.673018</td><td style=\"text-align: right;\">               0.056413 </td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
       " 48 49 50 51 52 54 56 61 62 63 64 67 75 77]                          </td><td>[6. 2. 3. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     36    149    150    151    236    244    248    249\n",
       "    250    261    448    449    450    451    452    453    454    683\n",
       "    697    718    761    762    763    764    765    766    769    770\n",
       "    771    773    955    958    960    963    965    966    968    984\n",
       "    985    987    990    991    993    994    995    997    998   1046\n",
       "   1059   1092   2238   2569   2904   2905   2946   3647   4692   4695\n",
       "   8366  13581  13694  13905  14220  14271  14712  14859  15019  15151\n",
       "  15392  15437  15522  15549  15560  15598  15889  16004  16182  16267\n",
       "  16359  20730  20771  20785  20843  22875  24480  24577  24852  53828\n",
       " 126816 132228 149513 157143 223244 230468]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>7a26b41f0501bf1a91fab3c66f48efa09bdc582dc213b9576d772e855188741da14096f3506daee11a591c6582df456d9620f119ea3a43decca1deec5e217bb5</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         4890.76</td><td style=\"text-align: right;\">       0.989061 </td><td style=\"text-align: right;\">            0.0830742</td><td style=\"text-align: right;\">            2499.05</td><td style=\"text-align: right;\">           0.833535</td><td style=\"text-align: right;\">               0.0577531</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
       " 48 49 50 51 52 54 56 57 58 60 61 65 66 79 80 83 84]                          </td><td>[3. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8      9\n",
       "     10     11     12     13     14     15     22     31     32     34\n",
       "     35     40     41     43     44     63     64     81     82     83\n",
       "     84    116    117    118    119    120    121    122    123    439\n",
       "    440    441    442    443    444    445    446   1009   1105   1113\n",
       "   1115   1117   1631   1648   1651   1995   2018   2042   2043   2046\n",
       "   3952  20759  21665  22274  23669  23838  24241  35472  36610  36770\n",
       "  37958  40184  41068  42712  42791  46046  49260  56168  64717 116078\n",
       " 131231 135097 140555 148227 159879 160558 166006 197239 205224]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>5383d9b8598680fcd978ce6238d65a0805eb63fa59e4b45b68486f97d3e70f149ca706a0be06be8e569543a63ae756d12219b6786d1ddf1424e912f6ee37201a</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         2159.28</td><td style=\"text-align: right;\">       0.0817671</td><td style=\"text-align: right;\">            0.0734992</td><td style=\"text-align: right;\">            1616.69</td><td style=\"text-align: right;\">           0.110837</td><td style=\"text-align: right;\">               0.0671171</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
       " 25 26 27 28 29 30 31 32 33 34 35 38 39 42 43 44 45 46 47 53 57 59 76 81]                          </td><td>[4. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 2. 2. 1.]                         </td><td>[     0      1      2      3      4      5      6      7      8     85\n",
       "     86     87     88    107   1064   1071   1072   1075   1110   1209\n",
       "   1210   1684   1849   1850   1851   2215   4202   5237   5401   7782\n",
       "  10454  10491  10643  10729  10838  11677  15369  68749  69787  74124\n",
       "  74497  75192  76197  77532  79903  80283  80548  82333  82510  83513\n",
       "  83794  86834  87132  87626  88160  90265  92572  93100  93159  93522\n",
       "  95128  95539  96911  99690 101130 103184 103602 105018 106722 108018\n",
       " 109202 109477 112092 112607 112670 113234 113509 119438 127234 153204\n",
       " 211147 213309]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "<tr><td>f17d2f1ac2307d595763c3422ecd432cabf485c700895111519c7952f583164ae27e400504a50a7a7e73dbe709a3434f023729ab306484da388a1080e674feca</td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">         2702.7 </td><td style=\"text-align: right;\">       0.984483 </td><td style=\"text-align: right;\">            0.0511013</td><td style=\"text-align: right;\">            3574.73</td><td style=\"text-align: right;\">           0.993018</td><td style=\"text-align: right;\">               0.0714935</td><td style=\"text-align: right;\">                0</td><td style=\"text-align: right;\">               85</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              262144</td><td>[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
       " 25 26 27 28 29 30 31 32 33 34 35 43 44 45 46 47 53 57 62 63 64 67 74 75]                          </td><td>[1. 4. 4. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 3.]                         </td><td>[     0      1      2      3      4      6      7      8    164    165\n",
       "    166    202    203    204    205    208    209    210    211    229\n",
       "    230    463    520    788    886    890    964   1174   1176   1177\n",
       "   1387   1460   1477   2295   2302   2306   2311   2322   2349   2594\n",
       "   2596   2605   2607   2608   2613   2644   2650   2663   3198   3946\n",
       "   4040   6150  12735  12951  13201  13331  13447  26320  47125  48118\n",
       "  48271  48651  49934  50964  52100  52279  52435  52678  54577  55736\n",
       "  56712  56718  58360  58963  59206  65094  65175 117790 121041 140200\n",
       " 152718 221068 224277 237713 248531]                             </td><td>[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
       " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]                            </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prediction\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">  p0</th><th style=\"text-align: right;\">         p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">1e-19      </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">3.25879e-18</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gbm prediction progress: |ââââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n",
      "Export File progress: |âââââââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n",
      "Export File progress: |âââââââââââââââââââââââââââââââââââââââââââââââââââ| 100%\n"
     ]
    }
   ],
   "source": [
    "#preds = aml.predict(test)\n",
    "# or:\n",
    "preds = aml.leader.predict(test)\n",
    "print(\"test\")\n",
    "print(test.tail(10))\n",
    "print(\"prediction\")\n",
    "print(preds.tail(10))\n",
    "# If you need to generate predictions on a test set, you can make\n",
    "# predictions directly on the `\"H2OAutoML\"` object, or on the leader\n",
    "# model object directly\n",
    "\n",
    "#preds = aml.predict(test)\n",
    "# or:\n",
    "preds_over_all_hf = aml.leader.predict(train)\n",
    "path_out_file1=\"product_model_prediction/file_prediction_\"+process_date+\".csv\"\n",
    "output_pred_file=h2o.export_file(frame=preds_over_all_hf, path=path_out_file1, force=False)\n",
    "#\n",
    "train_over_all_hf = train\n",
    "path_out_file2=\"product_model_prediction/file_train_\"+process_date+\".csv\"\n",
    "output_pred_file=h2o.export_file(frame=train_over_all_hf, path=path_out_file2, force=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- predict: integer (nullable = true)\n",
      " |-- p0: double (nullable = true)\n",
      " |-- p1: double (nullable = true)\n",
      " |-- id1: long (nullable = false)\n",
      "\n",
      "root\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- kl_fraud_words: double (nullable = true)\n",
      " |-- ks_fraud_words: double (nullable = true)\n",
      " |-- entropy_fraud_words: double (nullable = true)\n",
      " |-- kl_notfraud_words: double (nullable = true)\n",
      " |-- ks_notfraud_words: double (nullable = true)\n",
      " |-- entropy_notfraud_words: double (nullable = true)\n",
      " |-- features85_type: integer (nullable = true)\n",
      " |-- features85_size: integer (nullable = true)\n",
      " |-- ngramscounts7_type: integer (nullable = true)\n",
      " |-- ngramscounts7_size: integer (nullable = true)\n",
      " |-- features85_list_indices: string (nullable = true)\n",
      " |-- features85_list_values: string (nullable = true)\n",
      " |-- ngramscounts7_list_indices: string (nullable = true)\n",
      " |-- ngramscounts7_list_values: string (nullable = true)\n",
      " |-- id1: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import findspark\n",
    "findspark.init()\n",
    "#\n",
    "import pyspark\n",
    "from pyspark.sql import functions as pfunc\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Window, types\n",
    "#\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from scipy.stats import kstest\n",
    "from scipy import stats\n",
    "#\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "#\n",
    "#import org.apache.spark.ml.feature.NGram\n",
    "from pyspark.ml.feature import NGram\n",
    "#\n",
    "from collections import Counter\n",
    "#\n",
    "from pyspark.ml.feature import NGram\n",
    "#\n",
    "from pyspark.ml.feature import NGram, CountVectorizer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "#\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector\n",
    "#\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "#\n",
    "# monotonically_increasing_id allows to use join to merge save size DF\n",
    "# https://forums.databricks.com/questions/8180/how-to-merge-two-data-frames-column-wise-in-apache.html\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "#\n",
    "#  FILTER with PySpark SQL Functions F.\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def match_func(predict, fraud_label):\n",
    "    if predict != fraud_label:\n",
    "        return \"no_match\"\n",
    "    else:\n",
    "        return \"match\"\n",
    "match_udf = udf(match_func, StringType())    \n",
    "#match_udf = udf(lambda (prediction,fraud_label): \"no_match\" if prediction!=fraud_label else \"match\", StringType())\n",
    "\n",
    "#\n",
    "# Arguments\n",
    "#\n",
    "import argparse\n",
    "## Parse date_of execution\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--datev1\", help=\"Execution Date\")\n",
    "#args = parser.parse_args()\n",
    "#if args.datev1:\n",
    "#    processdate = args.datev1\n",
    "# GENERAL PREPARATION SCRIPT\n",
    "#\n",
    "#  Date in format YYYYMMDD\n",
    "#process_date = processdate\n",
    "#if not process_date:\n",
    "#    process_date = \"20181231\"\n",
    "#\n",
    "process_date = \"20190206\"\n",
    "#\n",
    "sc = pyspark.SparkContext(appName=\"FraudCanada-Merge-Scoring-Data-From-Model\")\n",
    "sqlContext = SQLContext(sc)\n",
    "#\n",
    "input_file_prediction_df=\"hdfs:///data/staged/ott_dazn/advanced-model-data/scoring-fraud-notfraud-kl-ks-entropy-ngrams7-features-85//dt=\"+process_date+\"/file_prediction_\"+process_date+\".csv\"\n",
    "#\n",
    "input_file_train_df=\"hdfs:///data/staged/ott_dazn/advanced-model-data/scoring-fraud-notfraud-kl-ks-entropy-ngrams7-features-85//dt=\"+process_date+\"/file_train_\"+process_date+\".csv\"\n",
    "#\n",
    "output_file1=\"hdfs:///data/staged/ott_dazn/advanced-model-data/scoring-fraud-notfraud-kl-ks-entropy-ngrams7-features-85//dt=\"+process_date+\"/merged_prediction_train\"\n",
    "#\n",
    "prediction_df=sqlContext.read.csv(input_file_prediction_df,header=True,inferSchema=True)\n",
    "prediction_df = prediction_df.withColumn(\"id1\", monotonically_increasing_id())\n",
    "prediction_df.printSchema()\n",
    "#\n",
    "train_df=sqlContext.read.csv(input_file_train_df,header=True,inferSchema=True,multiLine=True)\n",
    "train_df = train_df.withColumn(\"id1\", monotonically_increasing_id())\n",
    "train_df.printSchema()\n",
    "#\n",
    "merged_prediction_train_df3 = prediction_df.join(train_df, \"id1\", \"outer\") #.drop(\"id1\")\n",
    "#\n",
    "merged_prediction_train_df3\\\n",
    ".withColumn(\"match_no_match\",match_udf(col('predict').cast('int'),col('fraud_label').cast('int')))\\\n",
    ".sort(merged_prediction_train_df3.id1.desc()).coalesce(1).write.csv(output_file1,header=True)\n",
    "#\n",
    "sc.stop()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- id1: integer (nullable = true)\n",
      " |-- predict: integer (nullable = true)\n",
      " |-- p0: double (nullable = true)\n",
      " |-- p1: double (nullable = true)\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- kl_fraud_words: double (nullable = true)\n",
      " |-- ks_fraud_words: double (nullable = true)\n",
      " |-- entropy_fraud_words: double (nullable = true)\n",
      " |-- kl_notfraud_words: double (nullable = true)\n",
      " |-- ks_notfraud_words: double (nullable = true)\n",
      " |-- entropy_notfraud_words: double (nullable = true)\n",
      " |-- features85_type: integer (nullable = true)\n",
      " |-- features85_size: integer (nullable = true)\n",
      " |-- ngramscounts7_type: integer (nullable = true)\n",
      " |-- ngramscounts7_size: integer (nullable = true)\n",
      " |-- features85_list_indices: string (nullable = true)\n",
      " |-- features85_list_values: string (nullable = true)\n",
      " |-- ngramscounts7_list_indices: string (nullable = true)\n",
      " |-- ngramscounts7_list_values: string (nullable = true)\n",
      " |-- match_no_match: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- hash_message: string (nullable = true)\n",
      " |-- id1: integer (nullable = true)\n",
      " |-- predict: integer (nullable = true)\n",
      " |-- p0: double (nullable = true)\n",
      " |-- p1: double (nullable = true)\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- kl_fraud_words: double (nullable = true)\n",
      " |-- ks_fraud_words: double (nullable = true)\n",
      " |-- entropy_fraud_words: double (nullable = true)\n",
      " |-- kl_notfraud_words: double (nullable = true)\n",
      " |-- ks_notfraud_words: double (nullable = true)\n",
      " |-- entropy_notfraud_words: double (nullable = true)\n",
      " |-- features85_type: integer (nullable = true)\n",
      " |-- features85_size: integer (nullable = true)\n",
      " |-- ngramscounts7_type: integer (nullable = true)\n",
      " |-- ngramscounts7_size: integer (nullable = true)\n",
      " |-- features85_list_indices: string (nullable = true)\n",
      " |-- features85_list_values: string (nullable = true)\n",
      " |-- ngramscounts7_list_indices: string (nullable = true)\n",
      " |-- ngramscounts7_list_values: string (nullable = true)\n",
      " |-- match_no_match: string (nullable = true)\n",
      " |-- words_conc: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "import findspark\n",
    "findspark.init()\n",
    "#\n",
    "import pyspark\n",
    "from pyspark.sql import functions as pfunc\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Window, types\n",
    "#\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from scipy.stats import kstest\n",
    "from scipy import stats\n",
    "#\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "#\n",
    "#import org.apache.spark.ml.feature.NGram\n",
    "from pyspark.ml.feature import NGram\n",
    "#\n",
    "from collections import Counter\n",
    "#\n",
    "from pyspark.ml.feature import NGram\n",
    "#\n",
    "from pyspark.ml.feature import NGram, CountVectorizer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "#\n",
    "from pyspark.mllib.linalg import SparseVector, DenseVector\n",
    "#\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "#\n",
    "#  FILTER with PySpark SQL Functions F.\n",
    "from pyspark.sql import functions as F\n",
    "# FUNCTIONS\n",
    "#\n",
    "# Arguments\n",
    "#\n",
    "import argparse\n",
    "## Parse date_of execution\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--datev1\", help=\"Execution Date\")\n",
    "#args = parser.parse_args()\n",
    "#if args.datev1:\n",
    "#    processdate = args.datev1\n",
    "# GENERAL PREPARATION SCRIPT\n",
    "#\n",
    "#  Date in format YYYYMMDD\n",
    "#process_date = processdate\n",
    "#if not process_date:\n",
    "#    process_date = \"20190113\"\n",
    "#\n",
    "process_date = \"20190206\"\n",
    "#\n",
    "sc = pyspark.SparkContext(appName=\"FraudCanada-Search-Suspitious-hashmessage-matching_words-By-AUTOML-NGrams-CountVectorizer-KL-KS-Entropy\")\n",
    "sqlContext = SQLContext(sc)\n",
    "#\n",
    "input_fraud=\"hdfs:///data/staged/ott_dazn/advanced-model-data/fraud-notfraud-canada-tokenizedwords-ngrams-7-features-85/dt=\"+process_date\n",
    "#\n",
    "input_merged_prediction_train=\"hdfs:///data/staged/ott_dazn/advanced-model-data/scoring-fraud-notfraud-kl-ks-entropy-ngrams7-features-85/dt=\"+process_date+\"/merged_prediction_train/merge_prediction_train_\"+process_date+\".csv\"\n",
    "#\n",
    "output_no_match=\"hdfs:///data/staged/ott_dazn/advanced-model-data/scoring-fraud-notfraud-kl-ks-entropy-ngrams7-features-85/dt=\"+process_date+\"/no_match_hash_message\"\n",
    "#\n",
    "# The most Frequent would the ones with the max frequency of NGrams85 tokens\n",
    "pd.options.display.max_colwidth = 512\n",
    "#\n",
    "#\n",
    "# Select Tokens/words form the max frequency of NGrams85 tokens hash_message\n",
    "notfraud_df=sqlContext.read.json(input_fraud)\\\n",
    ".select(col('hash_message'),col('words'))\n",
    "notfraud_df.printSchema()\n",
    "#\n",
    "merged_df=sqlContext.read.csv(input_merged_prediction_train,header=True,inferSchema=True,multiLine=True)\\\n",
    ".filter(\" match_no_match = 'no_match' \")\n",
    "merged_df.printSchema()\n",
    "#\n",
    "join_results_df=merged_df.join(notfraud_df, \"hash_message\" )\\\n",
    ".withColumn('words_conc',F.concat_ws('',col('words')).cast('string')).drop('words')\\\n",
    ".persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "join_results_df.printSchema()\n",
    "#\n",
    "join_results_df.coalesce(1).write.csv(output_no_match,header=True)\n",
    "#\n",
    "### Argument from Merge_train_predict_YYYYMMDD.csv\n",
    "#        #47bade7fcc6883b26ff81f0681ff8b824d97718c1f9d4584c66f2d58b925f4da9447f6215c62293565820492f7499efea8aed25ef1bf5ddeb36b3872d4ad243d\n",
    "#var_hash_message='47bade7fcc6883b26ff81f0681ff8b824d97718c1f9d4584c66f2d58b925f4da9447f6215c62293565820492f7499efea8aed25ef1bf5ddeb36b3872d4ad243d'\n",
    "# Select Tokens/words form the max frequency of NGrams85 tokens hash_message\n",
    "#standard_notfraud_ngram_words=sqlContext.read.json(input_fraud)\n",
    "#standard_notfraud_ngram_words.printSchema()\n",
    "#\n",
    "#standard_notfraud_words_search=standard_notfraud_ngram_words\\\n",
    "#.withColumn('fraud_search_hash',lit(var_hash_message).cast('string'))\\\n",
    "#.filter(\" hash_message=fraud_search_hash \")\n",
    "#standard_notfraud_words_search.printSchema()\n",
    "#\n",
    "#standard_notfraud_words=standard_notfraud_words_search\\\n",
    "#.withColumn('words_conc',F.concat_ws('',col('words')).cast('string'))\\\n",
    "#.select(col('words_conc')).limit(1).toPandas()['words_conc'][0] \n",
    "#\n",
    "#print(\"From Identifyed Label=0, prediction p=1 : var_hash_message=\")\n",
    "#print(var_hash_message)\n",
    "#print(\"Potential fraud value FROM : standard_notfraud_words=\")\n",
    "#print(standard_notfraud_words)\n",
    "#\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
