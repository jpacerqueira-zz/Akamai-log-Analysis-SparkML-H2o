#!/usr/bin/env bash
echo "||_______________________________________________________________________________________________________________||" 
echo "||____________________________________ SPARK RAW OPERATIONS -- CLEAN DATA _______________________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "||> val df11 = spark.sql(\" CREATE TEMPORARY VIEW akamailogs USING com.databricks.spark.csv                     ||" 
echo "||      OPTIONS (path '/data/raw/akamai/dazn_logs/dt=0/id=*/hh=*/dz*', header 'true', delimiter '\t')\" )        ||"
echo "||_______________________________________________________________________________________________________________||" 
echo "||>   val akamaidf = spark.sql(\"SELECT * FROM akamailogs where sc_status IS NOT NULL\").coalesce(5).cache()||"
echo "||_______________________________________________________________________________________________________________||" 
echo "||>    akamaidf.write.mode(\"append\").partitionBy(\"dt\",\"hh\")      \                                         ||"
echo "||                                    .parquet(\"/data/staged/akamai/dazn_logs/dazn_logs\")                      ||"
echo "||_______________________________________________________________________________________________________________||" 
echo "||___________________________________ SPARK RAW OPERATIONS -- DONE ______________________________________________||"
echo "||_______________________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "||___________________________________ LOADING  LOADING  LOADING  LOADING  LOADING _______________________________||" 
echo "||___________________________________ LOADING  LOADING  LOADING  LOADING  LOADING _______________________________||" 
echo "||___________________________________ LOADING  LOADING  LOADING  LOADING  LOADING _______________________________||" 
echo "||___________________________________ LOADING  LOADING  LOADING  LOADING  LOADING _______________________________||" 
echo "||___________________________________ LOADING  LOADING  LOADING  LOADING  LOADING _______________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "||___________________________________ SPARK STAGE OPERATIONS - EXPLORE SAMPLES __________________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "||_______  1.)  _________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "|| scala> val df1 = spark.read.parquet(\"/data/staged/akamai/spark/dazn_logs\")                                   ||"
echo "|| scala> df1.registerTempTable(\"dazn_logs\")                                                                     ||"
echo "|| scala> spark.sql(\"select count(*) from dazn_logs where dt='2017-03-11' and hh between 0  and 5\").collect() ||"
echo "|| scala>  spark.sql(\"select hh,count(*) from dazn_logs where dt='2017-03-11' and hh between 0  and 24  group by hh \").collect() ||"
echo "||_______________________________________________________________________________________________________________||" 
echo "||_______  2.)  _________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "|| scala> val df2 = spark.sql(\"select dt,hh,time,cs_uri_stem,count(*) as group_counter from dazn_logs \      ||"
echo "||   where dt='2017-03-11' and hh between 0  and 5  and cs_uri_stem LIKE '/out/u/%_video_%.mp4' AND   \          ||" 
echo "||           sc_status=200 group by dt,hh,time,cs_uri_stem \")                                                   ||"
echo "|| df2: org.apache.spark.sql.DataFrame = [dt: string, hh: int, time: string, cs_uri_stem: string, group_counter: bigint] ||"
echo "|| scala>                                                                                                        ||"
echo "|| scala> df2.write.mode(\"append\").partitionBy(\"dt\",\"hh\").parquet(\"dazn_logs\")                                 ||"
echo "||_______________________________________________________________________________________________________________||" 
echo "||_______  3.)  _________________________________________________________________________________________________________________||" 
echo "|| scala>                                                                                                                        ||"
echo "|| scala> val df1= spark.read.parquet(\"/data/staged/akamai/spark/dazn_logs\")                                               ||"  
echo "|| df1: org.apache.spark.sql.DataFrame = [date: string, time: string, cs_ip: string, cs_method: string, cs_uri_origin: string,   ||"
echo "||    cs_uri_stem: string, cs_uri_query: string, sc_status: int, sc_bytes: int, time_taken: int, cs_referer: string,             ||"
echo "||    cs_user_agent: string, cs_cookie: string, cs_host_header: string, cs_request_id: string, cached: int, dt: string, hh: int] ||"
echo "|| scala>                                                                                                                        ||"
echo "|| scala>  df1.registerTempTable(\"dazn_logs\")                                                                                    ||"
echo "||_______________________________________________________________________________________________________________________________||" 
echo "|| scala> val df2 = spark.sql(\"select dt,hh,time,cs_uri_stem,count(*) as group_counter from dazn_logs where dt='2017-03-11'   ||"
echo "||    and hh between 6  and 8  and cs_uri_stem LIKE '/out/u/%_video_%.mp4' AND sc_status=200 group by dt,hh,time,cs_uri_stem \")  ||"
echo "||  df2: org.apache.spark.sql.DataFrame = [dt: string, hh: int, time: string, cs_uri_stem: string, group_counter: bigint]        ||"
echo "|| scala> df2.write.mode(\"append\").partitionBy(\"dt\",\"hh\").parquet(\"dazn_counter_logs\")                                           ||"
echo "||_______________________________________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________________________||" 
echo "|| scala> val df3 = spark.sql(\"select dt,hh,time,sc_status,cs_uri_stem,count(*) as group_counter from dazn_logs where  dt='2017-03-03' ||"
echo "||              and hh between 1  and 24 and sc_status=200  group by dt,hh,time,sc_status,cs_uri_stem \")                         ||"
echo "||_______________________________________________________________________________________________________________________________||" 
echo "|| df3: org.apache.spark.sql.DataFrame = [dt: date, hh: int ... 4 more fields]                                                   ||"
echo "||_______________________________________________________________________________________________________________________________||" 
echo "|| scala> df3.collect()                                                                                                          ||"
echo "|| res3: Array[org.apache.spark.sql.Row] = Array([2017-03-03,21,21:54:01,200,/out/u/333_audio_1_13_103987.mp4,4])                ||"
echo "|| scala> :quit                                                                                                                  ||"
echo "||_______________________________________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________________________||" 
echo "||_______________________________________________ SPARK STAGED OPERATIONS -- END ________________________________________________||" 
echo "||_______________________________________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________________________||" 
yarn application --list 
echo "||_______________________________________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________________________||" 
echo "__||   MY SPARK ||__"
cd ~
spark2-shell --master yarn-client --jars spark-csv_2.11-1.5.0.jar  ###ONLY FOR PROXY OPEN VMs###  --packages com.databricks:spark-csv_2.11:1.5.0 
echo "||_______________________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________||"

