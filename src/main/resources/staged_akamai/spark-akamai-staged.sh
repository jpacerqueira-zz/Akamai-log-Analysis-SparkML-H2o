#!/usr/bin/env bash
echo "||_______________________________________________________________________________________________________________||" 
echo "||____________________________________ SPARK RAW OPERATIONS -- CLEAN DATA _______________________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "||_______ 0.1.)  ________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "||> val df=sqlContext.sql(\"CREATE TEMPORARY TABLE akamailogs USING com.databricks.spark.csv                      ||" 
echo "||      OPTIONS (path '/data/raw/akamai/dazn_logs/dt=0/id=*/hh=*/dz*', header 'true', delimiter '\t')\" )         ||"
echo "||_______________________________________________________________________________________________________________||" 
echo "||>   val akamaidf = sqlContext.sql(\"SELECT * FROM akamailogs where sc_status IS NOT NULL\").coalesce(5).cache()  ||"
echo "||_______________________________________________________________________________________________________________||" 
echo "||>    akamaidf.write.mode(\"append\").partitionBy(\"dt\",\"hh\")      \                                               ||"
echo "||                                    .parquet(\"/data/staged/akamai/dazn_logs/dazn_logs\")                        ||"
echo "||_______________________________________________________________________________________________________________||" 
echo "||___________________________________ SPARK RAW OPERATIONS -- DONE ______________________________________________||"
echo "||_______________________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "||___________________________________ LOADING  LOADING  LOADING  LOADING  LOADING _______________________________||" 
echo "||___________________________________ LOADING  LOADING  LOADING  LOADING  LOADING _______________________________||" 
echo "||___________________________________ LOADING  LOADING  LOADING  LOADING  LOADING _______________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "||________________________________________________________________________________________________________________________||" 
echo "||________________________________________________________________________________________________________________________||" 
echo "||________________________________________________________________________________________________________________________||" 
echo "||___________________________________||  SPARK STAGE OPERATIONS - EXPLORE SAMPLES ||______________________________________||" 
echo "||________________________________________________________________________________________________________________________||" 
echo "||_______  1.)  __________________________________________________________________________________________________________||" 
echo "||________________________________________________________________________________________________________________________||" 
echo "|| scala> val df1=sqlContext.read.parquet(\"/user/joao.cerqueira/data/staged/akamai/spark/dazn_logs\")                      ||"
echo "|| scala> df1.registerTempTable(\"dazn_logs\")                                                                              ||"
echo "|| scala> sqlContext.sql(\"select count(*) from dazn_logs where dt='2017-03-11' and hh between 0  and 5\").collect()        ||"
echo "|| scala>  sqlContext.sql(\"select hh,count(*) from dazn_logs where dt='2017-03-11' and hh between 0  and 24  group by hh \").collect() ||"
echo "|| scala> sqlContext.sql(\"select dt,hh,count(*) from dazn_logs where dt='2017-03-11' and hh between 0  and 24  group by dt,hh \").collect() ||"
echo "||________________________________________________________________________________________________________________________||" 
echo "||_______  2.)  __________________________________________________________________________________________________________||" 
echo "||________________________________________________________________________________________________________________________||" 
echo "|| scala> val df2=sqlContext.sql(\"select dt,hh,time,cs_uri_stem,count(*) as group_counter from dazn_logs \                ||"
echo "||   where dt='2017-03-11' and hh between 0  and 5  and cs_uri_stem LIKE '/out/u/%_video_%.mp4' AND   \                   ||" 
echo "||           sc_status=200 group by dt,hh,time,cs_uri_stem \")                                                             ||"
echo "|| df2: org.apache.spark.sql.DataFrame = [dt: string, hh: int, time: string, cs_uri_stem: string, group_counter: bigint]  ||"
echo "|| scala>                                                                                                                 ||"
echo "|| scala> df2.write.mode(\"append\").partitionBy(\"dt\",\"hh\").parquet(\"dazn_logs\")                                            ||"
echo "||________________________________________________________________________________________________________________________||" 
echo "||_______  3.)  ___________________________________________________________________________________________________________________||" 
echo "|| scala>                                                                                                                          ||"
echo "|| scala> val df1=sqlContext.read.parquet(\"/data/staged/akamai/spark/dazn_logs\")                                                   ||"  
echo "|| df1: org.apache.spark.sql.DataFrame = [date: string, time: string, cs_ip: string, cs_method: string, cs_uri_origin: string,     ||"
echo "||    cs_uri_stem: string, cs_uri_query: string, sc_status: int, sc_bytes: int, time_taken: int, cs_referer: string,               ||"
echo "||    cs_user_agent: string, cs_cookie: string, cs_host_header: string, cs_request_id: string, cached: int, dt: string, hh: int]   ||"
echo "|| scala>                                                                                                                          ||"
echo "|| scala>  df1.registerTempTable(\"dazn_logs\")                                                                                      ||"
echo "||____________________________________________________________________________________________________________________________________||" 
echo "|| scala> val df2=sqlContext.sql(\"select dt,hh,time,cs_uri_stem,count(*) as group_counter from dazn_logs where dt='2017-03-11'        ||"
echo "||    and hh between 6  and 8  and cs_uri_stem LIKE '/out/u/%_video_%.mp4' AND sc_status=200 group by dt,hh,time,cs_uri_stem \")       ||"
echo "||  df2: org.apache.spark.sql.DataFrame = [dt: string, hh: int, time: string, cs_uri_stem: string, group_counter: bigint]             ||"
echo "|| scala> df2.write.mode(\"append\").partitionBy(\"dt\",\"hh\").parquet(\"dazn_counter_logs\")                                                ||"
echo "||____________________________________________________________________________________________________________________________________||" 
echo "||___________________________________________________________________________________________________________________________________________||" 
echo "||_______  4.)  _____________________________________________________________________________________________________________________________||" 
echo "||___________________________________________________________________________________________________________________________________________||" 
echo "|| scala> val df1=sqlContext.read.parquet(\"/data/staged/akamai/spark/dazn_logs\")                                                             ||"
echo "|| df1: org.apache.spark.sql.DataFrame = [date: string, time: string, cs_ip: string, cs_method: string, cs_uri_origin: string, cs_uri_stem: string, cs_uri_query: string, sc_status: int, sc_bytes: int, time_taken: int, cs_referer: string, cs_user_agent: string, cs_cookie: string, cs_host_header: string, cs_request_id: string, cached: int, dt: string, hh: int] ||"
echo "||___________________________________________________________________________________________________________________________________________||" 
echo "|| scala> df1.registerTempTable(\"dazn_logs\")                                                                                                 ||"   
echo "||___________________________________________________________________________________________________________________________________________||" 
echo "|| scala> val df2=sqlContext.sql(\" select dt,hh,time,cs_uri_stem,sc_bytes,count(*) as group_counter from dazn_logs where dt='2017-03-11'     ||" 
echo "||    and hh between 0  and 3  and cs_uri_stem LIKE '/out/u/%_video_%.mp4' AND sc_status=200 group by dt,hh,time,cs_uri_stem,sc_bytes \")     ||"
echo "|| df2: org.apache.spark.sql.DataFrame = [dt: string, hh: int, time: string, cs_uri_stem: string, sc_bytes: int, group_counter: bigint]      ||"
echo "||___________________________________________________________________________________________________________________________________________||" 
echo "|| scala> df2.coalesce(6).write.mode(\"append\").partitionBy(\"dt\",\"hh\").parquet(\"dazn_uri_counter\")                                            ||"
echo "||___________________________________________________________________________________________________________________________________________||" 
echo "|| scala> val df3=sqlContext.sql(\" select dt,hh,time,cs_uri_stem,sc_bytes,count(*) as group_counter from dazn_logs where dt='2017-03-11'     ||"
echo "||        and hh between 4  and 6  and cs_uri_stem LIKE '/out/u/%_video_%.mp4' AND sc_status=200 group by dt,hh,time,cs_uri_stem,sc_bytes \") ||"
echo "|| df3: org.apache.spark.sql.DataFrame = [dt: string, hh: int, time: string, cs_uri_stem: string, sc_bytes: int, group_counter: bigint]      ||"
echo "||___________________________________________________________________________________________________________________________________________||" 
echo "|| scala> df3.coalesce(6).write.mode(\"append\").partitionBy(\"dt\",\"hh\").parquet(\"dazn_uri_counter\")                                            ||"
echo "||____________________________________________________________________________________________________________________________________________________||" 
echo "|| scala> val df4=sqlContext.sql(\" select dt,hh,time,cs_uri_stem,sc_bytes,count(*) as group_counter from dazn_logs where dt='2017-03-11'              ||"
echo "||  and hh between 7  and 9  and cs_uri_stem LIKE '/out/u/%_video_%.mp4' AND sc_status=200 group by dt,hh,time,cs_uri_stem,sc_bytes \")                ||"
echo "|| scala> val df5=sqlContext.sql(\" select dt,hh,time,cs_uri_stem,sc_bytes,count(*) as group_counter from dazn_logs where dt='2017-03-12'              ||"
echo "||      and hh between 0  and 24  and cs_uri_stem LIKE '/out/u/%_video_%.mp4' AND sc_status=200 group by dt,hh,time,cs_uri_stem,sc_bytes \")           ||" 
echo "||  df8: org.apache.spark.sql.DataFrame = [dt: string, hh: int, time: string, cs_uri_stem: string, sc_bytes: int, group_counter: bigint]              ||"
echo "||____________________________________________________________________________________________________________________________________________________||" 
echo "|| scala> df8.coalesce(6).write.mode(\"append\").partitionBy(\"dt\",\"hh\").parquet(\"dazn_uri_counter\")                                                     ||"
echo "||____________________________________________________________________________________________________________________________________________________||" 
echo "|| scala> exit                                                                                                                                        ||"
echo "||    warning: there were 1 deprecation warning(s); re-run with -deprecation for details                                                              ||"
echo "||____________________________________________________________________________________________________________________________________________________||" 
echo "||____________________________________________________________________________________________________________________________________________________||" 
echo "||                                                                                                                                                    ||"
echo "||_______  5.)  ______________________________________________________________________________________________________________________________________||" 
echo "|| scala> val df1=sqlContext.read.parquet(\"dazn_uri_counter\")                                                                                         ||"
echo "|| df1: org.apache.spark.sql.DataFrame = [time: string, cs_uri_stem: string, sc_bytes: int, group_counter: bigint, dt: string, hh: int]               ||"
echo "|| scala> df1.registerTempTable(\"dazn_uri_logs\")                                                                                                      ||"
echo "|| scala> sqlContext.sql(\" select cs_uri_stem, sc_bytes, group_counter, dt, hh, time from dazn_uri_logs where dt='2017-03-11' and hh between 0 and 3 order by cs_uri_stem\")        ||"
echo "|| res1: org.apache.spark.sql.DataFrame = [cs_uri_stem: string, sc_bytes: int, group_counter: bigint, dt: string, hh: int, time: string]              ||"
echo "|| scala> res1.coalesce(6).write.mode(\"append\").partitionBy(\"dt\",\"hh\").parquet(\"dazn_uri_ordered\")                                                    ||"
echo "|| scala> sqlContext.sql(\" select cs_uri_stem, sc_bytes, group_counter, dt, hh, time from dazn_uri_logs where dt='2017-03-11' and hh between 4 and 5 order by cs_uri_stem\")        ||"
echo "|| res3: org.apache.spark.sql.DataFrame = [cs_uri_stem: string, sc_bytes: int, group_counter: bigint, dt: string, hh: int, time: string]              ||"
echo "|| scala> res3.coalesce(6).write.mode(\"append\").partitionBy(\"dt\",\"hh\").parquet(\"dazn_uri_ordered\")                                                    ||"
echo "|| scala> sqlContext.sql(\" select cs_uri_stem, sc_bytes, group_counter, dt, hh, time from dazn_uri_logs where dt='2017-03-11' and hh between 7 and 8 order by cs_uri_stem\")        ||"
echo "|| res5: org.apache.spark.sql.DataFrame = [cs_uri_stem: string, sc_bytes: int, group_counter: bigint, dt: string, hh: int, time: string]              ||"
echo "|| scala> res5.coalesce(6).write.mode(\"append\").partitionBy(\"dt\",\"hh\").parquet(\"dazn_uri_ordered\")                                                    ||"
echo "|| scala> sqlContext.sql(\" select cs_uri_stem, sc_bytes, group_counter, dt, hh, time from dazn_uri_logs where dt='2017-03-11' and hh between 9 and 12 order by cs_uri_stem\")       ||" 
echo "|| res7: org.apache.spark.sql.DataFrame = [cs_uri_stem: string, sc_bytes: int, group_counter: bigint, dt: string, hh: int, time: string]              ||"
echo "|| scala> res7.coalesce(6).write.mode(\"append\").partitionBy(\"dt\",\"hh\").parquet(\"dazn_uri_ordered\")                                                    ||"
echo "|| scala> sqlContext.sql(\" select cs_uri_stem, sc_bytes, group_counter, dt, hh, time from dazn_uri_logs where dt='2017-03-11' and hh between 13 and 18 order by cs_uri_stem\")      ||"
echo "|| res9: org.apache.spark.sql.DataFrame = [cs_uri_stem: string, sc_bytes: int, group_counter: bigint, dt: string, hh: int, time: string]              ||"
echo "|| scala> res9.coalesce(6).write.mode(\"append\").partitionBy(\"dt\",\"hh\").parquet(\"dazn_uri_ordered\")                                                    ||"
echo "|| scala> sqlContext.sql(\" select cs_uri_stem, sc_bytes, group_counter, dt, hh, time from dazn_uri_logs where dt='2017-03-11' and hh between 19 and 21 order by cs_uri_stem\")      ||"
echo "|| res11: org.apache.spark.sql.DataFrame = [cs_uri_stem: string, sc_bytes: int, group_counter: bigint, dt: string, hh: int, time: string]             ||"
echo "|| scala> res11.coalesce(6).write.mode(\"append\").partitionBy(\"dt\",\"hh\").parquet(\"dazn_uri_ordered\")                                                   ||"
echo "|| scala> sqlContext.sql(\" select cs_uri_stem, sc_bytes, group_counter, dt, hh, time from dazn_uri_logs where dt='2017-03-11' and hh between 22 and 24 order by cs_uri_stem\")      ||"
echo "|| res13: org.apache.spark.sql.DataFrame = [cs_uri_stem: string, sc_bytes: int, group_counter: bigint, dt: string, hh: int, time: string]             ||"
echo "|| scala> res13.coalesce(6).write.mode(\"append\").partitionBy(\"dt\",\"hh\").parquet(\"dazn_uri_ordered\")                                                   ||"
echo "|| scala> sqlContext.sql(\" select cs_uri_stem, sc_bytes, group_counter, dt, hh, time from dazn_uri_logs where dt='2017-03-12' and hh between 0 and 24 order by cs_uri_stem\")       ||" 
echo "|| res15: org.apache.spark.sql.DataFrame = [cs_uri_stem: string, sc_bytes: int, group_counter: bigint, dt: string, hh: int, time: string]             ||"
echo "|| scala> res15.coalesce(6).write.mode(\"append\").partitionBy(\"dt\",\"hh\").parquet(\"dazn_uri_ordered\")                                                   ||"
echo "|| scala> exit()                                                                                                                                      ||"
echo "|| warning: there were 1 deprecation warning(s); re-run with -deprecation for details                                                                 ||"
echo "||____________________________________________________________________________________________________________________________________________________||" 
echo "||_____________________________________________________ SPARK STAGED OPERATIONS -- END _______________________________________________________________||" 
echo "||____________________________________________________________________________________________________________________________________________________||" 
yarn application --list 
echo "||____________________________________________________________________________________________________________________________________________________||" 
echo "||____________________________________________________________________________________________________________________________________________________||" 
echo "||____________________________________________________________________________________________________________________________________________________||" 
echo "||_________________________________________________||   SPARK ADHOC OPERATIONS       ||_______________________________________________________________||"
echo "||____________________________________________________________________________________________________________________________________________________||" 
echo "||____________________________________________________________________________________________________________________________________________________||" 
echo "|| scala> val df1=sqlContext.read.parquet(\"dazn_uri_counter\")                                                                                         ||" 
echo "|| scala>  df1.registerTempTable(\"dazn_uri_logs\")                                                                                                     ||"
echo "|| scala>  sqlContext.sql(\" select count(*) from dazn_uri_counter limit 100 \")                                                                        ||"
echo "||____________________________________________________________________________________________________________________________________________________||" 
echo "||                                                                                                                                                    ||"
echo "|| scala> val df1=sqlContext.read.parquet(\"dazn_uri_ordered\")                                                                                         ||"
echo "||____________________________________________________________________________________________________________________________________________________||" 
echo "||____________________________________________________________________________________________________________________________________________________||" 
cd /u01/data/akamai/execute_akamai/staged_akamai
echo "$SPARK_HOME"
echo "$SPARK_LOCAL_IP"
#export SPARK_LOCAL_IP=0.0.0.0
spark-shell --master yarn-client  --jars ./commons-csv-1.1.jar,./parquetindexer-1.0-TPOC-jar-with-dependencies.jar,./spark-csv_2.10-1.5.0.jar ###ONLY FOR PROXY OPEN VMs### --packages com.databricks:spark-csv_2.10:1.5.0 
echo "||____________________________________________________________________________________________________________________________________________________||" 
echo "||____________________________________________________________________________________________________________________________________________________||"
echo "||____________________________________________________________________________________________________________________________________________________||"
