#!/usr/bin/env bash
echo "||_______________________________________________________________________________________________________________||" 
echo "||____________________________________ SPARK RAW OPERATIONS -- CLEAN DATA _______________________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "||_______ 0.1.)  ________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "||> val df=sqlContext.sql(\"CREATE TEMPORARY TABLE akamailogs USING com.databricks.spark.csv                      ||" 
echo "||      OPTIONS (path '/data/raw/content/vdp/dummy/dt=2017-07-17/*', header 'true', delimiter '\t')\" )           ||"
echo "||_______________________________________________________________________________________________________________||" 
echo "||>   val akamaidf = sqlContext.sql(\"SELECT * FROM akamailogs where date IS NOT NULL\").coalesce(5).cache()       ||"
echo "||_______________________________________________________________________________________________________________||" 
echo "||>    akamaidf.write.mode(\"append\").partitionBy(\"date\",\"time\")      \                                           ||"
echo "||                                    .parquet(\"/data/staged/content/vdp/dummy-test\")                            ||"
echo "||_______________________________________________________________________________________________________________||" 
echo "||___________________________________ SPARK RAW OPERATIONS -- DONE ______________________________________________||"
echo "||_______________________________________________________________________________________________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "||___________________________________ LOADING  LOADING  LOADING  LOADING  LOADING _______________________________||" 
echo "||___________________________________ LOADING  LOADING  LOADING  LOADING  LOADING _______________________________||" 
echo "||___________________________________ LOADING  LOADING  LOADING  LOADING  LOADING _______________________________||" 
echo "||_______________________________________________________________________________________________________________||" 
echo "||________________________________________________________________________________________________________________________||" 
echo "||________________________________________________________________________________________________________________________||" 
echo "||________________________________________________________________________________________________________________________||" 
echo "||___________________________________||  SPARK STAGE OPERATIONS - EXPLORE SAMPLES ||______________________________________||" 
echo "||________________________________________________________________________________________________________________________||" 
echo "||_______  1.)  __________________________________________________________________________________________________________||" 
echo "||________________________________________________________________________________________________________________________||" 
echo "|| scala> val df1=sqlContext.read.parquet(\"/data/staged/content/vdp/dummy\")                                               ||"
echo "|| scala> df1.registerTempTable(\"vdp_logs\")                                                                               ||"
echo "|| scala> sqlContext.sql(\"select count(*) from vdp_logs where dt='2017-07-17' and hh between 6  and 12\").collect()        ||"
echo "||        res1: Array[org.apache.spark.sql.Row] = Array([11793])                                                          ||"
echo "||________________________________________________________________________________________________________________________||" 
echo "|| scala> val df=sqlContext.sql(\"CREATE TEMPORARY TABLE akamailogs USING com.databricks.spark.csv OPTIONS (path '/data/raw/content/vdp/dummy/dt=2017-07-17/*', header 'true', delimiter '\t')\" )                                                                                            ||" 
echo "||        df: org.apache.spark.sql.DataFrame = []                                                                         ||"
echo "|| scala> val akamaidf = sqlContext.sql(\"SELECT count(*) FROM akamailogs where date IS NOT NULL\").collect()               ||"
echo "||        akamaidf: Array[org.apache.spark.sql.Row] = Array([11793])                                                      ||"
echo "||________________________________________________________________________________________________________________________||" 
echo "$SPARK_HOME"
echo "$SPARK_LOCAL_IP"
#export SPARK_LOCAL_IP=0.0.0.0
yarn application --list 
spark-shell --master yarn-client  --jars ./commons-csv-1.1.jar,./spark-csv_2.10-1.5.0.jar,./parquetindexer-1.0-TPOC-jar-with-dependencies.jar ###ONLY FOR PROXY OPEN VMs### --packages com.databricks:spark-csv_2.10:1.5.0 
